{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/zhihao/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/zhihao/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from time import time\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from collections import defaultdict\n",
    "\n",
    "# import pytrec_eval\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss, BCEWithLogitsLoss\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig, AutoTokenizer\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from accelerate import Accelerator\n",
    "from copy import deepcopy\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "from watchog.dataset import (\n",
    "    # collate_fn,\n",
    "    TURLColTypeTablewiseDataset,\n",
    "    TURLRelExtTablewiseDataset,\n",
    "    SatoCVTablewiseDataset,\n",
    "    ColPoplTablewiseDataset\n",
    ")\n",
    "\n",
    "from watchog.dataset import TableDataset, SupCLTableDataset, SemtableCVTablewiseDataset, GittablesColwiseDataset, GittablesTablewiseDataset\n",
    "from watchog.model import BertMultiPairPooler, BertForMultiOutputClassification, BertForMultiOutputClassificationColPopl, Verifier\n",
    "from watchog.model import SupCLforTable, UnsupCLforTable, lm_mp\n",
    "from watchog.utils import load_checkpoint, f1_score_multilabel, collate_fn, get_col_pred, ColPoplEvaluator\n",
    "from watchog.utils import task_num_class_dict\n",
    "from accelerate import DistributedDataParallelKwargs\n",
    "import wandb\n",
    "\n",
    "from argparse import Namespace\n",
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import re\n",
    "import transformers\n",
    "from torch.utils import data\n",
    "from torch.nn.utils import rnn\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from typing import List\n",
    "from functools import reduce\n",
    "import operator\n",
    "\n",
    "from itertools import chain\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args={\"wandb\": false, \"model\": \"Watchog\", \"unlabeled_train_only\": false, \"context_encoding_type\": \"v0\", \"pool_version\": \"v0.2\", \"random_sample\": false, \"comment\": \"debug\", \"shortcut_name\": \"bert-base-uncased\", \"max_length\": 64, \"adaptive_max_length\": false, \"max_num_col\": 8, \"batch_size\": 3, \"epoch\": 1, \"random_seed\": 4649, \"train_n_seed_cols\": -1, \"num_classes\": 101, \"multi_gpu\": false, \"fp16\": false, \"warmup\": 0.0, \"lr\": 5e-05, \"task\": \"gt-semtab22-dbpedia-all0\", \"colpair\": false, \"metadata\": false, \"from_scratch\": false, \"cl_tag\": \"wikitables/simclr/bert_100000_10_32_256_5e-05_sample_row4,sample_row4_tfidf_entity_column_0.05_0_last.pt\", \"dropout_prob\": 0.5, \"eval_test\": true, \"small_tag\": \"semi1\", \"data_path\": \"/data/zhihao/TU/\", \"pretrained_ckpt_path\": \"/data/zhihao/TU/Watchog/model/\"}\n",
      "gt-semtab22-dbpedia-all0/wikitables-simclr-bert_100000_10_32_256_5e-05_sample_row4,sample_row4_tfidf_entity_column_0.05_0_last.pt_bert-base-uncased-poolsemi1-max_colsv0.2-rand8-bsFalse-ml3-ne64-do10.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4026465/3989646571.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(ckpt_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(augment_op='sample_row4,sample_row4', batch_size=32, data_path='/data/zhihao/TU/TURL/', fp16=True, gpus='0', lm='bert', logdir='/data/zhihao/TU/Watchog/model/', lr=5e-05, max_len=256, mode='simclr', model='Watchog', n_epochs=10, pretrain_data='wikitables', pretrained_model_path='', projector=768, run_id=0, sample_meth='tfidf_entity', save_model=10, single_column=False, size=100000, table_order='column', temperature=0.05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n"
     ]
    }
   ],
   "source": [
    "    device = torch.device(2)\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--wandb\", type=bool, default=False)\n",
    "    parser.add_argument(\"--model\", type=str, default=\"Watchog\")\n",
    "    parser.add_argument(\"--unlabeled_train_only\", type=bool, default=False)\n",
    "    parser.add_argument(\"--context_encoding_type\", type=str, default=\"v0\")\n",
    "    parser.add_argument(\"--pool_version\", type=str, default=\"v0.2\")\n",
    "    parser.add_argument(\"--random_sample\", type=bool, default=False)\n",
    "    parser.add_argument(\"--comment\", type=str, default=\"debug\", help=\"to distinguish the runs\")\n",
    "    parser.add_argument(\n",
    "        \"--shortcut_name\",\n",
    "        default=\"bert-base-uncased\",\n",
    "        type=str,\n",
    "        help=\"Huggingface model shortcut name \",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_length\",\n",
    "        default=64,\n",
    "        type=int,\n",
    "        help=\n",
    "        \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "        \"than this will be truncated, sequences shorter will be padded.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--adaptive_max_length\",\n",
    "        default=False,\n",
    "        type=bool,\n",
    "    )    \n",
    "    parser.add_argument(\n",
    "        \"--max_num_col\",\n",
    "        default=8,\n",
    "        type=int,\n",
    "    )   \n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--batch_size\",\n",
    "        default=3,\n",
    "        type=int,\n",
    "        help=\"Batch size\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--epoch\",\n",
    "        default=1,\n",
    "        type=int,\n",
    "        help=\"Number of epochs for training\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--random_seed\",\n",
    "        default=4649,\n",
    "        type=int,\n",
    "        help=\"Random seed\",\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--train_n_seed_cols\",\n",
    "        default=-1,\n",
    "        type=int,\n",
    "        help=\"number of seeding columns in training\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--num_classes\",\n",
    "        default=78,\n",
    "        type=int,\n",
    "        help=\"Number of classes\",\n",
    "    )\n",
    "    parser.add_argument(\"--multi_gpu\",\n",
    "                        action=\"store_true\",\n",
    "                        default=False,\n",
    "                        help=\"Use multiple GPU\")\n",
    "    parser.add_argument(\"--fp16\",\n",
    "                        action=\"store_true\",\n",
    "                        default=False,\n",
    "                        help=\"Use FP16\")\n",
    "    parser.add_argument(\"--warmup\",\n",
    "                        type=float,\n",
    "                        default=0.,\n",
    "                        help=\"Warmup ratio\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=5e-5, help=\"Learning rate\")\n",
    "    parser.add_argument(\"--task\",\n",
    "                        type=str,\n",
    "                        default='gt-semtab22-dbpedia-all0',\n",
    "                        choices=[\n",
    "                            \"sato0\", \"sato1\", \"sato2\", \"sato3\", \"sato4\",\n",
    "                            \"msato0\", \"msato1\", \"msato2\", \"msato3\", \"msato4\",\n",
    "                            \"gt-dbpedia0\", \"gt-dbpedia1\", \"gt-dbpedia2\", \"gt-dbpedia3\", \"gt-dbpedia4\",\n",
    "                            \"gt-dbpedia-all0\", \"gt-dbpedia-all1\", \"gt-dbpedia-all2\", \"gt-dbpedia-all3\", \"gt-dbpedia-all4\",\n",
    "                            \"gt-schema-all0\", \"gt-schema-all1\", \"gt-schema-all2\", \"gt-schema-all3\", \"gt-schema-all4\",\n",
    "                            \"gt-semtab22-dbpedia\", \"gt-semtab22-dbpedia0\", \"gt-semtab22-dbpedia1\", \"gt-semtab22-dbpedia2\", \"gt-semtab22-dbpedia3\", \"gt-semtab22-dbpedia4\",\n",
    "                            \"gt-semtab22-dbpedia-all\", \"gt-semtab22-dbpedia-all0\", \"gt-semtab22-dbpedia-all1\", \"gt-semtab22-dbpedia-all2\", \"gt-semtab22-dbpedia-all3\", \"gt-semtab22-dbpedia-all4\",\n",
    "                            \"gt-semtab22-schema-class-all\", \"gt-semtab22-schema-property-all\",\n",
    "                            \"turl\", \"turl-re\", \"col-popl-1\", \"col-popl-2\", \"col-popl-3\", \"row-popl\",\n",
    "                            \"col-popl-turl-0\", \"col-popl-turl-1\", \"col-popl-turl-2\",\n",
    "                            \"col-popl-turl-mdonly-0\", \"col-popl-turl-mdonly-1\", \"col-popl-turl-mdonly-2\"\n",
    "                        ],\n",
    "                        help=\"Task names}\")\n",
    "    parser.add_argument(\"--colpair\",\n",
    "                        action=\"store_true\",\n",
    "                        help=\"Use column pair embedding\")\n",
    "    parser.add_argument(\"--metadata\",\n",
    "                        action=\"store_true\",\n",
    "                        help=\"Use column header metadata\")\n",
    "    parser.add_argument(\"--from_scratch\",\n",
    "                        action=\"store_true\",\n",
    "                        help=\"Training from scratch\")\n",
    "    parser.add_argument(\"--cl_tag\",\n",
    "                        type=str,\n",
    "                        default=\"wikitables/simclr/bert_100000_10_32_256_5e-05_sample_row4,sample_row4_tfidf_entity_column_0.05_0_last.pt\",\n",
    "                        help=\"path to the pre-trained file\")\n",
    "    parser.add_argument(\"--dropout_prob\",\n",
    "                        type=float,\n",
    "                        default=0.5)\n",
    "    parser.add_argument(\"--eval_test\",\n",
    "                        action=\"store_true\",\n",
    "                        help=\"evaluate on testset and do not save the model file\")\n",
    "    parser.add_argument(\"--small_tag\",\n",
    "                        type=str,\n",
    "                        default=\"semi1\",\n",
    "                        help=\"e.g., by_table_t5_v1\")\n",
    "    parser.add_argument(\"--data_path\",\n",
    "                        type=str,\n",
    "                        default=\"/data/zhihao/TU/\")\n",
    "    parser.add_argument(\"--pretrained_ckpt_path\",\n",
    "                        type=str,\n",
    "                        default=\"/data/zhihao/TU/Watchog/model/\")    \n",
    "\n",
    "    args = parser.parse_args([])\n",
    "    task = args.task\n",
    "    if args.small_tag != \"\":\n",
    "        args.eval_test = True\n",
    "    \n",
    "    args.num_classes = task_num_class_dict[task]\n",
    "    if args.colpair:\n",
    "        assert \"turl-re\" == task, \"colpair can be only used for Relation Extraction\"\n",
    "    if args.metadata:\n",
    "        assert \"turl-re\" == task or \"turl\" == task, \"metadata can be only used for TURL datasets\"\n",
    "    if \"col-popl\":\n",
    "        # metrics = {\n",
    "        #     \"accuracy\": CategoricalAccuracy(tie_break=True),\n",
    "        # }\n",
    "        if args.train_n_seed_cols != -1:\n",
    "            if \"col-popl\" in task:\n",
    "                assert args.train_n_seed_cols == int(task[-1]),  \"# of seed columns must match\"\n",
    "\n",
    "    print(\"args={}\".format(json.dumps(vars(args))))\n",
    "\n",
    "    max_length = args.max_length\n",
    "    batch_size = args.batch_size\n",
    "    num_train_epochs = args.epoch\n",
    "\n",
    "    shortcut_name = args.shortcut_name\n",
    "\n",
    "    if args.colpair and args.metadata:\n",
    "        taskname = \"{}-colpair-metadata\".format(task)\n",
    "    elif args.colpair:\n",
    "        taskname = \"{}-colpair\".format(task)\n",
    "    elif args.metadata:\n",
    "        taskname = \"{}-metadata\".format(task)\n",
    "    elif args.train_n_seed_cols == -1 and 'popl' in task:\n",
    "        taskname = \"{}-mix\".format(task)\n",
    "    else:\n",
    "        taskname = \"\".join(task)\n",
    "    cv = int(task[-1])\n",
    "\n",
    "    if args.from_scratch:\n",
    "        if \"gt\" in task:\n",
    "            tag_name = \"{}/{}-{}-{}-pool{}-max_cols{}-rand{}-bs{}-ml{}-ne{}-do{}{}\".format(\n",
    "                taskname,  \"{}-fromscratch\".format(shortcut_name), args.small_tag, args.comment, args.pool_version, args.max_num_col, args.random_sample,\n",
    "                batch_size, max_length, num_train_epochs, args.dropout_prob, \n",
    "                '-rs{}'.format(args.random_seed) if args.random_seed != 4649 else '')\n",
    "        else:\n",
    "            tag_name = \"{}/{}-{}-{}-bs{}-ml{}-ne{}-do{}{}\".format(\n",
    "                taskname,  \"{}-fromscratch\".format(shortcut_name), args.small_tag, args.comment, \n",
    "                batch_size, max_length, num_train_epochs, args.dropout_prob, \n",
    "                '-rs{}'.format(args.random_seed) if args.random_seed != 4649 else '')\n",
    "        \n",
    "    else:\n",
    "        if \"gt\" in task:\n",
    "            tag_name = \"{}/{}_{}-pool{}-max_cols{}-rand{}-bs{}-ml{}-ne{}-do{}{}\".format(\n",
    "                taskname, args.cl_tag.replace('/', '-'),  shortcut_name, args.small_tag, args.pool_version, args.max_num_col, args.random_sample,\n",
    "                batch_size, max_length, num_train_epochs, args.dropout_prob,\n",
    "                '-rs{}'.format(args.random_seed) if args.random_seed != 4649 else '')\n",
    "        else:\n",
    "            tag_name = \"{}/{}_{}-{}-bs{}-ml{}-ne{}-do{}{}\".format(\n",
    "                taskname, args.cl_tag.replace('/', '-'),  shortcut_name, args.small_tag,\n",
    "                batch_size, max_length, num_train_epochs, args.dropout_prob,\n",
    "                '-rs{}'.format(args.random_seed) if args.random_seed != 4649 else '')\n",
    "\n",
    "    # if args.eval_test:\n",
    "    #     if args.small_tag != '':\n",
    "    #         tag_name = tag_name.replace('outputs', 'small_outputs')\n",
    "    #         tag_name += '-' + args.small_tag\n",
    "    print(tag_name)\n",
    "    file_path = os.path.join(args.data_path, \"Watchog\", \"outputs\", tag_name)\n",
    "\n",
    "    dirpath = os.path.dirname(file_path)\n",
    "    if not os.path.exists(dirpath):\n",
    "        print(\"{} not exists. Created\".format(dirpath))\n",
    "        os.makedirs(dirpath)\n",
    "    \n",
    "    if args.fp16:\n",
    "        torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = True\n",
    "        \n",
    "      \n",
    "        \n",
    "    # accelerator = Accelerator(mixed_precision=\"no\" if not args.fp16 else \"fp16\")   \n",
    "    ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)\n",
    "    accelerator = Accelerator(mixed_precision=\"no\" if not args.fp16 else \"fp16\", kwargs_handlers=[ddp_kwargs])\n",
    "\n",
    "    device = torch.device(0)\n",
    "    ckpt_path = os.path.join(args.pretrained_ckpt_path, args.cl_tag)\n",
    "    # ckpt_path = '/efs/checkpoints/{}.pt'.format(args.cl_tag)\n",
    "    ckpt = torch.load(ckpt_path, map_location=device)\n",
    "    ckpt_hp = ckpt['hp']\n",
    "    print(ckpt_hp)\n",
    " \n",
    "    setattr(ckpt_hp, 'batch_size', args.batch_size)\n",
    "    setattr(ckpt_hp, 'hidden_dropout_prob', args.dropout_prob)\n",
    "    setattr(ckpt_hp, 'shortcut_name', args.shortcut_name)\n",
    "    setattr(ckpt_hp, 'num_labels', args.num_classes)\n",
    "    \n",
    "    \n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained(shortcut_name)\n",
    "    padder = collate_fn(tokenizer.pad_token_id)\n",
    "    if task == \"turl-re\" and args.colpair:\n",
    "        model = BertForMultiOutputClassification(ckpt_hp, device=device, lm=ckpt['hp'].lm, col_pair='Pair')\n",
    "    elif \"col-popl\" in task:\n",
    "        model = BertForMultiOutputClassificationColPopl(ckpt_hp, device=device, lm=ckpt['hp'].lm, n_seed_cols=int(task[i][-1]), cls_for_md=\"md\" in task)\n",
    "    else:\n",
    "        model = BertForMultiOutputClassification(ckpt_hp, device=device, lm=ckpt['hp'].lm, version=\"v0\", use_attention_mask=True)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3970067/307592194.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  best_state_dict = torch.load(\"/data/zhihao/TU/Watchog/outputs/gt-semtab22-dbpedia-all0/bert-base-uncased-fromscratch-semi1-Repeat@5-AttnMask-UnlabelValid-max-unlabeled@8-poolv0-unlabeled8-randFalse-bs16-ml128-ne50-do0.1_best_f1_macro.pt\", map_location=device)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best_state_dict = torch.load(\"/data/zhihao/TU/Watchog/outputs/gt-semtab22-dbpedia-all0/bert-base-uncased-fromscratch-semi1-AttnMask-max-unlabeled@8-poolv0-unlabeled8-randFalse-bs16-ml128-ne50-do0.1_best_f1_micro.pt\", map_location=device)\n",
    "best_state_dict = torch.load(\"/data/zhihao/TU/Watchog/outputs/gt-semtab22-dbpedia-all0/bert-base-uncased-fromscratch-semi1-Repeat@5-AttnMask-UnlabelValid-max-unlabeled@8-poolv0-unlabeled8-randFalse-bs16-ml128-ne50-do0.1_best_f1_macro.pt\", map_location=device)\n",
    "model.load_state_dict(best_state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GittablesTablewiseIterateDataset(data.Dataset):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            cv: int,\n",
    "            split: str,\n",
    "            src: str,  # train or test\n",
    "            tokenizer: AutoTokenizer,\n",
    "            max_length: int = 256,\n",
    "            gt_only: bool = False,\n",
    "            device: torch.device = None,\n",
    "            base_dirpath: str = \"/data/zhihao/TU/GitTables/semtab_gittables/2022\",\n",
    "            base_tag: str = '', # blank, comma\n",
    "            small_tag: str = \"\",\n",
    "            train_ratio: float = 1.0,\n",
    "            max_unlabeled=8,\n",
    "            adaptive_max_length=True,\n",
    "            random_sample=False, # TODO\n",
    "            train_only=False): # TODO\n",
    "        if device is None:\n",
    "            device = torch.device('cpu')\n",
    "        basename = small_tag+ \"_cv_{}.csv\"\n",
    "    \n",
    "        if split in [\"train\", \"valid\"]:\n",
    "            df_list = []\n",
    "            for i in range(5):\n",
    "                if i == cv:\n",
    "                    continue\n",
    "                filepath = os.path.join(base_dirpath, basename.format(i))\n",
    "                df_list.append(pd.read_csv(filepath))\n",
    "                print(split, i)\n",
    "            df = pd.concat(df_list, axis=0)\n",
    "        else:\n",
    "            # test\n",
    "            filepath = os.path.join(base_dirpath, basename.format(cv))\n",
    "            df = pd.read_csv(filepath)\n",
    "            print(split)\n",
    "\n",
    "\n",
    "        if gt_only:\n",
    "            df = df[df[\"class_id\"] > -1]\n",
    "        if train_only and split != \"train\":\n",
    "            df = df[df[\"class_id\"] > -1]\n",
    "\n",
    "        \n",
    "        data_list = []\n",
    "        \n",
    "        df['class_id'] = df['class_id'].astype(int)\n",
    "        df.drop(df[(df['data'].isna()) & (df['class_id'] == -1)].index, inplace=True)\n",
    "        df['col_idx'] = df['col_idx'].astype(int)\n",
    "        df['data'] = df['data'].astype(str)\n",
    "        \n",
    "        num_tables = len(df.groupby(\"table_id\"))\n",
    "        valid_index = int(num_tables * 0.8)\n",
    "        num_train = int(train_ratio * num_tables * 0.8)        \n",
    "        \n",
    "        # df.drop(df[(df['data'] == '') & (df['class_id'] == -1)].index, inplace=True)\n",
    "        total_num_cols = 0\n",
    "        for i, (index, group_df) in enumerate(df.groupby(\"table_id\")):\n",
    "            if (split == \"train\") and ((i >= num_train) or (i >= valid_index)):\n",
    "                break\n",
    "            if split == \"valid\" and i < valid_index:\n",
    "                continue\n",
    "            #     break\n",
    "            labeled_columns = group_df[group_df['class_id'] > -1]\n",
    "            unlabeled_columns = group_df[group_df['class_id'] == -1]\n",
    "            num_unlabeled = min(max(max_unlabeled-len(labeled_columns), 0), len(unlabeled_columns)) if max_unlabeled > 0 else len(unlabeled_columns)\n",
    "            unlabeled_columns = unlabeled_columns.sample(num_unlabeled) if random_sample else unlabeled_columns[0:num_unlabeled]\n",
    "            # group_df = pd.concat([group_df[group_df['class_id'] > -1], unlabeled_columns.sample(min(10-len(labeled_columns), len(unlabeled_columns)))])\n",
    "            # group_df = pd.concat([group_df[group_df['class_id'] > -1], unlabeled_columns[0:min(max(10-len(labeled_columns), 0), len(unlabeled_columns))]])\n",
    "            group_df = pd.concat([group_df[group_df['class_id'] > -1], unlabeled_columns]) # TODO\n",
    "            group_df.sort_values(by=['col_idx'], inplace=True)\n",
    "\n",
    "            if max_length <= 128 and adaptive_max_length:\n",
    "                cur_maxlen = min(max_length, 512 // len(list(group_df[\"class_id\"].values)) - 1)\n",
    "            else:\n",
    "                cur_maxlen = max_length\n",
    "                \n",
    "            token_ids_list = group_df[\"data\"].apply(lambda x: tokenizer.encode(\n",
    "                tokenizer.cls_token + \" \" + x, add_special_tokens=False, max_length=cur_maxlen, truncation=True)).tolist(\n",
    "                )\n",
    "            token_ids = torch.LongTensor(reduce(operator.add,\n",
    "                                                token_ids_list)).to(device)\n",
    "            for col_i in range(len(token_ids_list)):\n",
    "                if group_df[\"class_id\"].values[col_i] == -1:\n",
    "                    continue\n",
    "                target_col_mask = []\n",
    "                cls_index_value = 0\n",
    "                context_id = 1\n",
    "                for col_j in range(len(token_ids_list)):\n",
    "                    if col_j == col_i:\n",
    "                        target_col_mask += [0] * len(token_ids_list[col_j])\n",
    "                    else:\n",
    "                        target_col_mask += [context_id] * len(token_ids_list[col_j])\n",
    "                        context_id += 1\n",
    "                    if col_j < col_i:\n",
    "                        cls_index_value += len(token_ids_list[col_j])\n",
    "                cls_index_list = [cls_index_value] \n",
    "                for cls_index in cls_index_list:\n",
    "                    assert token_ids[\n",
    "                        cls_index] == tokenizer.cls_token_id, \"cls_indexes validation\"\n",
    "                cls_indexes = torch.LongTensor(cls_index_list).to(device)\n",
    "                class_ids = torch.LongTensor(\n",
    "                    [group_df[\"class_id\"].values[col_i]]).to(device)\n",
    "                target_col_mask = torch.LongTensor(target_col_mask).to(device)\n",
    "                data_list.append(\n",
    "                    [index,\n",
    "                    len(group_df), token_ids, class_ids, cls_indexes, target_col_mask])                \n",
    "        print(split, len(data_list))\n",
    "        self.table_df = pd.DataFrame(data_list,\n",
    "                                     columns=[\n",
    "                                         \"table_id\", \"num_col\", \"data_tensor\",\n",
    "                                         \"label_tensor\", \"cls_indexes\", \"target_col_mask\"\n",
    "                                     ])\n",
    "        \"\"\"\n",
    "        # NOTE: msato contains a small portion of single-col tables. keep it to be consistent.  \n",
    "        if multicol_only:\n",
    "            # Check\n",
    "            num_all_tables = len(self.table_df)\n",
    "            self.table_df = self.table_df[self.table_df[\"num_col\"] > 1]\n",
    "            assert len(self.table_df) == num_all_tables\n",
    "        \"\"\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.table_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"data\": self.table_df.iloc[idx][\"data_tensor\"],\n",
    "            \"label\": self.table_df.iloc[idx][\"label_tensor\"],\n",
    "            \"cls_indexes\": self.table_df.iloc[idx][\"cls_indexes\"],\n",
    "            \"target_col_mask\": self.table_df.iloc[idx][\"target_col_mask\"],\n",
    "        }\n",
    "        #\"idx\": torch.LongTensor([idx])}\n",
    "        #\"cls_indexes\": self.table_df.iloc[idx][\"cls_indexes\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(pad_token_id, data_only=True):\n",
    "    '''padder for input batch'''\n",
    "\n",
    "    def padder(samples):    \n",
    "        data = torch.nn.utils.rnn.pad_sequence(\n",
    "            [sample[\"data\"] for sample in samples], padding_value=pad_token_id)\n",
    "        if not data_only:\n",
    "            label = torch.nn.utils.rnn.pad_sequence(\n",
    "                [sample[\"label\"] for sample in samples], padding_value=-1)\n",
    "        else:\n",
    "            label = torch.cat([sample[\"label\"] for sample in samples])\n",
    "        batch = {\"data\": data, \"label\": label}\n",
    "        if \"idx\" in samples[0]:\n",
    "            batch[\"idx\"] = [sample[\"idx\"] for sample in samples]\n",
    "        if \"cls_indexes\" in samples[0]:\n",
    "            cls_indexes = torch.nn.utils.rnn.pad_sequence(\n",
    "                [sample[\"cls_indexes\"] for sample in samples], padding_value=0)\n",
    "            batch[\"cls_indexes\"] = cls_indexes\n",
    "        if \"target_col_mask\" in samples[0]:\n",
    "            target_col_mask = torch.nn.utils.rnn.pad_sequence(\n",
    "                [sample[\"target_col_mask\"] for sample in samples], padding_value=-1)\n",
    "            batch[\"target_col_mask\"] = target_col_mask\n",
    "        if \"table_embedding\" in samples[0]:\n",
    "            table_embeddings = [sample[\"table_embedding\"] for sample in samples]\n",
    "            batch[\"table_embedding\"] = torch.stack(table_embeddings, dim=0)\n",
    "        return batch\n",
    "        \n",
    "    return padder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = None\n",
    "test_dataset_iter = GittablesTablewiseIterateDataset(cv=cv,\n",
    "                            split=\"test\", src=src,\n",
    "                            tokenizer=tokenizer,\n",
    "                            max_length=max_length,\n",
    "                            gt_only='all' not in task,\n",
    "                            device=device,\n",
    "                            base_dirpath=os.path.join(args.data_path, \"GitTables/semtab_gittables/2022\"),\n",
    "                            small_tag=\"semi1\")\n",
    "padder = collate_fn(tokenizer.pad_token_id)\n",
    "test_dataloader_iter = DataLoader(test_dataset_iter,\n",
    "                                batch_size=1,\n",
    "                            #   collate_fn=collate_fn)\n",
    "                            collate_fn=padder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_iter = GittablesTablewiseIterateDataset(cv=cv,\n",
    "                            split=\"train\", src=src,\n",
    "                            tokenizer=tokenizer,\n",
    "                            max_length=max_length,\n",
    "                            gt_only='all' not in task,\n",
    "                            device=device,\n",
    "                            base_dirpath=os.path.join(args.data_path, \"GitTables/semtab_gittables/2022\"),\n",
    "                            small_tag=\"semi1\")\n",
    "padder = collate_fn(tokenizer.pad_token_id)\n",
    "train_dataloader_iter = DataLoader(train_dataset_iter,\n",
    "                                batch_size=1,\n",
    "                            #   collate_fn=collate_fn)\n",
    "                            collate_fn=padder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = []\n",
    "for batch_idx, batch in enumerate(train_dataloader_iter):\n",
    "    cls_indexes = torch.LongTensor([[0, batch[\"cls_indexes\"].cpu().item()]]).to(device)\n",
    "    target_col_mask = batch[\"target_col_mask\"].T\n",
    "    num_cols.append(batch[\"target_col_mask\"].max().item()+1)\n",
    "num_cols = torch.tensor(num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.histplot(num_cols.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = None\n",
    "valid_dataset_iter = GittablesTablewiseIterateDataset(cv=cv,\n",
    "                            split=\"valid\", src=src,\n",
    "                            tokenizer=tokenizer,\n",
    "                            max_length=max_length,\n",
    "                            gt_only='all' not in task,\n",
    "                            device=device,\n",
    "                            base_dirpath=os.path.join(args.data_path, \"GitTables/semtab_gittables/2022\"),\n",
    "                            small_tag=\"semi1\")\n",
    "padder = collate_fn(tokenizer.pad_token_id)\n",
    "valid_dataloader_iter = DataLoader(valid_dataset_iter,\n",
    "                                batch_size=1,\n",
    "                            #   collate_fn=collate_fn)\n",
    "                            collate_fn=padder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ts_micro_f1=0.5355, ts_macro_f1=0.2745\n",
      "ts_micro_f1=0.5351, ts_macro_f1=0.2745\n",
      "ts_micro_f1=1.0000, ts_macro_f1=1.0000\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "model = model.to(device)\n",
    "ft_embs_test = []\n",
    "labels_test = []\n",
    "logits_test = []\n",
    "num_cols = []\n",
    "for batch_idx, batch in enumerate(test_dataloader_iter):\n",
    "    cls_indexes = torch.LongTensor([[0, batch[\"cls_indexes\"].cpu().item()]]).to(device)\n",
    "    target_col_mask = batch[\"target_col_mask\"].T\n",
    "    logits, embs = model(batch[\"data\"].T, cls_indexes=cls_indexes, get_enc=True)\n",
    "    labels_test.append(batch[\"label\"].cpu())\n",
    "    logits_test.append(logits.detach().cpu())\n",
    "    num_cols.append(batch[\"target_col_mask\"].max().item())\n",
    "    ft_embs_test.append(embs.detach().cpu().reshape(1, -1))\n",
    "labels_test = torch.cat(labels_test, dim=0)\n",
    "logits_test = torch.stack(logits_test, dim=0)\n",
    "preds_test = torch.argmax(logits_test, dim=1)\n",
    "num_cols = torch.tensor(num_cols)\n",
    "ft_embs_test = torch.cat(ft_embs_test, dim=0)\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "mask = num_cols > 0\n",
    "ts_pred_list = logits_test.argmax(\n",
    "                            1).cpu().detach().numpy().tolist()\n",
    "ts_micro_f1 = f1_score(labels_test.reshape(-1).numpy().tolist(),\n",
    "                    ts_pred_list,\n",
    "                    average=\"micro\")\n",
    "ts_macro_f1 = f1_score(labels_test.reshape(-1).numpy().tolist(),\n",
    "                    ts_pred_list,\n",
    "                    average=\"macro\")\n",
    "full_f1_init = f1_score(labels_test.reshape(-1).numpy().tolist(),\n",
    "                    ts_pred_list,\n",
    "                    average=None)\n",
    "print(\"ts_micro_f1={:.4f}, ts_macro_f1={:.4f}\".format(ts_micro_f1, ts_macro_f1))\n",
    "ts_pred_list = logits_test.argmax(\n",
    "                            1).cpu().detach()[mask].numpy().tolist()\n",
    "ts_micro_f1 = f1_score(labels_test.reshape(-1)[mask].numpy().tolist(),\n",
    "                    ts_pred_list,\n",
    "                    average=\"micro\")\n",
    "ts_macro_f1 = f1_score(labels_test.reshape(-1)[mask].numpy().tolist(),\n",
    "                    ts_pred_list,\n",
    "                    average=\"macro\")\n",
    "print(\"ts_micro_f1={:.4f}, ts_macro_f1={:.4f}\".format(ts_micro_f1, ts_macro_f1))\n",
    "ts_pred_list = logits_test.argmax(\n",
    "                            1).cpu().detach()[~mask].numpy().tolist()\n",
    "ts_micro_f1 = f1_score(labels_test.reshape(-1)[~mask].numpy().tolist(),\n",
    "                    ts_pred_list,\n",
    "                    average=\"micro\")\n",
    "ts_macro_f1 = f1_score(labels_test.reshape(-1)[~mask].numpy().tolist(),\n",
    "                    ts_pred_list,\n",
    "                    average=\"macro\")\n",
    "print(\"ts_micro_f1={:.4f}, ts_macro_f1={:.4f}\".format(ts_micro_f1, ts_macro_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_col in num_cols.unique().tolist():\n",
    "    mask = num_cols == n_col\n",
    "    ts_pred_list = logits_test.argmax(\n",
    "                                1).cpu().detach()[mask].numpy().tolist()\n",
    "    ts_micro_f1 = f1_score(labels_test.reshape(-1)[mask].numpy().tolist(),\n",
    "                        ts_pred_list,\n",
    "                        average=\"micro\")\n",
    "    ts_macro_f1 = f1_score(labels_test.reshape(-1)[mask].numpy().tolist(),\n",
    "                        ts_pred_list,\n",
    "                        average=\"macro\")\n",
    "    print(\"num_cols={} ts_micro_f1={:.4f}, ts_macro_f1={:.4f}\".format(n_col, ts_micro_f1, ts_macro_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_embedding = nn.Embedding(2, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_embedding(torch.ones(32).long()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_f1_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model = model.to(device)\n",
    "ft_embs_test = []\n",
    "labels_test = []\n",
    "logits_test = []\n",
    "num_cols = []\n",
    "for batch_idx, batch in enumerate(valid_dataloader_iter):\n",
    "    cls_indexes = torch.LongTensor([[0, batch[\"cls_indexes\"].cpu().item()]]).to(device)\n",
    "    target_col_mask = batch[\"target_col_mask\"].T\n",
    "    logits, embs = model(batch[\"data\"].T, cls_indexes=cls_indexes, get_enc=True)\n",
    "    labels_test.append(batch[\"label\"].cpu())\n",
    "    logits_test.append(logits.detach().cpu())\n",
    "    num_cols.append(batch[\"target_col_mask\"].max().item())\n",
    "    ft_embs_test.append(embs.detach().cpu().reshape(1, -1))\n",
    "labels_test = torch.cat(labels_test, dim=0)\n",
    "logits_test = torch.stack(logits_test, dim=0)\n",
    "preds_test = torch.argmax(logits_test, dim=1)\n",
    "num_cols = torch.tensor(num_cols)\n",
    "ft_embs_test = torch.cat(ft_embs_test, dim=0)\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "mask = num_cols > 0\n",
    "ts_pred_list = logits_test.argmax(\n",
    "                            1).cpu().detach().numpy().tolist()\n",
    "ts_micro_f1 = f1_score(labels_test.reshape(-1).numpy().tolist(),\n",
    "                    ts_pred_list,\n",
    "                    average=\"micro\")\n",
    "ts_macro_f1 = f1_score(labels_test.reshape(-1).numpy().tolist(),\n",
    "                    ts_pred_list,\n",
    "                    average=\"macro\")\n",
    "print(\"ts_micro_f1={:.4f}, ts_macro_f1={:.4f}\".format(ts_micro_f1, ts_macro_f1))\n",
    "ts_pred_list = logits_test.argmax(\n",
    "                            1).cpu().detach()[mask].numpy().tolist()\n",
    "ts_micro_f1 = f1_score(labels_test.reshape(-1)[mask].numpy().tolist(),\n",
    "                    ts_pred_list,\n",
    "                    average=\"micro\")\n",
    "ts_macro_f1 = f1_score(labels_test.reshape(-1)[mask].numpy().tolist(),\n",
    "                    ts_pred_list,\n",
    "                    average=\"macro\")\n",
    "print(\"ts_micro_f1={:.4f}, ts_macro_f1={:.4f}\".format(ts_micro_f1, ts_macro_f1))\n",
    "ts_pred_list = logits_test.argmax(\n",
    "                            1).cpu().detach()[~mask].numpy().tolist()\n",
    "ts_micro_f1 = f1_score(labels_test.reshape(-1)[~mask].numpy().tolist(),\n",
    "                    ts_pred_list,\n",
    "                    average=\"micro\")\n",
    "ts_macro_f1 = f1_score(labels_test.reshape(-1)[~mask].numpy().tolist(),\n",
    "                    ts_pred_list,\n",
    "                    average=\"macro\")\n",
    "print(\"ts_micro_f1={:.4f}, ts_macro_f1={:.4f}\".format(ts_micro_f1, ts_macro_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_freq = torch.zeros(args.num_classes)\n",
    "for batch_idx, batch in enumerate(train_dataloader_iter):\n",
    "    class_freq[batch[\"label\"].item()] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def reweight_logits(logits, class_weights):\n",
    "#     # Reweight the logits by multiplying with class weights\n",
    "#     reweighted_logits = logits * torch.sqrt(class_weights)\n",
    "    \n",
    "#     # Apply softmax to the reweighted logits\n",
    "#     reweighted_probs = F.softmax(reweighted_logits, dim=-1)\n",
    "    \n",
    "#     return reweighted_probs\n",
    "def reweight_logits(logits, class_weights):\n",
    "    # Step 1: Apply exp(logits)\n",
    "    exp_logits = torch.exp(logits)\n",
    "    \n",
    "    # Step 2: Multiply by class weights\n",
    "    # reweighted_exp = exp_logits * torch.sqrt(class_weights)\n",
    "    reweighted_exp = exp_logits * class_weights\n",
    "    # Step 3: Normalize to get valid probabilities (like softmax)\n",
    "    reweighted_probs = reweighted_exp / reweighted_exp.sum(dim=-1, keepdim=True)\n",
    "    \n",
    "    return reweighted_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# brute force perumate, find uncertain init prediction, weighted msp\n",
    "import itertools\n",
    "\n",
    "def is_sublist(A, B):\n",
    "    it = iter(B)\n",
    "    return all(x in it for x in A)\n",
    "def get_permutation(x):\n",
    "    new = []\n",
    "    x = x.tolist()\n",
    "    if len(x) == 1:\n",
    "        x = x[0]\n",
    "    for k in x:\n",
    "        if k not in new:\n",
    "            new.append(k)\n",
    "    return new\n",
    "    \n",
    "model.load_state_dict(best_state_dict, strict=False)\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "change_log = []\n",
    "score_init = []\n",
    "score_best = [] \n",
    "idx_list = []\n",
    "uncertain_init_mask = []\n",
    "uncertain_target_mask = []\n",
    "correct_init_mask = []\n",
    "correct_target_mask = []\n",
    "correct_permutation_mask = []\n",
    "correct_permutation_confident_mask = []\n",
    "correct_permutation_confident_scores = []\n",
    "correct_msp_mask = []\n",
    "init_target_mask = []\n",
    "ood_score_init_list = []\n",
    "ood_score_target_list = []\n",
    "ood_score_final_list = []\n",
    "msp_predicts = []\n",
    "max_col_length = 3\n",
    "\n",
    "alpha = 0\n",
    "# class_weights = (1.0 / class_freq) ** alpha\n",
    "# debias_threshold = 1.0\n",
    "# # Normalize the weights\n",
    "# class_weights /= class_weights.sum()\n",
    "for threshold in [0.9]:\n",
    "    print(f\"*********************Threshold: {threshold}****************************\")\n",
    "    ft_embs_test = []\n",
    "    labels_test = []\n",
    "    logits_test = []\n",
    "    log = defaultdict(list)\n",
    "    num_cols = []\n",
    "    corrected = 0\n",
    "    total_mistakes = 0\n",
    "\n",
    "    for batch_idx, batch in enumerate(test_dataloader_iter):\n",
    "        target_col_mask = batch[\"target_col_mask\"].T\n",
    "        if 1 in target_col_mask:\n",
    "            idx_list.append(batch_idx)\n",
    "\n",
    "            cls_indexes = torch.LongTensor([[0, batch[\"cls_indexes\"].cpu().item()]]).to(device)\n",
    "            logits = model(batch[\"data\"].T, cls_indexes=cls_indexes,)\n",
    "            # logits = reweight_logits(logits.detach().cpu(), class_weights)\n",
    "            logits = F.softmax(logits, dim=-1)\n",
    "            num_cols.append(batch[\"target_col_mask\"].max().item())         \n",
    "            ood_score_init = logits.max().item()\n",
    "            ood_score_init_list.append(ood_score_init)\n",
    "            uncertain_init_mask.append(ood_score_init < threshold)\n",
    "            predict_init = logits.argmax().item()\n",
    "            correct_init_mask.append(predict_init == batch[\"label\"].item())\n",
    "            init_permutation_i = get_permutation(target_col_mask)\n",
    "            \n",
    "            correct_permutation = False\n",
    "            correct_permutation_ood_score = 0.0\n",
    "            correct_permutation_confident = False\n",
    "            ood_score_max = max_msp = 0\n",
    "            max_msp_debiased = 0\n",
    "            debias_class = []\n",
    "            col_idx_set = target_col_mask.unique().tolist()\n",
    "            assert -1 not in col_idx_set\n",
    "            for r in range(len(col_idx_set)-1, len(col_idx_set)//2, -1):\n",
    "                for subset in itertools.combinations(col_idx_set, r):\n",
    "                    if 0 not in subset and r != 1:\n",
    "                        continue\n",
    "                    for x in itertools.permutations(subset):\n",
    "                        if not is_sublist(x, init_permutation_i):\n",
    "                            continue\n",
    "                        new_batch_data = []\n",
    "                        for col_i in x:\n",
    "                            if col_i == 0:\n",
    "                                if len(new_batch_data) == 0:\n",
    "                                    cls_indexes_value = 0\n",
    "                                else:\n",
    "                                    cls_indexes_value = sum([len(new_batch_data[i]) for i in range(len(new_batch_data))])\n",
    "                            new_batch_data.append(batch[\"data\"].T[target_col_mask==col_i])\n",
    "                        if 0 not in x:\n",
    "                            cls_indexes_value = 0\n",
    "                        new_batch_data = torch.cat(new_batch_data, dim=-1).reshape(1, -1)\n",
    "                        cls_indexes = torch.tensor([0, cls_indexes_value]).reshape(1, -1).to(device)\n",
    "                        logits_temp = model(new_batch_data, cls_indexes=cls_indexes,)\n",
    "                        logits_temp = F.softmax(logits_temp, dim=-1)\n",
    "                        # logits_temp = reweight_logits(logits_temp.detach().cpu(), class_weights)\n",
    "                        msp_temp = logits_temp.max().item()\n",
    "                        predict_temp = logits_temp.argmax().item()\n",
    "                        # if 0 not in x and msp_temp > debias_threshold and (predict_temp!=predict_target or predict_temp!=predict_init):\n",
    "                        #     debias_class.append(predict_temp)\n",
    "                        #     continue\n",
    "                        # print(x, msp_temp, predict_temp)\n",
    "                        # if msp_temp > max_msp and 0 in x and predict_temp not in debias_class:\n",
    "                        if msp_temp > max_msp and 0 in x:\n",
    "                            max_msp = msp_temp\n",
    "                            msp_predict = predict_temp\n",
    "                        if predict_temp == batch[\"label\"].item():\n",
    "                            correct_permutation = True\n",
    "                            if msp_temp > correct_permutation_ood_score:\n",
    "                                correct_permutation_ood_score = msp_temp\n",
    "            msp_predicts.append(msp_predict)\n",
    "            ood_score_final_list.append(max_msp)\n",
    "            correct_permutation_mask.append(correct_permutation)\n",
    "            correct_permutation_confident_scores.append(correct_permutation_ood_score)\n",
    "            correct_msp_mask.append(msp_predict == batch[\"label\"].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "idx_list = torch.tensor(idx_list).reshape(-1)\n",
    "uncertain_init_mask = torch.tensor(uncertain_init_mask).reshape(-1)\n",
    "uncertain_target_mask = torch.tensor(uncertain_target_mask).reshape(-1)\n",
    "correct_init_mask = torch.tensor(correct_init_mask).reshape(-1)\n",
    "correct_target_mask = torch.tensor(correct_target_mask).reshape(-1)\n",
    "correct_permutation_mask = torch.tensor(correct_permutation_mask).reshape(-1)\n",
    "correct_permutation_confident_scores = torch.tensor(correct_permutation_confident_scores).reshape(-1)\n",
    "correct_msp_mask = torch.tensor(correct_msp_mask).reshape(-1)\n",
    "init_target_mask = torch.tensor(init_target_mask).reshape(-1)\n",
    "ood_score_init_list = torch.tensor(ood_score_init_list).reshape(-1)\n",
    "ood_score_target_list = torch.tensor(ood_score_target_list).reshape(-1)\n",
    "ood_score_final_list = torch.tensor(ood_score_final_list).reshape(-1)\n",
    "msp_predicts = torch.tensor(msp_predicts).reshape(-1)\n",
    "print(len(idx_list), len(init_target_mask), len(correct_init_mask), len(correct_target_mask), len(correct_permutation_mask))\n",
    "torch.save({\"idx_list\": idx_list, \"uncertain_init_mask\": uncertain_init_mask, \"uncertain_target_mask\": uncertain_target_mask, \"correct_init_mask\": correct_init_mask, \"correct_target_mask\": correct_target_mask, \"correct_permutation_mask\": correct_permutation_mask, \"correct_msp_mask\": correct_msp_mask, \"init_target_mask\": init_target_mask, \"ood_score_init_list\": ood_score_init_list, \"ood_score_target_list\": ood_score_target_list, \"ood_score_final_list\": ood_score_final_list}, f\"./results/brute_force_permutation_alpha@{alpha}_drop.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# brute force perumate, find uncertain init prediction, weighted msp\n",
    "import itertools\n",
    "\n",
    "def is_sublist(A, B):\n",
    "    it = iter(B)\n",
    "    return all(x in it for x in A)\n",
    "def get_permutation(x):\n",
    "    new = []\n",
    "    x = x.tolist()\n",
    "    if len(x) == 1:\n",
    "        x = x[0]\n",
    "    for k in x:\n",
    "        if k not in new:\n",
    "            new.append(k)\n",
    "    return new\n",
    "    \n",
    "model.load_state_dict(best_state_dict, strict=False)\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "change_log = []\n",
    "score_init = []\n",
    "score_best = [] \n",
    "idx_list = []\n",
    "uncertain_init_mask = []\n",
    "uncertain_target_mask = []\n",
    "correct_init_mask = []\n",
    "correct_target_mask = []\n",
    "correct_permutation_mask = []\n",
    "correct_permutation_confident_mask = []\n",
    "correct_permutation_confident_scores = []\n",
    "correct_msp_mask = []\n",
    "init_target_mask = []\n",
    "ood_score_init_list = []\n",
    "ood_score_target_list = []\n",
    "ood_score_final_list = []\n",
    "msp_predicts = []\n",
    "max_col_length = 3\n",
    "\n",
    "alpha = 0.25\n",
    "class_weights = (1.0 / class_freq) ** alpha\n",
    "debias_threshold = 1.0\n",
    "# Normalize the weights\n",
    "class_weights /= class_weights.sum()\n",
    "for threshold in [0.9]:\n",
    "    print(f\"*********************Threshold: {threshold}****************************\")\n",
    "    ft_embs_test = []\n",
    "    labels_test = []\n",
    "    logits_test = []\n",
    "    log = defaultdict(list)\n",
    "    num_cols = []\n",
    "    corrected = 0\n",
    "    total_mistakes = 0\n",
    "\n",
    "    for batch_idx, batch in enumerate(test_dataloader_iter):\n",
    "        target_col_mask = batch[\"target_col_mask\"].T\n",
    "        if 1 in target_col_mask:\n",
    "            idx_list.append(batch_idx)\n",
    "\n",
    "            cls_indexes = torch.LongTensor([[0, batch[\"cls_indexes\"].cpu().item()]]).to(device)\n",
    "            logits = model(batch[\"data\"].T, cls_indexes=cls_indexes,)\n",
    "            logits = reweight_logits(logits.detach().cpu(), class_weights)\n",
    "            num_cols.append(batch[\"target_col_mask\"].max().item())         \n",
    "            ood_score_init = logits.max().item()\n",
    "            ood_score_init_list.append(ood_score_init)\n",
    "            uncertain_init_mask.append(ood_score_init < threshold)\n",
    "            predict_init = logits.argmax().item()\n",
    "            correct_init_mask.append(predict_init == batch[\"label\"].item())\n",
    "            init_permutation_i = get_permutation(target_col_mask)\n",
    "            \n",
    "            correct_permutation = False\n",
    "            correct_permutation_ood_score = 0.0\n",
    "            correct_permutation_confident = False\n",
    "            ood_score_max = max_msp = 0\n",
    "            max_msp_debiased = 0\n",
    "            debias_class = []\n",
    "            col_idx_set = target_col_mask.unique().tolist()\n",
    "            assert -1 not in col_idx_set\n",
    "            for r in range(len(col_idx_set)-1, len(col_idx_set)//2, -1):\n",
    "                for subset in itertools.combinations(col_idx_set, r):\n",
    "                    if 0 not in subset and r != 1:\n",
    "                        continue\n",
    "                    for x in itertools.permutations(subset):\n",
    "                        if not is_sublist(x, init_permutation_i):\n",
    "                            continue\n",
    "                        new_batch_data = []\n",
    "                        for col_i in x:\n",
    "                            if col_i == 0:\n",
    "                                if len(new_batch_data) == 0:\n",
    "                                    cls_indexes_value = 0\n",
    "                                else:\n",
    "                                    cls_indexes_value = sum([len(new_batch_data[i]) for i in range(len(new_batch_data))])\n",
    "                            new_batch_data.append(batch[\"data\"].T[target_col_mask==col_i])\n",
    "                        if 0 not in x:\n",
    "                            cls_indexes_value = 0\n",
    "                        new_batch_data = torch.cat(new_batch_data, dim=-1).reshape(1, -1)\n",
    "                        cls_indexes = torch.tensor([0, cls_indexes_value]).reshape(1, -1).to(device)\n",
    "                        logits_temp = model(new_batch_data, cls_indexes=cls_indexes,)\n",
    "                        logits_temp = reweight_logits(logits_temp.detach().cpu(), class_weights)\n",
    "                        msp_temp = logits_temp.max().item()\n",
    "                        predict_temp = logits_temp.argmax().item()\n",
    "                        # if 0 not in x and msp_temp > debias_threshold and (predict_temp!=predict_target or predict_temp!=predict_init):\n",
    "                        #     debias_class.append(predict_temp)\n",
    "                        #     continue\n",
    "                        # print(x, msp_temp, predict_temp)\n",
    "                        # if msp_temp > max_msp and 0 in x and predict_temp not in debias_class:\n",
    "                        if msp_temp > max_msp and 0 in x:\n",
    "                            max_msp = msp_temp\n",
    "                            msp_predict = predict_temp\n",
    "                        if predict_temp == batch[\"label\"].item():\n",
    "                            correct_permutation = True\n",
    "                            if msp_temp > correct_permutation_ood_score:\n",
    "                                correct_permutation_ood_score = msp_temp\n",
    "            msp_predicts.append(msp_predict)\n",
    "            ood_score_final_list.append(max_msp)\n",
    "            correct_permutation_mask.append(correct_permutation)\n",
    "            correct_permutation_confident_scores.append(correct_permutation_ood_score)\n",
    "            correct_msp_mask.append(msp_predict == batch[\"label\"].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "idx_list = torch.tensor(idx_list).reshape(-1)\n",
    "uncertain_init_mask = torch.tensor(uncertain_init_mask).reshape(-1)\n",
    "uncertain_target_mask = torch.tensor(uncertain_target_mask).reshape(-1)\n",
    "correct_init_mask = torch.tensor(correct_init_mask).reshape(-1)\n",
    "correct_target_mask = torch.tensor(correct_target_mask).reshape(-1)\n",
    "correct_permutation_mask = torch.tensor(correct_permutation_mask).reshape(-1)\n",
    "correct_permutation_confident_scores = torch.tensor(correct_permutation_confident_scores).reshape(-1)\n",
    "correct_msp_mask = torch.tensor(correct_msp_mask).reshape(-1)\n",
    "init_target_mask = torch.tensor(init_target_mask).reshape(-1)\n",
    "ood_score_init_list = torch.tensor(ood_score_init_list).reshape(-1)\n",
    "ood_score_target_list = torch.tensor(ood_score_target_list).reshape(-1)\n",
    "ood_score_final_list = torch.tensor(ood_score_final_list).reshape(-1)\n",
    "msp_predicts = torch.tensor(msp_predicts).reshape(-1)\n",
    "print(len(idx_list), len(init_target_mask), len(correct_init_mask), len(correct_target_mask), len(correct_permutation_mask))\n",
    "torch.save({\"idx_list\": idx_list, \"uncertain_init_mask\": uncertain_init_mask, \"uncertain_target_mask\": uncertain_target_mask, \"correct_init_mask\": correct_init_mask, \"correct_target_mask\": correct_target_mask, \"correct_permutation_mask\": correct_permutation_mask, \"correct_msp_mask\": correct_msp_mask, \"init_target_mask\": init_target_mask, \"ood_score_init_list\": ood_score_init_list, \"ood_score_target_list\": ood_score_target_list, \"ood_score_final_list\": ood_score_final_list}, f\"./results/brute_force_permutation_alpha@{alpha}_drop.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighted, only uncertain init\n",
    "\n",
    "for threshold in [0.8, 0.9, 0.99]:\n",
    "    correct_permutation_confident_mask = correct_permutation_confident_scores>threshold\n",
    "    uncertain_init_mask = ood_score_init_list < threshold\n",
    "    uncertain_target_mask = ood_score_target_list < threshold\n",
    "    condition_mask = ~uncertain_init_mask\n",
    "    print(f\"*********************Threshold: {threshold}****************************\")\n",
    "    print(\"Init\")\n",
    "    # init prediction\n",
    "    print((condition_mask&correct_init_mask).sum().item(), \n",
    "        (condition_mask).sum().item(), \n",
    "        (condition_mask&correct_init_mask).sum().item()/(condition_mask).sum().item())\n",
    "    # init prediction\n",
    "    print((~condition_mask&correct_init_mask).sum().item(), \n",
    "        (~condition_mask).sum().item(), \n",
    "       (~condition_mask&correct_init_mask).sum().item()/(~condition_mask).sum().item())\n",
    "    print(\"MSP\")\n",
    "    # msp prediction\n",
    "    print((condition_mask&correct_msp_mask).sum().item(), \n",
    "        (condition_mask).sum().item(), \n",
    "        (condition_mask&correct_msp_mask).sum().item()/(condition_mask).sum().item())\n",
    "    # msp prediction\n",
    "    print((~condition_mask&correct_msp_mask).sum().item(), \n",
    "        (~condition_mask).sum().item(), \n",
    "        (~condition_mask&correct_msp_mask).sum().item()/(~condition_mask).sum().item())\n",
    "    print(\"MSP & Init\")\n",
    "    print(((condition_mask&correct_init_mask)&(condition_mask&correct_msp_mask)).sum().item())\n",
    "    # print(((~(init_target_mask&(~uncertain_init_mask & ~uncertain_target_mask))& correct_msp_mask)&(~(init_target_mask&(~uncertain_init_mask & ~uncertain_target_mask))& correct_init_mask)).sum().item())\n",
    "    print(((condition_mask&correct_init_mask)|(condition_mask&correct_msp_mask)).sum().item(),\n",
    "          (condition_mask).sum().item(), \n",
    "          ((condition_mask&correct_init_mask)|(condition_mask&correct_msp_mask)).sum().item()/(condition_mask).sum().item())\n",
    "    print(((~condition_mask&correct_init_mask)|(~condition_mask&correct_msp_mask)).sum().item(),\n",
    "          (~condition_mask).sum().item(), \n",
    "          ((~condition_mask&correct_init_mask)|(~condition_mask&correct_msp_mask)).sum().item()/(~condition_mask).sum().item())\n",
    "    print(\"Permutation\")\n",
    "    print((condition_mask&correct_permutation_mask).sum().item(), \n",
    "        (condition_mask).sum().item(), \n",
    "        (condition_mask&correct_permutation_mask).sum().item()/(condition_mask).sum().item())\n",
    "    print((~condition_mask&correct_permutation_mask).sum().item(), \n",
    "         (~condition_mask).sum().item(), \n",
    "        (~condition_mask&correct_permutation_mask).sum().item()/(~condition_mask).sum().item())\n",
    "    print(\"Permutation Confident\")\n",
    "    print((condition_mask&correct_permutation_confident_mask).sum().item(), \n",
    "        (condition_mask).sum().item(), \n",
    "        (condition_mask&correct_permutation_confident_mask).sum().item()/(condition_mask).sum().item())\n",
    "    print((~condition_mask&correct_permutation_confident_mask).sum().item(),\n",
    "            (~condition_mask).sum().item(), \n",
    "            (~condition_mask&correct_permutation_confident_mask).sum().item()/(~condition_mask).sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighted, only uncertain init\n",
    "\n",
    "for threshold in [0.82]:\n",
    "    correct_permutation_confident_mask = correct_permutation_confident_scores>threshold\n",
    "    uncertain_init_mask = ood_score_init_list < threshold\n",
    "    uncertain_target_mask = ood_score_target_list < threshold\n",
    "    condition_mask = ~uncertain_init_mask\n",
    "    print(f\"*********************Threshold: {threshold}****************************\")\n",
    "    print(\"Init\")\n",
    "    # init prediction\n",
    "    print((condition_mask&correct_init_mask).sum().item(), \n",
    "        (condition_mask).sum().item(), \n",
    "        (condition_mask&correct_init_mask).sum().item()/(condition_mask).sum().item())\n",
    "    # init prediction\n",
    "    print((~condition_mask&correct_init_mask).sum().item(), \n",
    "        (~condition_mask).sum().item(), \n",
    "       (~condition_mask&correct_init_mask).sum().item()/(~condition_mask).sum().item())\n",
    "    print(\"MSP\")\n",
    "    # msp prediction\n",
    "    print((condition_mask&correct_msp_mask).sum().item(), \n",
    "        (condition_mask).sum().item(), \n",
    "        (condition_mask&correct_msp_mask).sum().item()/(condition_mask).sum().item())\n",
    "    # msp prediction\n",
    "    print((~condition_mask&correct_msp_mask).sum().item(), \n",
    "        (~condition_mask).sum().item(), \n",
    "        (~condition_mask&correct_msp_mask).sum().item()/(~condition_mask).sum().item())\n",
    "    print(\"MSP & Init\")\n",
    "    print(((condition_mask&correct_init_mask)&(condition_mask&correct_msp_mask)).sum().item())\n",
    "    # print(((~(init_target_mask&(~uncertain_init_mask & ~uncertain_target_mask))& correct_msp_mask)&(~(init_target_mask&(~uncertain_init_mask & ~uncertain_target_mask))& correct_init_mask)).sum().item())\n",
    "    print(((condition_mask&correct_init_mask)|(condition_mask&correct_msp_mask)).sum().item(),\n",
    "          (condition_mask).sum().item(), \n",
    "          ((condition_mask&correct_init_mask)|(condition_mask&correct_msp_mask)).sum().item()/(condition_mask).sum().item())\n",
    "    print(((~condition_mask&correct_init_mask)|(~condition_mask&correct_msp_mask)).sum().item(),\n",
    "          (~condition_mask).sum().item(), \n",
    "          ((~condition_mask&correct_init_mask)|(~condition_mask&correct_msp_mask)).sum().item()/(~condition_mask).sum().item())\n",
    "    print(\"Permutation\")\n",
    "    print((condition_mask&correct_permutation_mask).sum().item(), \n",
    "        (condition_mask).sum().item(), \n",
    "        (condition_mask&correct_permutation_mask).sum().item()/(condition_mask).sum().item())\n",
    "    print((~condition_mask&correct_permutation_mask).sum().item(), \n",
    "         (~condition_mask).sum().item(), \n",
    "        (~condition_mask&correct_permutation_mask).sum().item()/(~condition_mask).sum().item())\n",
    "    print(\"Permutation Confident\")\n",
    "    print((condition_mask&correct_permutation_confident_mask).sum().item(), \n",
    "        (condition_mask).sum().item(), \n",
    "        (condition_mask&correct_permutation_confident_mask).sum().item()/(condition_mask).sum().item())\n",
    "    print((~condition_mask&correct_permutation_confident_mask).sum().item(),\n",
    "            (~condition_mask).sum().item(), \n",
    "            (~condition_mask&correct_permutation_confident_mask).sum().item()/(~condition_mask).sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSP is wrong, but init is correct\n",
    "target_col_idx_msp_init = idx_list[~condition_mask&correct_msp_mask&correct_init_mask]\n",
    "target_col_idx_msp =  idx_list[~condition_mask&correct_msp_mask&~correct_init_mask]\n",
    "target_col_idx_init =  idx_list[~condition_mask&~correct_msp_mask&correct_init_mask]\n",
    "target_col_idx_permutation = idx_list[~condition_mask&correct_permutation_confident_mask&~correct_msp_mask&~correct_init_mask]\n",
    "target_col_idx_permutation_msp =  idx_list[~condition_mask&correct_permutation_confident_mask&~correct_msp_mask]\n",
    "target_col_idx_permutation_init =  idx_list[~condition_mask&correct_permutation_confident_mask&~correct_init_mask]\n",
    "\n",
    "print(\"Both correct\", (~condition_mask&correct_msp_mask&correct_init_mask).sum(), \n",
    "      ood_score_init_list[~condition_mask&correct_msp_mask&correct_init_mask].mean(), \n",
    "      # ood_score_target_list[~condition_mask&correct_msp_mask&correct_init_mask].mean(),\n",
    "      ood_score_final_list[~condition_mask&correct_msp_mask&correct_init_mask].mean())\n",
    "print(\"MSP correct \", (~condition_mask&correct_msp_mask&~correct_init_mask).sum(), \n",
    "      ood_score_init_list[~condition_mask&correct_msp_mask&~correct_init_mask].mean(),\n",
    "      #   ood_score_target_list[~condition_mask&correct_msp_mask&~correct_init_mask].mean(),\n",
    "        ood_score_final_list[~condition_mask&correct_msp_mask&~correct_init_mask].mean())\n",
    "print(\"Init correct\", (~condition_mask&~correct_msp_mask&correct_init_mask).sum(), \n",
    "      ood_score_init_list[~condition_mask&~correct_msp_mask&correct_init_mask].mean(),\n",
    "      #   ood_score_target_list[~condition_mask&~correct_msp_mask&correct_init_mask].mean(),\n",
    "        ood_score_final_list[~condition_mask&~correct_msp_mask&correct_init_mask].mean())\n",
    "print(\"Permutation correct\",(~condition_mask&correct_permutation_confident_mask&~correct_msp_mask&~correct_init_mask).sum(), \n",
    "      ood_score_init_list[~condition_mask&correct_permutation_confident_mask&~correct_msp_mask&~correct_init_mask].mean(),\n",
    "      #   ood_score_target_list[~condition_mask&correct_permutation_confident_mask&~correct_msp_mask&~correct_init_mask].mean(),\n",
    "        ood_score_final_list[~condition_mask&correct_permutation_confident_mask&~correct_msp_mask&~correct_init_mask].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col_idx_permutation_msp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use target as head, the MSP context\n",
    "import itertools\n",
    "\n",
    "def is_sublist(A, B):\n",
    "    it = iter(B)\n",
    "    return all(x in it for x in A)\n",
    "    \n",
    "model.load_state_dict(best_state_dict, strict=False)\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "ft_embs_test = []\n",
    "labels_test = []\n",
    "logits_test = []\n",
    "log = defaultdict(list)\n",
    "num_cols = []\n",
    "threshold = 0.8\n",
    "corrected = 0\n",
    "total_mistakes = 0\n",
    "init_permutation = defaultdict(list)\n",
    "corrected_permutation = defaultdict(list)\n",
    "init_logits = defaultdict(list)\n",
    "corrected_logits = defaultdict(list)\n",
    "max_col_length = 3\n",
    "msp_threshold = 0.9\n",
    "\n",
    "\n",
    "alpha = 0.25\n",
    "\n",
    "class_weights = (1.0 / class_freq) ** alpha\n",
    "\n",
    "# Normalize the weights\n",
    "class_weights /= class_weights.sum()\n",
    "\n",
    "for batch_idx, batch in enumerate(test_dataset_iter):\n",
    "    if batch_idx == 19:\n",
    "        break\n",
    "cls_indexes = torch.LongTensor([[0, batch[\"cls_indexes\"].cpu().item()]]).to(device)\n",
    "target_col_mask = batch[\"target_col_mask\"].T\n",
    "col_idx_set = target_col_mask.unique().tolist()\n",
    "init_permutation_i = get_permutation(target_col_mask)\n",
    "successs = False\n",
    "# logits = model(batch[\"data\"].T, cls_indexes=cls_indexes,).detach().cpu()\n",
    "# logits = reweight_logits(logits, class_weights)\n",
    "# logits_init = logits.clone()\n",
    "# num_cols.append(batch[\"target_col_mask\"].max().item())\n",
    "\n",
    "\n",
    "# col_idx_set = target_col_mask.unique().tolist()\n",
    "# successs = False\n",
    "# init_permutation_i = get_permutation(target_col_mask)\n",
    "# init_permutation[batch_idx].append(get_permutation(target_col_mask))\n",
    "# init_logits[batch_idx].append(logits_init.detach().cpu())     \n",
    "# init_msp = logits_init.max().item()\n",
    "# print(batch[\"label\"].item())\n",
    "# print(get_permutation(target_col_mask), init_msp, init_logits[batch_idx][0].argmax().item()) \n",
    "print(batch[\"label\"].item())  \n",
    "print(batch[\"target_col_mask\"].max().item(), get_permutation(target_col_mask))\n",
    "print(\"********************************************************\")\n",
    "assert -1 not in col_idx_set\n",
    "max_msp = 0 \n",
    "# for r in range(1, min(len(col_idx_set), max_col_length) + 1):\n",
    "enough = False\n",
    "# for r in range(len(col_idx_set)-1, 0, -1):\n",
    "for r in range(len(col_idx_set)-1, len(col_idx_set)//2, -1):\n",
    "    for subset in itertools.combinations(col_idx_set, r):\n",
    "        if 0 not in subset and r != 1:\n",
    "            continue\n",
    "        for x in itertools.permutations(subset):\n",
    "            if not is_sublist(x, init_permutation_i):\n",
    "                continue\n",
    "            new_batch_data = []\n",
    "            for col_i in x:\n",
    "                if col_i == 0:\n",
    "                    if len(new_batch_data) == 0:\n",
    "                        cls_indexes_value = 0\n",
    "                    else:\n",
    "                        cls_indexes_value = sum([len(new_batch_data[i]) for i in range(len(new_batch_data))])\n",
    "                new_batch_data.append(batch[\"data\"].T[target_col_mask==col_i])\n",
    "            if 0 not in x:\n",
    "                cls_indexes_value = 0\n",
    "            new_batch_data = torch.cat(new_batch_data, dim=-1).reshape(1, -1)\n",
    "            cls_indexes = torch.tensor([0, cls_indexes_value]).reshape(1, -1).to(device)\n",
    "            logits_temp = model(new_batch_data, cls_indexes=cls_indexes,).detach().cpu()\n",
    "            logits_temp = reweight_logits(logits_temp, class_weights)\n",
    "            msp_temp = logits_temp.max()\n",
    "            predict_temp = logits_temp.argmax()\n",
    "            # msp_temp = F.softmax(logits_temp).max().item()\n",
    "            # predict_temp = F.softmax(logits_temp).argmax().item()\n",
    "\n",
    "            print(x, msp_temp,predict_temp)\n",
    "            if msp_temp > max_msp and 0 in x:\n",
    "                max_msp = msp_temp\n",
    "                best_msp_perm = x\n",
    "                msp_predict = predict_temp\n",
    "        \n",
    "print(\"********************************************************\")\n",
    "print(best_msp_perm, max_msp, msp_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# brute force perumate, early stop\n",
    "import itertools\n",
    "\n",
    "\n",
    "    \n",
    "model.load_state_dict(best_state_dict, strict=False)\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "change_log = []\n",
    "score_init = []\n",
    "score_best = [] \n",
    "\n",
    "max_col_length = 3\n",
    "for alpha in [0.25]:\n",
    "    class_weights = (1.0 / class_freq) ** alpha\n",
    "    # Normalize the weights\n",
    "    class_weights /= class_weights.sum()\n",
    "    for uncertainty_threshold in [ 0.8]:\n",
    "        for msp_threshold in [ 0.8, 0.9, 0.99]:\n",
    "            for debias_threshold in [1.0]:\n",
    "                print(f\"*********************alpha: {alpha}; uncertain: {uncertainty_threshold}; debias: {debias_threshold}; msp: {msp_threshold}****************************\")\n",
    "                ft_embs_test = []\n",
    "                labels_test = []\n",
    "                logits_test = []\n",
    "                log = defaultdict(list)\n",
    "                num_cols = []\n",
    "                corrected = 0\n",
    "                total_mistakes = 0\n",
    "\n",
    "                for batch_idx, batch in enumerate(test_dataloader_iter):\n",
    "                    cls_indexes = torch.LongTensor([[0, batch[\"cls_indexes\"].cpu().item()]]).to(device)\n",
    "                    target_col_mask = batch[\"target_col_mask\"].T\n",
    "                    logits = model(batch[\"data\"].T, cls_indexes=cls_indexes,)\n",
    "                    logits = reweight_logits(logits.detach().cpu(), class_weights) # Here !!!!!!!!!!\n",
    "                    msp_init = logits.max().item()\n",
    "                    num_cols.append(batch[\"target_col_mask\"].max().item())\n",
    "                    predict_init = logits.argmax().item()\n",
    "                    max_msp = 0\n",
    "                    \n",
    "                    if 1 in target_col_mask and msp_init < uncertainty_threshold:\n",
    "                        init_permutation_i = get_permutation(target_col_mask)\n",
    "                        col_idx_set = target_col_mask.unique().tolist()\n",
    "                        debias_classes = []\n",
    "                        early_stop = False\n",
    "                        assert -1 not in col_idx_set\n",
    "                        for r in range(len(col_idx_set)-1, len(col_idx_set)//2, -1):\n",
    "                            round_classes = set()\n",
    "                            for subset in itertools.combinations(col_idx_set, r):\n",
    "                                if 0 not in subset and r != 1:\n",
    "                                    continue\n",
    "                                for x in itertools.permutations(subset):\n",
    "                                    if not is_sublist(x, init_permutation_i):\n",
    "                                        continue\n",
    "                                    new_batch_data = []\n",
    "                                    for col_i in x:\n",
    "                                        if col_i == 0:\n",
    "                                            if len(new_batch_data) == 0:\n",
    "                                                cls_indexes_value = 0\n",
    "                                            else:\n",
    "                                                cls_indexes_value = sum([len(new_batch_data[i]) for i in range(len(new_batch_data))])\n",
    "                                        new_batch_data.append(batch[\"data\"].T[target_col_mask==col_i])\n",
    "                                    if 0 not in x:\n",
    "                                        cls_indexes_value = 0\n",
    "                                    new_batch_data = torch.cat(new_batch_data, dim=-1).reshape(1, -1)\n",
    "                                    cls_indexes = torch.tensor([0, cls_indexes_value]).reshape(1, -1).to(device)\n",
    "                                    logits_temp = model(new_batch_data, cls_indexes=cls_indexes,)\n",
    "                                    logits_temp = reweight_logits(logits_temp.detach().cpu(), class_weights)\n",
    "                                    msp_temp = logits_temp.max().item()\n",
    "                                    predict_temp = logits_temp.argmax().item()\n",
    "                                    round_classes.add(predict_temp)\n",
    "                                    # if len(x) == 1 and 0 in x:\n",
    "                                    #     predict_target = predict_temp\n",
    "                                    #     msp_target = msp_temp\n",
    "                                    # # print(x, msp_temp, predict_temp)\n",
    "                                    # if 0 not in x and msp_temp > debias_threshold and (predict_temp != predict_target):\n",
    "                                    #     debias_classes.append(predict_temp)\n",
    "                                    #     continue\n",
    "                                    if msp_temp > max_msp and 0 in x and predict_temp not in debias_classes:\n",
    "                                        max_msp = msp_temp\n",
    "                                        best_msp_perm = x\n",
    "                                        msp_predict = predict_temp\n",
    "                                        logits_msp = logits_temp.clone()\n",
    "                                        if max_msp > msp_threshold and 0 in x and len(round_classes) > 1: # make sure the classes in one round is not homogenous \n",
    "                                            early_stop = True\n",
    "                            if early_stop:\n",
    "                                break\n",
    "                    if max_msp > msp_threshold:\n",
    "                        logits = logits_msp.clone()\n",
    "                    labels_test.append(batch[\"label\"].cpu())\n",
    "                    logits_test.append(logits.detach().cpu())\n",
    "                labels_test = torch.cat(labels_test, dim=0)\n",
    "                logits_test = torch.stack(logits_test, dim=0)\n",
    "                preds_test = torch.argmax(logits_test, dim=1)\n",
    "                num_cols = torch.tensor(num_cols)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                from sklearn.metrics import confusion_matrix, f1_score\n",
    "                mask = num_cols > 0\n",
    "                ts_pred_list = logits_test.argmax(\n",
    "                                            1).cpu().detach().numpy().tolist()\n",
    "                ts_micro_f1 = f1_score(labels_test.reshape(-1).numpy().tolist(),\n",
    "                                    ts_pred_list,\n",
    "                                    average=\"micro\")\n",
    "                ts_macro_f1 = f1_score(labels_test.reshape(-1).numpy().tolist(),\n",
    "                                    ts_pred_list,\n",
    "                                    average=\"macro\")\n",
    "                print(\"ts_micro_f1={:.4f}, ts_macro_f1={:.4f}\".format(ts_micro_f1, ts_macro_f1))\n",
    "                ts_pred_list = logits_test.argmax(\n",
    "                                            1).cpu().detach()[mask].numpy().tolist()\n",
    "                ts_micro_f1 = f1_score(labels_test.reshape(-1)[mask].numpy().tolist(),\n",
    "                                    ts_pred_list,\n",
    "                                    average=\"micro\")\n",
    "                ts_macro_f1 = f1_score(labels_test.reshape(-1)[mask].numpy().tolist(),\n",
    "                                    ts_pred_list,\n",
    "                                    average=\"macro\")\n",
    "                print(\"ts_micro_f1={:.4f}, ts_macro_f1={:.4f}\".format(ts_micro_f1, ts_macro_f1))\n",
    "                ts_pred_list = logits_test.argmax(\n",
    "                                            1).cpu().detach()[~mask].numpy().tolist()\n",
    "                ts_micro_f1 = f1_score(labels_test.reshape(-1)[~mask].numpy().tolist(),\n",
    "                                    ts_pred_list,\n",
    "                                    average=\"micro\")\n",
    "                ts_macro_f1 = f1_score(labels_test.reshape(-1)[~mask].numpy().tolist(),\n",
    "                                    ts_pred_list,\n",
    "                                    average=\"macro\")\n",
    "                print(\"ts_micro_f1={:.4f}, ts_macro_f1={:.4f}\".format(ts_micro_f1, ts_macro_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# brute force perumate, early stop\n",
    "import itertools\n",
    "\n",
    "\n",
    "    \n",
    "model.load_state_dict(best_state_dict, strict=False)\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "change_log = []\n",
    "score_init = []\n",
    "score_best = [] \n",
    "\n",
    "max_col_length = 3\n",
    "for alpha in [0.25]:\n",
    "    class_weights = (1.0 / class_freq) ** alpha\n",
    "    # Normalize the weights\n",
    "    class_weights /= class_weights.sum()\n",
    "    for uncertainty_threshold in [ 0.8]:\n",
    "        for msp_threshold in [0.0]:\n",
    "            for debias_threshold in [1.0]:\n",
    "                print(f\"*********************alpha: {alpha}; uncertain: {uncertainty_threshold}; debias: {debias_threshold}; msp: {msp_threshold}****************************\")\n",
    "                ft_embs_test = []\n",
    "                labels_test = []\n",
    "                logits_test = []\n",
    "                log = defaultdict(list)\n",
    "                num_cols = []\n",
    "                corrected = 0\n",
    "                total_mistakes = 0\n",
    "\n",
    "                for batch_idx, batch in enumerate(test_dataloader_iter):\n",
    "                    cls_indexes = torch.LongTensor([[0, batch[\"cls_indexes\"].cpu().item()]]).to(device)\n",
    "                    target_col_mask = batch[\"target_col_mask\"].T\n",
    "                    logits = model(batch[\"data\"].T, cls_indexes=cls_indexes,)\n",
    "                    logits = reweight_logits(logits.detach().cpu(), class_weights) # Here !!!!!!!!!!\n",
    "                    msp_init = logits.max().item()\n",
    "                    num_cols.append(batch[\"target_col_mask\"].max().item())\n",
    "                    predict_init = logits.argmax().item()\n",
    "                    max_msp = 0\n",
    "                    \n",
    "                    if 1 in target_col_mask and msp_init < uncertainty_threshold:\n",
    "                        init_permutation_i = get_permutation(target_col_mask)\n",
    "                        col_idx_set = target_col_mask.unique().tolist()\n",
    "                        debias_classes = []\n",
    "                        early_stop = False\n",
    "                        assert -1 not in col_idx_set\n",
    "                        for r in range(len(col_idx_set)-1, len(col_idx_set)//2, -1):\n",
    "                            round_classes = set()\n",
    "                            for subset in itertools.combinations(col_idx_set, r):\n",
    "                                if 0 not in subset and r != 1:\n",
    "                                    continue\n",
    "                                for x in itertools.permutations(subset):\n",
    "                                    if not is_sublist(x, init_permutation_i):\n",
    "                                        continue\n",
    "                                    new_batch_data = []\n",
    "                                    for col_i in x:\n",
    "                                        if col_i == 0:\n",
    "                                            if len(new_batch_data) == 0:\n",
    "                                                cls_indexes_value = 0\n",
    "                                            else:\n",
    "                                                cls_indexes_value = sum([len(new_batch_data[i]) for i in range(len(new_batch_data))])\n",
    "                                        new_batch_data.append(batch[\"data\"].T[target_col_mask==col_i])\n",
    "                                    if 0 not in x:\n",
    "                                        cls_indexes_value = 0\n",
    "                                    new_batch_data = torch.cat(new_batch_data, dim=-1).reshape(1, -1)\n",
    "                                    cls_indexes = torch.tensor([0, cls_indexes_value]).reshape(1, -1).to(device)\n",
    "                                    logits_temp = model(new_batch_data, cls_indexes=cls_indexes,)\n",
    "                                    logits_temp = reweight_logits(logits_temp.detach().cpu(), class_weights)\n",
    "                                    msp_temp = logits_temp.max().item()\n",
    "                                    predict_temp = logits_temp.argmax().item()\n",
    "                                    round_classes.add(predict_temp)\n",
    "                                    # if len(x) == 1 and 0 in x:\n",
    "                                    #     predict_target = predict_temp\n",
    "                                    #     msp_target = msp_temp\n",
    "                                    # # print(x, msp_temp, predict_temp)\n",
    "                                    # if 0 not in x and msp_temp > debias_threshold and (predict_temp != predict_target):\n",
    "                                    #     debias_classes.append(predict_temp)\n",
    "                                    #     continue\n",
    "                                    if msp_temp > max_msp and 0 in x and predict_temp not in debias_classes:\n",
    "                                        max_msp = msp_temp\n",
    "                                        best_msp_perm = x\n",
    "                                        msp_predict = predict_temp\n",
    "                                        logits_msp = logits_temp.clone()\n",
    "                                        if max_msp > msp_threshold and 0 in x and len(round_classes) > 1: # make sure the classes in one round is not homogenous \n",
    "                                            early_stop = True\n",
    "                            if early_stop:\n",
    "                                break\n",
    "                    if max_msp > msp_threshold:\n",
    "                        logits = logits_msp.clone()\n",
    "                    labels_test.append(batch[\"label\"].cpu())\n",
    "                    logits_test.append(logits.detach().cpu())\n",
    "                labels_test = torch.cat(labels_test, dim=0)\n",
    "                logits_test = torch.stack(logits_test, dim=0)\n",
    "                preds_test = torch.argmax(logits_test, dim=1)\n",
    "                num_cols = torch.tensor(num_cols)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                from sklearn.metrics import confusion_matrix, f1_score\n",
    "                mask = num_cols > 0\n",
    "                ts_pred_list = logits_test.argmax(\n",
    "                                            1).cpu().detach().numpy().tolist()\n",
    "                ts_micro_f1 = f1_score(labels_test.reshape(-1).numpy().tolist(),\n",
    "                                    ts_pred_list,\n",
    "                                    average=\"micro\")\n",
    "                ts_macro_f1 = f1_score(labels_test.reshape(-1).numpy().tolist(),\n",
    "                                    ts_pred_list,\n",
    "                                    average=\"macro\")\n",
    "                print(\"ts_micro_f1={:.4f}, ts_macro_f1={:.4f}\".format(ts_micro_f1, ts_macro_f1))\n",
    "                ts_pred_list = logits_test.argmax(\n",
    "                                            1).cpu().detach()[mask].numpy().tolist()\n",
    "                ts_micro_f1 = f1_score(labels_test.reshape(-1)[mask].numpy().tolist(),\n",
    "                                    ts_pred_list,\n",
    "                                    average=\"micro\")\n",
    "                ts_macro_f1 = f1_score(labels_test.reshape(-1)[mask].numpy().tolist(),\n",
    "                                    ts_pred_list,\n",
    "                                    average=\"macro\")\n",
    "                print(\"ts_micro_f1={:.4f}, ts_macro_f1={:.4f}\".format(ts_micro_f1, ts_macro_f1))\n",
    "                ts_pred_list = logits_test.argmax(\n",
    "                                            1).cpu().detach()[~mask].numpy().tolist()\n",
    "                ts_micro_f1 = f1_score(labels_test.reshape(-1)[~mask].numpy().tolist(),\n",
    "                                    ts_pred_list,\n",
    "                                    average=\"micro\")\n",
    "                ts_macro_f1 = f1_score(labels_test.reshape(-1)[~mask].numpy().tolist(),\n",
    "                                    ts_pred_list,\n",
    "                                    average=\"macro\")\n",
    "                print(\"ts_micro_f1={:.4f}, ts_macro_f1={:.4f}\".format(ts_micro_f1, ts_macro_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# brute force perumate, early stop\n",
    "import itertools\n",
    "\n",
    "\n",
    "    \n",
    "model.load_state_dict(best_state_dict, strict=False)\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "change_log = []\n",
    "score_init = []\n",
    "score_best = [] \n",
    "\n",
    "max_col_length = 3\n",
    "for alpha in [0.25]:\n",
    "    class_weights = (1.0 / class_freq) ** alpha\n",
    "    # Normalize the weights\n",
    "    class_weights /= class_weights.sum()\n",
    "    for uncertainty_threshold in [ 0.85]:\n",
    "        for msp_threshold in [0.9]:\n",
    "            for debias_threshold in [1.0]:\n",
    "                print(f\"*********************alpha: {alpha}; uncertain: {uncertainty_threshold}; debias: {debias_threshold}; msp: {msp_threshold}****************************\")\n",
    "                ft_embs_test = []\n",
    "                labels_test = []\n",
    "                logits_test = []\n",
    "                log = defaultdict(list)\n",
    "                num_cols = []\n",
    "                corrected = 0\n",
    "                total_mistakes = 0\n",
    "\n",
    "                for batch_idx, batch in enumerate(test_dataloader_iter):\n",
    "                    cls_indexes = torch.LongTensor([[0, batch[\"cls_indexes\"].cpu().item()]]).to(device)\n",
    "                    target_col_mask = batch[\"target_col_mask\"].T\n",
    "                    logits = model(batch[\"data\"].T, cls_indexes=cls_indexes,)\n",
    "                    logits = reweight_logits(logits.detach().cpu(), class_weights) # Here !!!!!!!!!!\n",
    "                    msp_init = logits.max().item()\n",
    "                    num_cols.append(batch[\"target_col_mask\"].max().item())\n",
    "                    predict_init = logits.argmax().item()\n",
    "                    max_msp = 0\n",
    "                    \n",
    "                    if 1 in target_col_mask and msp_init < uncertainty_threshold:\n",
    "                        init_permutation_i = get_permutation(target_col_mask)\n",
    "                        col_idx_set = target_col_mask.unique().tolist()\n",
    "                        debias_classes = []\n",
    "                        early_stop = False\n",
    "                        assert -1 not in col_idx_set\n",
    "                        for r in range(len(col_idx_set)-1, len(col_idx_set)//2, -1):\n",
    "                            round_classes = set()\n",
    "                            for subset in itertools.combinations(col_idx_set, r):\n",
    "                                if 0 not in subset and r != 1:\n",
    "                                    continue\n",
    "                                for x in itertools.permutations(subset):\n",
    "                                    if not is_sublist(x, init_permutation_i):\n",
    "                                        continue\n",
    "                                    new_batch_data = []\n",
    "                                    for col_i in x:\n",
    "                                        if col_i == 0:\n",
    "                                            if len(new_batch_data) == 0:\n",
    "                                                cls_indexes_value = 0\n",
    "                                            else:\n",
    "                                                cls_indexes_value = sum([len(new_batch_data[i]) for i in range(len(new_batch_data))])\n",
    "                                        new_batch_data.append(batch[\"data\"].T[target_col_mask==col_i])\n",
    "                                    if 0 not in x:\n",
    "                                        cls_indexes_value = 0\n",
    "                                    new_batch_data = torch.cat(new_batch_data, dim=-1).reshape(1, -1)\n",
    "                                    cls_indexes = torch.tensor([0, cls_indexes_value]).reshape(1, -1).to(device)\n",
    "                                    logits_temp = model(new_batch_data, cls_indexes=cls_indexes,)\n",
    "                                    logits_temp = reweight_logits(logits_temp.detach().cpu(), class_weights)\n",
    "                                    msp_temp = logits_temp.max().item()\n",
    "                                    predict_temp = logits_temp.argmax().item()\n",
    "                                    round_classes.add(predict_temp)\n",
    "                                    # if len(x) == 1 and 0 in x:\n",
    "                                    #     predict_target = predict_temp\n",
    "                                    #     msp_target = msp_temp\n",
    "                                    # # print(x, msp_temp, predict_temp)\n",
    "                                    # if 0 not in x and msp_temp > debias_threshold and (predict_temp != predict_target):\n",
    "                                    #     debias_classes.append(predict_temp)\n",
    "                                    #     continue\n",
    "                                    if msp_temp > max_msp and 0 in x and predict_temp not in debias_classes:\n",
    "                                        max_msp = msp_temp\n",
    "                                        best_msp_perm = x\n",
    "                                        msp_predict = predict_temp\n",
    "                                        logits_msp = logits_temp.clone()\n",
    "                                        if max_msp > msp_threshold and 0 in x and len(round_classes) > 1: # make sure the classes in one round is not homogenous \n",
    "                                            early_stop = True\n",
    "                            if early_stop:\n",
    "                                break\n",
    "                    if max_msp > msp_threshold:\n",
    "                        logits = logits_msp.clone()\n",
    "                    labels_test.append(batch[\"label\"].cpu())\n",
    "                    logits_test.append(logits.detach().cpu())\n",
    "                labels_test = torch.cat(labels_test, dim=0)\n",
    "                logits_test = torch.stack(logits_test, dim=0)\n",
    "                preds_test = torch.argmax(logits_test, dim=1)\n",
    "                num_cols = torch.tensor(num_cols)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                from sklearn.metrics import confusion_matrix, f1_score\n",
    "                mask = num_cols > 0\n",
    "                ts_pred_list = logits_test.argmax(\n",
    "                                            1).cpu().detach().numpy().tolist()\n",
    "                ts_micro_f1 = f1_score(labels_test.reshape(-1).numpy().tolist(),\n",
    "                                    ts_pred_list,\n",
    "                                    average=\"micro\")\n",
    "                ts_macro_f1 = f1_score(labels_test.reshape(-1).numpy().tolist(),\n",
    "                                    ts_pred_list,\n",
    "                                    average=\"macro\")\n",
    "                print(\"ts_micro_f1={:.4f}, ts_macro_f1={:.4f}\".format(ts_micro_f1, ts_macro_f1))\n",
    "                ts_pred_list = logits_test.argmax(\n",
    "                                            1).cpu().detach()[mask].numpy().tolist()\n",
    "                ts_micro_f1 = f1_score(labels_test.reshape(-1)[mask].numpy().tolist(),\n",
    "                                    ts_pred_list,\n",
    "                                    average=\"micro\")\n",
    "                ts_macro_f1 = f1_score(labels_test.reshape(-1)[mask].numpy().tolist(),\n",
    "                                    ts_pred_list,\n",
    "                                    average=\"macro\")\n",
    "                print(\"ts_micro_f1={:.4f}, ts_macro_f1={:.4f}\".format(ts_micro_f1, ts_macro_f1))\n",
    "                ts_pred_list = logits_test.argmax(\n",
    "                                            1).cpu().detach()[~mask].numpy().tolist()\n",
    "                ts_micro_f1 = f1_score(labels_test.reshape(-1)[~mask].numpy().tolist(),\n",
    "                                    ts_pred_list,\n",
    "                                    average=\"micro\")\n",
    "                ts_macro_f1 = f1_score(labels_test.reshape(-1)[~mask].numpy().tolist(),\n",
    "                                    ts_pred_list,\n",
    "                                    average=\"macro\")\n",
    "                print(\"ts_micro_f1={:.4f}, ts_macro_f1={:.4f}\".format(ts_micro_f1, ts_macro_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# brute force perumate, early stop\n",
    "import itertools\n",
    "\n",
    "\n",
    "    \n",
    "model.load_state_dict(best_state_dict, strict=False)\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "change_log = []\n",
    "score_init = []\n",
    "score_best = [] \n",
    "\n",
    "max_col_length = 3\n",
    "for alpha in [0.25]:\n",
    "    class_weights = (1.0 / class_freq) ** alpha\n",
    "    # Normalize the weights\n",
    "    class_weights /= class_weights.sum()\n",
    "    for uncertainty_threshold in [ 0.85, 0.8]:\n",
    "        for msp_threshold in [0.0, 0.8, 0.85, 0.9]:\n",
    "            for debias_threshold in [1.0]:\n",
    "                print(f\"*********************alpha: {alpha}; uncertain: {uncertainty_threshold}; debias: {debias_threshold}; msp: {msp_threshold}****************************\")\n",
    "                ft_embs_test = []\n",
    "                labels_test = []\n",
    "                logits_test = []\n",
    "                log = defaultdict(list)\n",
    "                num_cols = []\n",
    "                corrected = 0\n",
    "                total_mistakes = 0\n",
    "\n",
    "                for batch_idx, batch in enumerate(test_dataloader_iter):\n",
    "                    cls_indexes = torch.LongTensor([[0, batch[\"cls_indexes\"].cpu().item()]]).to(device)\n",
    "                    target_col_mask = batch[\"target_col_mask\"].T\n",
    "                    logits = model(batch[\"data\"].T, cls_indexes=cls_indexes,)\n",
    "                    logits = reweight_logits(logits.detach().cpu(), class_weights) # Here !!!!!!!!!!\n",
    "                    msp_init = logits.max().item()\n",
    "                    num_cols.append(batch[\"target_col_mask\"].max().item())\n",
    "                    predict_init = logits.argmax().item()\n",
    "                    max_msp = 0\n",
    "                    \n",
    "                    if 1 in target_col_mask and msp_init < uncertainty_threshold:\n",
    "                        init_permutation_i = get_permutation(target_col_mask)\n",
    "                        col_idx_set = target_col_mask.unique().tolist()\n",
    "                        debias_classes = []\n",
    "                        early_stop = False\n",
    "                        assert -1 not in col_idx_set\n",
    "                        for r in range(len(col_idx_set)-1, len(col_idx_set)//2, -1):\n",
    "                            round_classes = set()\n",
    "                            for subset in itertools.combinations(col_idx_set, r):\n",
    "                                if 0 not in subset and r != 1:\n",
    "                                    continue\n",
    "                                for x in itertools.permutations(subset):\n",
    "                                    if not is_sublist(x, init_permutation_i):\n",
    "                                        continue\n",
    "                                    new_batch_data = []\n",
    "                                    for col_i in x:\n",
    "                                        if col_i == 0:\n",
    "                                            if len(new_batch_data) == 0:\n",
    "                                                cls_indexes_value = 0\n",
    "                                            else:\n",
    "                                                cls_indexes_value = sum([len(new_batch_data[i]) for i in range(len(new_batch_data))])\n",
    "                                        new_batch_data.append(batch[\"data\"].T[target_col_mask==col_i])\n",
    "                                    if 0 not in x:\n",
    "                                        cls_indexes_value = 0\n",
    "                                    new_batch_data = torch.cat(new_batch_data, dim=-1).reshape(1, -1)\n",
    "                                    cls_indexes = torch.tensor([0, cls_indexes_value]).reshape(1, -1).to(device)\n",
    "                                    logits_temp = model(new_batch_data, cls_indexes=cls_indexes,)\n",
    "                                    logits_temp = reweight_logits(logits_temp.detach().cpu(), class_weights)\n",
    "                                    msp_temp = logits_temp.max().item()\n",
    "                                    predict_temp = logits_temp.argmax().item()\n",
    "                                    round_classes.add(predict_temp)\n",
    "                                    # if len(x) == 1 and 0 in x:\n",
    "                                    #     predict_target = predict_temp\n",
    "                                    #     msp_target = msp_temp\n",
    "                                    # # print(x, msp_temp, predict_temp)\n",
    "                                    # if 0 not in x and msp_temp > debias_threshold and (predict_temp != predict_target):\n",
    "                                    #     debias_classes.append(predict_temp)\n",
    "                                    #     continue\n",
    "                                    if msp_temp > max_msp and 0 in x and predict_temp not in debias_classes:\n",
    "                                        max_msp = msp_temp\n",
    "                                        best_msp_perm = x\n",
    "                                        msp_predict = predict_temp\n",
    "                                        logits_msp = logits_temp.clone()\n",
    "                                        if max_msp > msp_threshold and 0 in x: # make sure the classes in one round is not homogenous \n",
    "                                            early_stop = True\n",
    "                            if early_stop:\n",
    "                                break\n",
    "                    if max_msp > msp_threshold:\n",
    "                        logits = logits_msp.clone()\n",
    "                    labels_test.append(batch[\"label\"].cpu())\n",
    "                    logits_test.append(logits.detach().cpu())\n",
    "                labels_test = torch.cat(labels_test, dim=0)\n",
    "                logits_test = torch.stack(logits_test, dim=0)\n",
    "                preds_test = torch.argmax(logits_test, dim=1)\n",
    "                num_cols = torch.tensor(num_cols)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                from sklearn.metrics import confusion_matrix, f1_score\n",
    "                mask = num_cols > 0\n",
    "                ts_pred_list = logits_test.argmax(\n",
    "                                            1).cpu().detach().numpy().tolist()\n",
    "                ts_micro_f1 = f1_score(labels_test.reshape(-1).numpy().tolist(),\n",
    "                                    ts_pred_list,\n",
    "                                    average=\"micro\")\n",
    "                ts_macro_f1 = f1_score(labels_test.reshape(-1).numpy().tolist(),\n",
    "                                    ts_pred_list,\n",
    "                                    average=\"macro\")\n",
    "                print(\"ts_micro_f1={:.4f}, ts_macro_f1={:.4f}\".format(ts_micro_f1, ts_macro_f1))\n",
    "                ts_pred_list = logits_test.argmax(\n",
    "                                            1).cpu().detach()[mask].numpy().tolist()\n",
    "                ts_micro_f1 = f1_score(labels_test.reshape(-1)[mask].numpy().tolist(),\n",
    "                                    ts_pred_list,\n",
    "                                    average=\"micro\")\n",
    "                ts_macro_f1 = f1_score(labels_test.reshape(-1)[mask].numpy().tolist(),\n",
    "                                    ts_pred_list,\n",
    "                                    average=\"macro\")\n",
    "                print(\"ts_micro_f1={:.4f}, ts_macro_f1={:.4f}\".format(ts_micro_f1, ts_macro_f1))\n",
    "                ts_pred_list = logits_test.argmax(\n",
    "                                            1).cpu().detach()[~mask].numpy().tolist()\n",
    "                ts_micro_f1 = f1_score(labels_test.reshape(-1)[~mask].numpy().tolist(),\n",
    "                                    ts_pred_list,\n",
    "                                    average=\"micro\")\n",
    "                ts_macro_f1 = f1_score(labels_test.reshape(-1)[~mask].numpy().tolist(),\n",
    "                                    ts_pred_list,\n",
    "                                    average=\"macro\")\n",
    "                print(\"ts_micro_f1={:.4f}, ts_macro_f1={:.4f}\".format(ts_micro_f1, ts_macro_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *********************alpha: 0.25; uncertain: 0.85; debias: 1.0; msp: 0.9****************************\n",
    "# ts_micro_f1=0.5456, ts_macro_f1=0.2627\n",
    "# ts_micro_f1=0.5452, ts_macro_f1=0.2626\n",
    "# ts_micro_f1=1.0000, ts_macro_f1=1.0000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# brute force perumate, early stop\n",
    "import itertools\n",
    "\n",
    "\n",
    "    \n",
    "model.load_state_dict(best_state_dict, strict=False)\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "change_log = []\n",
    "score_init = []\n",
    "score_best = [] \n",
    "\n",
    "max_col_length = 3\n",
    "for alpha in [0.0]:\n",
    "    # class_weights = (1.0 / class_freq) ** alpha\n",
    "    # # Normalize the weights\n",
    "    # class_weights /= class_weights.sum()\n",
    "    for uncertainty_threshold in [ 0.85, 0.8, 0.9]:\n",
    "        for msp_threshold in [0.0, 0.8, 0.85, 0.9]:\n",
    "            for debias_threshold in [1.0]:\n",
    "                print(f\"*********************alpha: {alpha}; uncertain: {uncertainty_threshold}; debias: {debias_threshold}; msp: {msp_threshold}****************************\")\n",
    "                ft_embs_test = []\n",
    "                labels_test = []\n",
    "                logits_test = []\n",
    "                log = defaultdict(list)\n",
    "                num_cols = []\n",
    "                corrected = 0\n",
    "                total_mistakes = 0\n",
    "\n",
    "                for batch_idx, batch in enumerate(test_dataloader_iter):\n",
    "                    cls_indexes = torch.LongTensor([[0, batch[\"cls_indexes\"].cpu().item()]]).to(device)\n",
    "                    target_col_mask = batch[\"target_col_mask\"].T\n",
    "                    logits = model(batch[\"data\"].T, cls_indexes=cls_indexes,)\n",
    "                    # logits = reweight_logits(logits.detach().cpu(), class_weights) # Here !!!!!!!!!!\n",
    "                    logits = F.softmax(logits, dim=-1)\n",
    "                    msp_init = logits.max().item()\n",
    "                    num_cols.append(batch[\"target_col_mask\"].max().item())\n",
    "                    predict_init = logits.argmax().item()\n",
    "                    max_msp = 0\n",
    "                    \n",
    "                    if 1 in target_col_mask and msp_init < uncertainty_threshold:\n",
    "                        init_permutation_i = get_permutation(target_col_mask)\n",
    "                        col_idx_set = target_col_mask.unique().tolist()\n",
    "                        debias_classes = []\n",
    "                        early_stop = False\n",
    "                        assert -1 not in col_idx_set\n",
    "                        for r in range(len(col_idx_set)-1, len(col_idx_set)//2, -1):\n",
    "                            round_classes = set()\n",
    "                            for subset in itertools.combinations(col_idx_set, r):\n",
    "                                if 0 not in subset and r != 1:\n",
    "                                    continue\n",
    "                                for x in itertools.permutations(subset):\n",
    "                                    if not is_sublist(x, init_permutation_i):\n",
    "                                        continue\n",
    "                                    new_batch_data = []\n",
    "                                    for col_i in x:\n",
    "                                        if col_i == 0:\n",
    "                                            if len(new_batch_data) == 0:\n",
    "                                                cls_indexes_value = 0\n",
    "                                            else:\n",
    "                                                cls_indexes_value = sum([len(new_batch_data[i]) for i in range(len(new_batch_data))])\n",
    "                                        new_batch_data.append(batch[\"data\"].T[target_col_mask==col_i])\n",
    "                                    if 0 not in x:\n",
    "                                        cls_indexes_value = 0\n",
    "                                    new_batch_data = torch.cat(new_batch_data, dim=-1).reshape(1, -1)\n",
    "                                    cls_indexes = torch.tensor([0, cls_indexes_value]).reshape(1, -1).to(device)\n",
    "                                    logits_temp = model(new_batch_data, cls_indexes=cls_indexes,)\n",
    "                                    # logits_temp = reweight_logits(logits_temp.detach().cpu(), class_weights)\n",
    "                                    logits_temp = F.softmax(logits_temp, dim=-1)\n",
    "                                    msp_temp = logits_temp.max().item()\n",
    "                                    predict_temp = logits_temp.argmax().item()\n",
    "                                    round_classes.add(predict_temp)\n",
    "                                    # if len(x) == 1 and 0 in x:\n",
    "                                    #     predict_target = predict_temp\n",
    "                                    #     msp_target = msp_temp\n",
    "                                    # # print(x, msp_temp, predict_temp)\n",
    "                                    # if 0 not in x and msp_temp > debias_threshold and (predict_temp != predict_target):\n",
    "                                    #     debias_classes.append(predict_temp)\n",
    "                                    #     continue\n",
    "                                    if msp_temp > max_msp and 0 in x and predict_temp not in debias_classes:\n",
    "                                        max_msp = msp_temp\n",
    "                                        best_msp_perm = x\n",
    "                                        msp_predict = predict_temp\n",
    "                                        logits_msp = logits_temp.clone()\n",
    "                                        if max_msp > msp_threshold and 0 in x: # make sure the classes in one round is not homogenous \n",
    "                                            early_stop = True\n",
    "                            if early_stop:\n",
    "                                break\n",
    "                    if max_msp > msp_threshold:\n",
    "                        logits = logits_msp.clone()\n",
    "                    labels_test.append(batch[\"label\"].cpu())\n",
    "                    logits_test.append(logits.detach().cpu())\n",
    "                labels_test = torch.cat(labels_test, dim=0)\n",
    "                logits_test = torch.stack(logits_test, dim=0)\n",
    "                preds_test = torch.argmax(logits_test, dim=1)\n",
    "                num_cols = torch.tensor(num_cols)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                from sklearn.metrics import confusion_matrix, f1_score\n",
    "                mask = num_cols > 0\n",
    "                ts_pred_list = logits_test.argmax(\n",
    "                                            1).cpu().detach().numpy().tolist()\n",
    "                ts_micro_f1 = f1_score(labels_test.reshape(-1).numpy().tolist(),\n",
    "                                    ts_pred_list,\n",
    "                                    average=\"micro\")\n",
    "                ts_macro_f1 = f1_score(labels_test.reshape(-1).numpy().tolist(),\n",
    "                                    ts_pred_list,\n",
    "                                    average=\"macro\")\n",
    "                print(\"ts_micro_f1={:.4f}, ts_macro_f1={:.4f}\".format(ts_micro_f1, ts_macro_f1))\n",
    "                ts_pred_list = logits_test.argmax(\n",
    "                                            1).cpu().detach()[mask].numpy().tolist()\n",
    "                ts_micro_f1 = f1_score(labels_test.reshape(-1)[mask].numpy().tolist(),\n",
    "                                    ts_pred_list,\n",
    "                                    average=\"micro\")\n",
    "                ts_macro_f1 = f1_score(labels_test.reshape(-1)[mask].numpy().tolist(),\n",
    "                                    ts_pred_list,\n",
    "                                    average=\"macro\")\n",
    "                print(\"ts_micro_f1={:.4f}, ts_macro_f1={:.4f}\".format(ts_micro_f1, ts_macro_f1))\n",
    "                ts_pred_list = logits_test.argmax(\n",
    "                                            1).cpu().detach()[~mask].numpy().tolist()\n",
    "                ts_micro_f1 = f1_score(labels_test.reshape(-1)[~mask].numpy().tolist(),\n",
    "                                    ts_pred_list,\n",
    "                                    average=\"micro\")\n",
    "                ts_macro_f1 = f1_score(labels_test.reshape(-1)[~mask].numpy().tolist(),\n",
    "                                    ts_pred_list,\n",
    "                                    average=\"macro\")\n",
    "                print(\"ts_micro_f1={:.4f}, ts_macro_f1={:.4f}\".format(ts_micro_f1, ts_macro_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# brute force perumate in training TODO: restrict mamimus length of permutation\n",
    "import itertools\n",
    "import torch.nn.functional as F\n",
    "\n",
    "    \n",
    "model.load_state_dict(best_state_dict, strict=False)\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "change_log = []\n",
    "score_init = []\n",
    "score_best = [] \n",
    "for threshold in [1.5]:\n",
    "    print(f\"*********************Threshold: {threshold}****************************\")\n",
    "    ft_embs_test = []\n",
    "    labels_test = []\n",
    "    logits_test = []\n",
    "    log = defaultdict(list)\n",
    "    num_cols = []\n",
    "    corrected = 0\n",
    "    total_mistakes = 0\n",
    "    num_permutations = {}\n",
    "    init_permutation = {}\n",
    "    init_correctness = {}\n",
    "    score_init = {}\n",
    "    score_permutation = defaultdict(list)\n",
    "    permutation_correctness = defaultdict(list)\n",
    "    for batch_idx, batch in enumerate(train_dataloader_iter):\n",
    "        cls_indexes = torch.LongTensor([[0, batch[\"cls_indexes\"].cpu().item()]]).to(device)\n",
    "        target_col_mask = batch[\"target_col_mask\"].T\n",
    "        logits = model(batch[\"data\"].T, cls_indexes=cls_indexes,)\n",
    "        score_init[batch_idx] = F.softmax(logits.detach()).max().item()\n",
    "        label_i = batch[\"label\"].item()\n",
    "        predict_init = logits.argmax().item()\n",
    "        init_permutation_i = get_permutation(target_col_mask)\n",
    "        init_permutation[batch_idx] = init_permutation_i\n",
    "        if predict_init == label_i:\n",
    "            init_correctness[batch_idx] = True\n",
    "        else:\n",
    "            init_correctness[batch_idx] = False\n",
    "        num_permutations[batch_idx] = 0\n",
    "        if 1 in target_col_mask:\n",
    "            col_idx_set = target_col_mask.unique().tolist()\n",
    "            assert -1 not in col_idx_set\n",
    "            for r in range(len(init_permutation_i)-1, len(init_permutation_i)//2, -1): # not \n",
    "                for x in itertools.combinations(init_permutation_i, r):\n",
    "                    if 0 not in x:\n",
    "                        continue\n",
    "\n",
    "                    num_permutations[batch_idx] += 1\n",
    "                    new_batch_data = []\n",
    "                    for col_i in x:\n",
    "                        if col_i == 0:\n",
    "                            if len(new_batch_data) == 0:\n",
    "                                cls_indexes_value = 0\n",
    "                            else:\n",
    "                                cls_indexes_value = sum([len(new_batch_data[i]) for i in range(len(new_batch_data))])\n",
    "                        new_batch_data.append(batch[\"data\"].T[target_col_mask==col_i])\n",
    "                    new_batch_data = torch.cat(new_batch_data, dim=-1).reshape(1, -1)\n",
    "                    cls_indexes = torch.tensor([0, cls_indexes_value]).reshape(1, -1).to(device)\n",
    "                    logits_temp = model(new_batch_data, cls_indexes=cls_indexes,)\n",
    "                    ood_score_temp = F.softmax(logits_temp.detach()).max().item()\n",
    "                    score_permutation[batch_idx].append(ood_score_temp)\n",
    "                    predict_temp = logits_temp.argmax().item()\n",
    "                    permutation_correctness[batch_idx].append(predict_temp == label_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation_correctness_ratio = []\n",
    "score_permutation_avg = []\n",
    "for i in score_init:\n",
    "    permutation_correctness_ratio.append(np.mean(permutation_correctness[i]))\n",
    "    score_permutation_avg.append(np.mean(score_permutation[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.histplot(score_permutation_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.histplot(permutation_correctness_ratio)\n",
    "permutation_correctness_ratio = np.array(permutation_correctness_ratio)\n",
    "print(np.mean(permutation_correctness_ratio[~np.isnan(permutation_correctness_ratio)]), sum(permutation_correctness_ratio==1)/len(permutation_correctness_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation_correctness[18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(permutation_correctness[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in score_init:\n",
    "    print(score_init[i], init_correctness[i], num_permutations[i], init_permutation[i])\n",
    "    for score, correct in zip(score_permutation[i], permutation_correctness[i], ):\n",
    "        print(score, correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(i, score_init[i], init_correctness[i], num_permutations[i], init_permutation[i])\n",
    "    print(np.mean(score_permutation[i]), np.mean(permutation_correctness[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation_correctness[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_valid = []\n",
    "for batch_idx, batch in enumerate(valid_dataloader_iter):\n",
    "    labels_valid.append(batch[\"label\"].cpu())\n",
    "labels_valid = torch.tensor(labels_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(labels_valid.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, batch in enumerate(valid_dataloader_iter):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"target_col_mask\"].T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " batch[\"cls_indexes\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# brute force perumate in validation TODO: restrict mamimus length of permutation\n",
    "import itertools\n",
    "import torch.nn.functional as F\n",
    "\n",
    "    \n",
    "model.load_state_dict(best_state_dict, strict=False)\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "change_log = []\n",
    "score_init = []\n",
    "score_best = [] \n",
    "for threshold in [1.5]:\n",
    "    print(f\"*********************Threshold: {threshold}****************************\")\n",
    "    ft_embs_test = []\n",
    "    labels_test = []\n",
    "    logits_test = []\n",
    "    log = defaultdict(list)\n",
    "    num_cols = []\n",
    "    corrected = 0\n",
    "    total_mistakes = 0\n",
    "    num_permutations = {}\n",
    "    init_permutation = {}\n",
    "    init_correctness = {}\n",
    "    score_init = {}\n",
    "    score_permutation = defaultdict(list)\n",
    "    permutation_correctness = defaultdict(list)\n",
    "    for batch_idx, batch in enumerate(valid_dataloader_iter):\n",
    "        cls_indexes = torch.LongTensor([[0, batch[\"cls_indexes\"].cpu().item()]]).to(device)\n",
    "        target_col_mask = batch[\"target_col_mask\"].T\n",
    "        logits = model(batch[\"data\"].T, cls_indexes=cls_indexes,)\n",
    "        score_init[batch_idx] = F.softmax(logits.detach()).max().item()\n",
    "        label_i = batch[\"label\"].item()\n",
    "        predict_init = logits.argmax().item()\n",
    "        init_permutation_i = get_permutation(target_col_mask)\n",
    "        init_permutation[batch_idx] = init_permutation_i\n",
    "        if predict_init == label_i:\n",
    "            init_correctness[batch_idx] = True\n",
    "        else:\n",
    "            init_correctness[batch_idx] = False\n",
    "        num_permutations[batch_idx] = 0\n",
    "        if 1 in target_col_mask:\n",
    "            col_idx_set = target_col_mask.unique().tolist()\n",
    "            assert -1 not in col_idx_set\n",
    "            for r in range(len(init_permutation_i), max(len(init_permutation_i)//2, len(init_permutation_i)-3), -1): # not \n",
    "                for subset in itertools.combinations(init_permutation_i, r):\n",
    "                    if 0 not in subset and r != 1:\n",
    "                        continue\n",
    "                    for x in itertools.permutations(subset):\n",
    "                    num_permutations[batch_idx] += 1\n",
    "                    new_batch_data = []\n",
    "                    for col_i in x:\n",
    "                        if col_i == 0:\n",
    "                            if len(new_batch_data) == 0:\n",
    "                                cls_indexes_value = 0\n",
    "                            else:\n",
    "                                cls_indexes_value = sum([len(new_batch_data[i]) for i in range(len(new_batch_data))])\n",
    "                        new_batch_data.append(batch[\"data\"].T[target_col_mask==col_i])\n",
    "                    new_batch_data = torch.cat(new_batch_data, dim=-1).reshape(1, -1)\n",
    "                    cls_indexes = torch.tensor([0, cls_indexes_value]).reshape(1, -1).to(device)\n",
    "                    logits_temp = model(new_batch_data, cls_indexes=cls_indexes,)\n",
    "                    ood_score_temp = F.softmax(logits_temp.detach()).max().item()\n",
    "                    score_permutation[batch_idx].append(ood_score_temp)\n",
    "                    predict_temp = logits_temp.argmax().item()\n",
    "                    permutation_correctness[batch_idx].append(predict_temp == label_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# brute force perumate in validation TODO: restrict mamimus length of permutation\n",
    "import itertools\n",
    "import torch.nn.functional as F\n",
    "\n",
    "    \n",
    "model.load_state_dict(best_state_dict, strict=False)\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "change_log = []\n",
    "score_init = []\n",
    "score_best = [] \n",
    "for threshold in [1.5]:\n",
    "    print(f\"*********************Threshold: {threshold}****************************\")\n",
    "    ft_embs_test = []\n",
    "    labels_test = []\n",
    "    logits_test = []\n",
    "    log = defaultdict(list)\n",
    "    num_cols = []\n",
    "    corrected = 0\n",
    "    total_mistakes = 0\n",
    "    num_permutations = {}\n",
    "    init_permutation = {}\n",
    "    init_correctness = {}\n",
    "    score_init = {}\n",
    "    score_permutation = defaultdict(list)\n",
    "    permutation_correctness = defaultdict(list)\n",
    "    for batch_idx, batch in enumerate(valid_dataloader_iter):\n",
    "        cls_indexes = torch.LongTensor([[0, batch[\"cls_indexes\"].cpu().item()]]).to(device)\n",
    "        target_col_mask = batch[\"target_col_mask\"].T\n",
    "        logits = model(batch[\"data\"].T, cls_indexes=cls_indexes,)\n",
    "        score_init[batch_idx] = F.softmax(logits.detach()).max().item()\n",
    "        label_i = batch[\"label\"].item()\n",
    "        predict_init = logits.argmax().item()\n",
    "        init_permutation_i = get_permutation(target_col_mask)\n",
    "        init_permutation[batch_idx] = init_permutation_i\n",
    "        if predict_init == label_i:\n",
    "            init_correctness[batch_idx] = True\n",
    "        else:\n",
    "            init_correctness[batch_idx] = False\n",
    "        num_permutations[batch_idx] = 0\n",
    "        if 1 in target_col_mask:\n",
    "            col_idx_set = target_col_mask.unique().tolist()\n",
    "            assert -1 not in col_idx_set\n",
    "            for r in range(len(init_permutation_i), max(len(init_permutation_i)//2, len(init_permutation_i)-3), -1): # not \n",
    "                for x in itertools.combinations(init_permutation_i, r):\n",
    "                    if 0 not in x:\n",
    "                        continue\n",
    "\n",
    "                    num_permutations[batch_idx] += 1\n",
    "                    new_batch_data = []\n",
    "                    for col_i in x:\n",
    "                        if col_i == 0:\n",
    "                            if len(new_batch_data) == 0:\n",
    "                                cls_indexes_value = 0\n",
    "                            else:\n",
    "                                cls_indexes_value = sum([len(new_batch_data[i]) for i in range(len(new_batch_data))])\n",
    "                        new_batch_data.append(batch[\"data\"].T[target_col_mask==col_i])\n",
    "                    new_batch_data = torch.cat(new_batch_data, dim=-1).reshape(1, -1)\n",
    "                    cls_indexes = torch.tensor([0, cls_indexes_value]).reshape(1, -1).to(device)\n",
    "                    logits_temp = model(new_batch_data, cls_indexes=cls_indexes,)\n",
    "                    ood_score_temp = F.softmax(logits_temp.detach()).max().item()\n",
    "                    score_permutation[batch_idx].append(ood_score_temp)\n",
    "                    predict_temp = logits_temp.argmax().item()\n",
    "                    permutation_correctness[batch_idx].append(predict_temp == label_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation_correctness_ratio = []\n",
    "score_permutation_avg = []\n",
    "for i in score_init:\n",
    "    permutation_correctness_ratio.append(np.mean(permutation_correctness[i]))\n",
    "    score_permutation_avg.append(np.mean(score_permutation[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.histplot(permutation_correctness_ratio)\n",
    "permutation_correctness_ratio = np.array(permutation_correctness_ratio)\n",
    "print(np.mean(permutation_correctness_ratio[~np.isnan(permutation_correctness_ratio)]), sum(permutation_correctness_ratio==1)/len(permutation_correctness_ratio), sum(permutation_correctness_ratio==0)/len(permutation_correctness_ratio), (sum(permutation_correctness_ratio==0)+sum(permutation_correctness_ratio==1))/len(permutation_correctness_ratio))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(labels_valid[permutation_correctness_ratio==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(labels_valid[permutation_correctness_ratio==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(labels_valid[(permutation_correctness_ratio==1)|(permutation_correctness_ratio==0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation_correctness_ratio.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_init = torch.tensor(list(score_init.values()))\n",
    "score_init.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation_correctness_ratio = torch.tensor(permutation_correctness_ratio)\n",
    "print(score_init[permutation_correctness_ratio==1].mean())\n",
    "False in torch.tensor(list(init_correctness.values()))[permutation_correctness_ratio==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(score_init[permutation_correctness_ratio==0].mean())\n",
    "True in torch.tensor(list(init_correctness.values()))[permutation_correctness_ratio==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_permutations_all = num_permutations.values()\n",
    "sns.histplot(num_permutations_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_permutations.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(num_permutations[i], len(init_permutation[i]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_correct_permutations = [sum(permutation_correctness[i]) for i in permutation_correctness]\n",
    "num_wrong_permutations = [len(permutation_correctness[i])-sum(permutation_correctness[i]) for i in permutation_correctness]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.median(num_correct_permutations))\n",
    "print(torch.tensor(num_correct_permutations).unique())\n",
    "sns.histplot(num_correct_permutations )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.median(num_wrong_permutations))\n",
    "sns.histplot(num_wrong_permutations )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(num_wrong_permutations).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, batch in enumerate(valid_dataloader_iter):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "            veri_data[batch_idx].append(new_batch_data.cpu())\n",
    "            veri_label[batch_idx].append(torch.tensor(predict_temp == label_i).long())\n",
    "            veri_cls_indexes[batch_idx].append(cls_indexes_value)\n",
    "            veri_embs[batch_idx].append(embs_temp.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "veri_embs[batch_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1 in target_col_mask:\n",
    "    col_idx_set = target_col_mask.unique().tolist()\n",
    "    assert -1 not in col_idx_set\n",
    "    for r in range(len(init_permutation_i), max(len(init_permutation_i)//2, len(init_permutation_i)-3), -1): # not \n",
    "        for x in itertools.combinations(init_permutation_i, r):\n",
    "            if 0 not in x:\n",
    "                continue\n",
    "\n",
    "            num_permutations[batch_idx] += 1\n",
    "            new_batch_data = []\n",
    "            for col_i in x:\n",
    "                if col_i == 0:\n",
    "                    if len(new_batch_data) == 0:\n",
    "                        cls_indexes_value = 0\n",
    "                    else:\n",
    "                        cls_indexes_value = sum([len(new_batch_data[i]) for i in range(len(new_batch_data))])\n",
    "                new_batch_data.append(batch[\"data\"].T[target_col_mask==col_i])\n",
    "            new_batch_data = torch.cat(new_batch_data, dim=-1).reshape(1, -1)\n",
    "            cls_indexes = torch.tensor([0, cls_indexes_value]).reshape(1, -1).to(device)\n",
    "            logits_temp, embs_temp = model(new_batch_data, cls_indexes=cls_indexes, get_enc=True)\n",
    "            logits_temp = logits_temp.detach().cpu()\n",
    "            ood_score_temp = F.softmax(logits_temp.detach()).max().item()\n",
    "            score_permutation[batch_idx].append(ood_score_temp)\n",
    "            predict_temp = logits_temp.argmax().item()\n",
    "            permutation_correctness[batch_idx].append(predict_temp == label_i)\n",
    "            \n",
    "            # veri_data[batch_idx].append(new_batch_data.cpu())\n",
    "            # veri_label[batch_idx].append(torch.tensor(predict_temp == label_i).long())\n",
    "            # veri_cls_indexes[batch_idx].append(cls_indexes_value)\n",
    "            # veri_embs[batch_idx].append(embs_temp.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# brute force perumate in validation TODO: restrict mamimus length of permutation\n",
    "import itertools\n",
    "\n",
    "def is_sublist(A, B):\n",
    "    it = iter(B)\n",
    "    return all(x in it for x in A)\n",
    "def get_permutation(x):\n",
    "    new = []\n",
    "    x = x.tolist()\n",
    "    if len(x) == 1:\n",
    "        x = x[0]\n",
    "    for k in x:\n",
    "        if k not in new:\n",
    "            new.append(k)\n",
    "    return new\n",
    "\n",
    "import torch.nn.functional as F\n",
    "train_data = defaultdict(list)\n",
    "train_logits = defaultdict(list)\n",
    "train_cls_indexes = defaultdict(list)\n",
    "train_target_col_mask = defaultdict(list)\n",
    "train_embs = defaultdict(list)\n",
    "train_col_num = defaultdict(list)\n",
    "train_label = defaultdict(list)\n",
    "train_class = defaultdict(list)\n",
    "train_target_embs = defaultdict(list)\n",
    "    \n",
    "model.load_state_dict(best_state_dict, strict=False)\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "change_log = []\n",
    "score_init = []\n",
    "score_best = [] \n",
    "for threshold in [1.5]:\n",
    "    print(f\"*********************Threshold: {threshold}****************************\")\n",
    "    ft_embs_test = []\n",
    "    labels_test = []\n",
    "    logits_test = []\n",
    "    log = defaultdict(list)\n",
    "    num_cols = []\n",
    "    corrected = 0\n",
    "    total_mistakes = 0\n",
    "    num_permutations = {}\n",
    "    init_permutation = {}\n",
    "    init_correctness = {}\n",
    "    score_init = {}\n",
    "    score_permutation = defaultdict(list)\n",
    "    permutation_correctness = defaultdict(list)\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(train_dataloader_iter):\n",
    "            cls_indexes = torch.LongTensor([[0, batch[\"cls_indexes\"].cpu().item()]]).to(device)\n",
    "            target_col_mask = batch[\"target_col_mask\"].T\n",
    "            logits, embs = model(batch[\"data\"].T, cls_indexes=cls_indexes, get_enc=True)\n",
    "            score_init[batch_idx] = F.softmax(logits.detach()).max().item()\n",
    "            label_i = batch[\"label\"].item()\n",
    "            predict_init = logits.argmax().item()\n",
    "            init_permutation_i = get_permutation(target_col_mask)\n",
    "            init_permutation[batch_idx] = init_permutation_i\n",
    "            if predict_init == label_i:\n",
    "                init_correctness[batch_idx] = True\n",
    "            else:\n",
    "                init_correctness[batch_idx] = False\n",
    "            num_permutations[batch_idx] = 0\n",
    "\n",
    "            train_data[batch_idx].append(batch[\"data\"].T.cpu())\n",
    "            train_logits[batch_idx].append(logits.detach().cpu())\n",
    "            train_cls_indexes[batch_idx].append(batch[\"cls_indexes\"].cpu().item())\n",
    "            train_embs[batch_idx].append(embs.cpu())\n",
    "            train_col_num[batch_idx].append(len(init_permutation_i))\n",
    "            train_label[batch_idx].append(torch.tensor(predict_init == label_i).long()) # indicate whether the permutation is correct or not\n",
    "            train_class[batch_idx].append(label_i)\n",
    "\n",
    "\n",
    "            x = [0]\n",
    "            new_batch_data = []\n",
    "            for col_i in x:\n",
    "                if col_i == 0:\n",
    "                    if len(new_batch_data) == 0:\n",
    "                        cls_indexes_value = 0\n",
    "                    else:\n",
    "                        cls_indexes_value = sum([len(new_batch_data[i]) for i in range(len(new_batch_data))])\n",
    "                new_batch_data.append(batch[\"data\"].T[target_col_mask==col_i])\n",
    "            new_batch_data = torch.cat(new_batch_data, dim=-1).reshape(1, -1)\n",
    "            cls_indexes = torch.tensor([0, cls_indexes_value]).reshape(1, -1).to(device)\n",
    "            logits_temp, embs_temp = model(new_batch_data, cls_indexes=cls_indexes, get_enc=True)\n",
    "            train_target_embs[batch_idx].append(embs_temp.cpu())\n",
    "\n",
    "            num_cols.append(len(init_permutation_i))\n",
    "            labels_test.append(batch[\"label\"].cpu().item())\n",
    "            col_idx_set = target_col_mask.unique().tolist()\n",
    "            assert -1 not in col_idx_set\n",
    "            for r in range(len(init_permutation_i)-1, 0, -1): # not \n",
    "                for x in itertools.combinations(init_permutation_i, r):\n",
    "                    if 0 not in x:\n",
    "                        continue\n",
    "\n",
    "                    num_permutations[batch_idx] += 1\n",
    "                    new_batch_data = []\n",
    "                    for col_i in x:\n",
    "                        if col_i == 0:\n",
    "                            if len(new_batch_data) == 0:\n",
    "                                cls_indexes_value = 0\n",
    "                            else:\n",
    "                                cls_indexes_value = sum([len(new_batch_data[i]) for i in range(len(new_batch_data))])\n",
    "                        new_batch_data.append(batch[\"data\"].T[target_col_mask==col_i])\n",
    "                    new_batch_data = torch.cat(new_batch_data, dim=-1).reshape(1, -1)\n",
    "                    cls_indexes = torch.tensor([0, cls_indexes_value]).reshape(1, -1).to(device)\n",
    "                    logits_temp, embs_temp = model(new_batch_data, cls_indexes=cls_indexes, get_enc=True)\n",
    "                    ood_score_temp = F.softmax(logits_temp.detach()).max().item()\n",
    "                    score_permutation[batch_idx].append(ood_score_temp)\n",
    "                    predict_temp = logits_temp.argmax().item()\n",
    "                    permutation_correctness[batch_idx].append(predict_temp == label_i)\n",
    "                    if predict_temp == label_i:\n",
    "                        continue\n",
    "                    \n",
    "                    \n",
    "                    train_data[batch_idx].append(new_batch_data.cpu())\n",
    "                    train_logits[batch_idx].append(logits_temp.detach().cpu())\n",
    "                    train_cls_indexes[batch_idx].append(cls_indexes_value)\n",
    "                    train_embs[batch_idx].append(embs_temp.cpu())\n",
    "                    train_col_num[batch_idx].append(len(x))\n",
    "                    train_label[batch_idx].append(torch.tensor(predict_temp == label_i).long()) # indicate whether the permutation is correct or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# brute force perumate in validation TODO: restrict mamimus length of permutation\n",
    "import itertools\n",
    "\n",
    "def is_sublist(A, B):\n",
    "    it = iter(B)\n",
    "    return all(x in it for x in A)\n",
    "def get_permutation(x):\n",
    "    new = []\n",
    "    x = x.tolist()\n",
    "    if len(x) == 1:\n",
    "        x = x[0]\n",
    "    for k in x:\n",
    "        if k not in new:\n",
    "            new.append(k)\n",
    "    return new\n",
    "\n",
    "import torch.nn.functional as F\n",
    "train_data = defaultdict(list)\n",
    "train_logits = defaultdict(list)\n",
    "train_cls_indexes = defaultdict(list)\n",
    "train_target_col_mask = defaultdict(list)\n",
    "train_embs = defaultdict(list)\n",
    "train_col_num = defaultdict(list)\n",
    "train_label = defaultdict(list)\n",
    "train_class = defaultdict(list)\n",
    "train_target_embs = defaultdict(list)\n",
    "    \n",
    "model.load_state_dict(best_state_dict, strict=False)\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "change_log = []\n",
    "score_init = []\n",
    "score_best = [] \n",
    "max_col_length = 3\n",
    "for threshold in [1.5]:\n",
    "    print(f\"*********************Threshold: {threshold}****************************\")\n",
    "    ft_embs_test = []\n",
    "    labels_test = []\n",
    "    logits_test = []\n",
    "    log = defaultdict(list)\n",
    "    num_cols = []\n",
    "    corrected = 0\n",
    "    total_mistakes = 0\n",
    "    num_permutations = {}\n",
    "    init_permutation = {}\n",
    "    init_correctness = {}\n",
    "    score_init = {}\n",
    "    score_permutation = defaultdict(list)\n",
    "    permutation_correctness = defaultdict(list)\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(train_dataloader_iter):\n",
    "            cls_indexes = torch.LongTensor([[0, batch[\"cls_indexes\"].cpu().item()]]).to(device)\n",
    "            target_col_mask = batch[\"target_col_mask\"].T\n",
    "            logits, embs = model(batch[\"data\"].T, cls_indexes=cls_indexes, get_enc=True)\n",
    "            score_init[batch_idx] = F.softmax(logits.detach()).max().item()\n",
    "            label_i = batch[\"label\"].item()\n",
    "            predict_init = logits.argmax().item()\n",
    "            init_permutation_i = get_permutation(target_col_mask)\n",
    "            init_permutation[batch_idx] = init_permutation_i\n",
    "            if predict_init == label_i:\n",
    "                init_correctness[batch_idx] = True\n",
    "            else:\n",
    "                init_correctness[batch_idx] = False\n",
    "            num_permutations[batch_idx] = 0\n",
    "\n",
    "            train_data[batch_idx].append(batch[\"data\"].T.cpu())\n",
    "            train_logits[batch_idx].append(logits.detach().cpu())\n",
    "            train_cls_indexes[batch_idx].append(batch[\"cls_indexes\"].cpu().item())\n",
    "            train_embs[batch_idx].append(embs.cpu())\n",
    "            train_col_num[batch_idx].append(len(init_permutation_i))\n",
    "            train_label[batch_idx].append(torch.tensor(predict_init == label_i).long()) # indicate whether the permutation is correct or not\n",
    "            train_class[batch_idx].append(label_i)\n",
    "\n",
    "\n",
    "            x = [0]\n",
    "            new_batch_data = []\n",
    "            for col_i in x:\n",
    "                if col_i == 0:\n",
    "                    if len(new_batch_data) == 0:\n",
    "                        cls_indexes_value = 0\n",
    "                    else:\n",
    "                        cls_indexes_value = sum([len(new_batch_data[i]) for i in range(len(new_batch_data))])\n",
    "                new_batch_data.append(batch[\"data\"].T[target_col_mask==col_i])\n",
    "            new_batch_data = torch.cat(new_batch_data, dim=-1).reshape(1, -1)\n",
    "            cls_indexes = torch.tensor([0, cls_indexes_value]).reshape(1, -1).to(device)\n",
    "            logits_temp, embs_temp = model(new_batch_data, cls_indexes=cls_indexes, get_enc=True)\n",
    "            train_target_embs[batch_idx].append(embs_temp.cpu())\n",
    "\n",
    "            num_cols.append(len(init_permutation_i))\n",
    "            labels_test.append(batch[\"label\"].cpu().item())\n",
    "            col_idx_set = target_col_mask.unique().tolist()\n",
    "            assert -1 not in col_idx_set\n",
    "            for r in range(1, min(len(col_idx_set), max_col_length) + 1):\n",
    "                for subset in itertools.combinations(col_idx_set, r):\n",
    "                    if 0 not in subset:\n",
    "                        continue\n",
    "                    for x in itertools.permutations(subset):\n",
    "                        num_permutations[batch_idx] += 1\n",
    "                        new_batch_data = []\n",
    "                        for col_i in x:\n",
    "                            if col_i == 0:\n",
    "                                if len(new_batch_data) == 0:\n",
    "                                    cls_indexes_value = 0\n",
    "                                else:\n",
    "                                    cls_indexes_value = sum([len(new_batch_data[i]) for i in range(len(new_batch_data))])\n",
    "                            new_batch_data.append(batch[\"data\"].T[target_col_mask==col_i])\n",
    "                        new_batch_data = torch.cat(new_batch_data, dim=-1).reshape(1, -1)\n",
    "                        cls_indexes = torch.tensor([0, cls_indexes_value]).reshape(1, -1).to(device)\n",
    "                        logits_temp, embs_temp = model(new_batch_data, cls_indexes=cls_indexes, get_enc=True)\n",
    "                        ood_score_temp = F.softmax(logits_temp.detach()).max().item()\n",
    "                        score_permutation[batch_idx].append(ood_score_temp)\n",
    "                        predict_temp = logits_temp.argmax().item()\n",
    "                        permutation_correctness[batch_idx].append(predict_temp == label_i)\n",
    "                        if predict_temp == label_i:\n",
    "                            continue\n",
    "                        \n",
    "                        \n",
    "                        train_data[batch_idx].append(new_batch_data.cpu())\n",
    "                        train_logits[batch_idx].append(logits_temp.detach().cpu())\n",
    "                        train_cls_indexes[batch_idx].append(cls_indexes_value)\n",
    "                        train_embs[batch_idx].append(embs_temp.cpu())\n",
    "                        train_col_num[batch_idx].append(len(x))\n",
    "                        train_label[batch_idx].append(torch.tensor(predict_temp == label_i).long()) # indicate whether the permutation is correct or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = torch.tensor(num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"/data/zhihao/TU/Watchog/trainfication\", exist_ok=True)\n",
    "torch.save({\"data\": train_data, \"label\": train_label, \"logits\": train_logits, \"cls_indexes\": train_cls_indexes, \"embs\": train_embs, \"target_embs\":train_target_embs , \"col_num\": train_col_num}, f\"/data/zhihao/TU/Watchog/trainfication/{args.task}_train_data_7.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# brute force perumate in validation TODO: restrict mamimus length of permutation\n",
    "import itertools\n",
    "\n",
    "def is_sublist(A, B):\n",
    "    it = iter(B)\n",
    "    return all(x in it for x in A)\n",
    "def get_permutation(x):\n",
    "    new = []\n",
    "    x = x.tolist()\n",
    "    if len(x) == 1:\n",
    "        x = x[0]\n",
    "    for k in x:\n",
    "        if k not in new:\n",
    "            new.append(k)\n",
    "    return new\n",
    "\n",
    "import torch.nn.functional as F\n",
    "train_data = defaultdict(list)\n",
    "train_logits = defaultdict(list)\n",
    "train_cls_indexes = defaultdict(list)\n",
    "train_target_col_mask = defaultdict(list)\n",
    "train_embs = defaultdict(list)\n",
    "train_col_num = defaultdict(list)\n",
    "train_label = defaultdict(list)\n",
    "train_class = defaultdict(list)\n",
    "train_target_embs = defaultdict(list)\n",
    "    \n",
    "model.load_state_dict(best_state_dict, strict=False)\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "change_log = []\n",
    "score_init = []\n",
    "score_best = [] \n",
    "max_col_length = 3\n",
    "for threshold in [1.5]:\n",
    "    print(f\"*********************Threshold: {threshold}****************************\")\n",
    "    ft_embs_test = []\n",
    "    labels_test = []\n",
    "    logits_test = []\n",
    "    log = defaultdict(list)\n",
    "    num_cols = []\n",
    "    corrected = 0\n",
    "    total_mistakes = 0\n",
    "    num_permutations = {}\n",
    "    init_permutation = {}\n",
    "    init_correctness = {}\n",
    "    score_init = {}\n",
    "    score_permutation = defaultdict(list)\n",
    "    permutation_correctness = defaultdict(list)\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(train_dataloader_iter):\n",
    "            cls_indexes = torch.LongTensor([[0, batch[\"cls_indexes\"].cpu().item()]]).to(device)\n",
    "            target_col_mask = batch[\"target_col_mask\"].T\n",
    "            logits, embs = model(batch[\"data\"].T, cls_indexes=cls_indexes, get_enc=True)\n",
    "            score_init[batch_idx] = F.softmax(logits.detach()).max().item()\n",
    "            label_i = batch[\"label\"].item()\n",
    "            predict_init = logits.argmax().item()\n",
    "            init_permutation_i = get_permutation(target_col_mask)\n",
    "            init_permutation[batch_idx] = init_permutation_i\n",
    "            if predict_init == label_i:\n",
    "                init_correctness[batch_idx] = True\n",
    "            else:\n",
    "                init_correctness[batch_idx] = False\n",
    "            num_permutations[batch_idx] = 0\n",
    "\n",
    "            train_data[batch_idx].append(batch[\"data\"].T.cpu())\n",
    "            train_logits[batch_idx].append(logits.detach().cpu())\n",
    "            train_cls_indexes[batch_idx].append(batch[\"cls_indexes\"].cpu().item())\n",
    "            train_embs[batch_idx].append(embs.cpu())\n",
    "            train_col_num[batch_idx].append(len(init_permutation_i))\n",
    "            train_label[batch_idx].append(torch.tensor(predict_init == label_i).long()) # indicate whether the permutation is correct or not\n",
    "            train_class[batch_idx].append(label_i)\n",
    "\n",
    "\n",
    "            x = [0]\n",
    "            new_batch_data = []\n",
    "            for col_i in x:\n",
    "                if col_i == 0:\n",
    "                    if len(new_batch_data) == 0:\n",
    "                        cls_indexes_value = 0\n",
    "                    else:\n",
    "                        cls_indexes_value = sum([len(new_batch_data[i]) for i in range(len(new_batch_data))])\n",
    "                new_batch_data.append(batch[\"data\"].T[target_col_mask==col_i])\n",
    "            new_batch_data = torch.cat(new_batch_data, dim=-1).reshape(1, -1)\n",
    "            cls_indexes = torch.tensor([0, cls_indexes_value]).reshape(1, -1).to(device)\n",
    "            logits_temp, embs_temp = model(new_batch_data, cls_indexes=cls_indexes, get_enc=True)\n",
    "            train_target_embs[batch_idx].append(embs_temp.cpu())\n",
    "\n",
    "            num_cols.append(len(init_permutation_i))\n",
    "            labels_test.append(batch[\"label\"].cpu().item())\n",
    "            col_idx_set = target_col_mask.unique().tolist()\n",
    "            assert -1 not in col_idx_set\n",
    "            for r in range(1, min(len(col_idx_set), max_col_length) + 1):\n",
    "                for subset in itertools.combinations(col_idx_set, r):\n",
    "                    if 0 not in subset:\n",
    "                        continue\n",
    "                    for x in itertools.permutations(subset):\n",
    "                        num_permutations[batch_idx] += 1\n",
    "                        new_batch_data = []\n",
    "                        for col_i in x:\n",
    "                            if col_i == 0:\n",
    "                                if len(new_batch_data) == 0:\n",
    "                                    cls_indexes_value = 0\n",
    "                                else:\n",
    "                                    cls_indexes_value = sum([len(new_batch_data[i]) for i in range(len(new_batch_data))])\n",
    "                            new_batch_data.append(batch[\"data\"].T[target_col_mask==col_i])\n",
    "                        new_batch_data = torch.cat(new_batch_data, dim=-1).reshape(1, -1)\n",
    "                        cls_indexes = torch.tensor([0, cls_indexes_value]).reshape(1, -1).to(device)\n",
    "                        logits_temp, embs_temp = model(new_batch_data, cls_indexes=cls_indexes, get_enc=True)\n",
    "                        ood_score_temp = F.softmax(logits_temp.detach()).max().item()\n",
    "                        score_permutation[batch_idx].append(ood_score_temp)\n",
    "                        predict_temp = logits_temp.argmax().item()\n",
    "                        permutation_correctness[batch_idx].append(predict_temp == label_i)\n",
    "\n",
    "                        \n",
    "                        \n",
    "                        train_data[batch_idx].append(new_batch_data.cpu())\n",
    "                        train_logits[batch_idx].append(logits_temp.detach().cpu())\n",
    "                        train_cls_indexes[batch_idx].append(cls_indexes_value)\n",
    "                        train_embs[batch_idx].append(embs_temp.cpu())\n",
    "                        train_col_num[batch_idx].append(len(x))\n",
    "                        train_label[batch_idx].append(torch.tensor(predict_temp == label_i).long()) # indicate whether the permutation is correct or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"/data/zhihao/TU/Watchog/trainfication\", exist_ok=True)\n",
    "torch.save({\"data\": train_data, \"label\": train_label, \"logits\": train_logits, \"cls_indexes\": train_cls_indexes, \"embs\": train_embs, \"target_embs\":train_target_embs , \"col_num\": train_col_num}, f\"/data/zhihao/TU/Watchog/trainfication/{args.task}_train_data_8.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_all = torch.cat([torch.tensor(train_label[i]) for i in train_label], dim=0)\n",
    "print(len(train_labels_all))\n",
    "train_labels_all.sum()/len(train_labels_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V6\n",
    "train_labels_all = torch.cat([torch.tensor(train_label[i]) for i in train_label], dim=0)\n",
    "print(len(train_labels_all))\n",
    "train_labels_all.sum()/len(train_labels_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V8\n",
    "train_labels_all = torch.cat([torch.tensor(train_label[i]) for i in train_label], dim=0)\n",
    "print(len(train_labels_all))\n",
    "train_labels_all.sum()/len(train_labels_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V4\n",
    "train_labels_all = torch.cat([torch.tensor(train_label[i]) for i in train_label], dim=0)\n",
    "print(len(train_labels_all))\n",
    "train_labels_all.sum()/len(train_labels_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "os.makedirs(\"/data/zhihao/TU/Watchog/verification\", exist_ok=True)\n",
    "# {args.task}_veri_data: min length: max(len(init_permutation_i)//2, len(init_permutation_i)-3)\n",
    "# {args.task}_veri_data_1: min length: for those without correct permutation, \n",
    "veri_results = torch.load( f\"/data/zhihao/TU/Watchog/verification/{args.task}_veri_data_5.pth\")\n",
    "veri_data = veri_results[\"data\"]\n",
    "veri_label = veri_results[\"label\"]\n",
    "veri_logits = veri_results[\"logits\"]\n",
    "veri_cls_indexes = veri_results[\"cls_indexes\"]\n",
    "veri_embs = veri_results[\"embs\"]\n",
    "veri_target_embs = veri_results[\"target_embs\"]\n",
    "veri_col_num = veri_results[\"col_num\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = len(train_data)\n",
    "for i in veri_data:\n",
    "    train_data[num_train+i] = veri_data[i]\n",
    "    train_label[num_train+i] = veri_label[i]\n",
    "    train_logits[num_train+i] = veri_logits[i]\n",
    "    train_cls_indexes[num_train+i] = veri_cls_indexes[i]\n",
    "    train_embs[num_train+i] = veri_embs[i]\n",
    "    train_target_embs[num_train+i] = veri_target_embs[i]\n",
    "    train_col_num[num_train+i] = veri_col_num[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# veri V5\n",
    "train_labels_all = torch.cat([torch.tensor(train_label[i]) for i in train_label], dim=0)\n",
    "print(len(train_labels_all))\n",
    "train_labels_all.sum()/len(train_labels_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_init_mask = []\n",
    "for i in train_label:\n",
    "    k = 0\n",
    "    for j in train_label[i]:\n",
    "        if k == 0:\n",
    "            train_init_mask.append(True)\n",
    "        else:\n",
    "            train_init_mask.append(False)\n",
    "        k += 1\n",
    "train_init_mask = torch.tensor(train_init_mask)\n",
    "train_labels_all[train_init_mask] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_init_idx = train_init_mask.nonzero().reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((train_labels_all==2).sum(), (train_labels_all==2).sum()/len(train_labels_all))\n",
    "print((train_labels_all==1).sum(), (train_labels_all==1).sum()/len(train_labels_all))\n",
    "print((train_labels_all==0).sum(), (train_labels_all==0).sum()/len(train_labels_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# veri V5\n",
    "train_embs_all = torch.cat([torch.stack(train_embs[i], dim=0) for i in train_embs], dim=0)\n",
    "print(train_embs_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "172055/5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# veri V5 init\n",
    "\n",
    "train_embs_all_init = torch.stack([train_embs[i][0] for i in train_embs], dim=0)\n",
    "print(train_embs_all_init.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_idx = torch.randperm(len(train_embs_all))[:1000]\n",
    "chosen_embs = train_embs_all[chosen_idx]\n",
    "chosen_labels = train_labels_all[chosen_idx]\n",
    "# umap \n",
    "import umap\n",
    "reducer = umap.UMAP()\n",
    "embedding = reducer.fit_transform(chosen_embs)\n",
    "chosen_colors = []\n",
    "for label in chosen_labels:\n",
    "    if label == 0:\n",
    "        chosen_colors.append(\"red\")\n",
    "    elif label == 1:\n",
    "        chosen_colors.append(\"blue\")\n",
    "    else:\n",
    "        chosen_colors.append(\"green\")\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(embedding[:, 0], embedding[:, 1], c=chosen_colors, s=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_init_mask.nonzero().reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_idx = torch.randperm(len(train_embs_all))[:1000]\n",
    "chosen_init_idx = torch.randperm(len(train_init_idx))[:500]\n",
    "chosen_idx = torch.cat([chosen_idx, train_init_idx[chosen_init_idx]])\n",
    "chosen_embs = train_embs_all[chosen_idx]\n",
    "chosen_labels = train_labels_all[chosen_idx]\n",
    "# umap \n",
    "import umap\n",
    "reducer = umap.UMAP()\n",
    "embedding = reducer.fit_transform(chosen_embs)\n",
    "chosen_colors = []\n",
    "for label in chosen_labels:\n",
    "    if label == 0:\n",
    "        chosen_colors.append(\"red\")\n",
    "    elif label == 1:\n",
    "        chosen_colors.append(\"blue\")\n",
    "    else:\n",
    "        chosen_colors.append(\"green\")\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(embedding[:, 0], embedding[:, 1], c=chosen_colors, s=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.makedirs(\"/data/zhihao/TU/Watchog/trainfication\", exist_ok=True)\n",
    "veri_results = torch.load( f\"/data/zhihao/TU/Watchog/verification/{args.task}_veri_data_4.pth\")\n",
    "veri_embs_4 = veri_results[\"embs\"]\n",
    "veri_label_4 = veri_results[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.makedirs(\"/data/zhihao/TU/Watchog/trainfication\", exist_ok=True)\n",
    "veri_results = torch.load( f\"/data/zhihao/TU/Watchog/verification/{args.task}_veri_data_5.pth\")\n",
    "veri_embs_5 = veri_results[\"embs\"]\n",
    "veri_label_5 = veri_results[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# veri V5\n",
    "veri_labels_all_5 = torch.cat([torch.tensor(veri_label_5[i]) for i in veri_label_5], dim=0)\n",
    "print(len(veri_labels_all_5))\n",
    "veri_labels_all_5.sum()/len(veri_labels_all_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# veri V4\n",
    "veri_labels_all_4 = torch.cat([torch.tensor(veri_label_4[i]) for i in veri_label_4], dim=0)\n",
    "print(len(veri_labels_all_4))\n",
    "veri_labels_all_4.sum()/len(veri_labels_all_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# veri V4\n",
    "veri_embs_all_4 = torch.cat([torch.stack(veri_embs_4[i], dim=0) for i in veri_embs_4], dim=0)\n",
    "print(veri_embs_all_4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_idx = torch.randperm(len(veri_embs_all_4))[:1000]\n",
    "chosen_embs = veri_embs_all_4[chosen_idx]\n",
    "chosen_labels = veri_labels_all_4[chosen_idx]\n",
    "# umap \n",
    "import umap\n",
    "reducer = umap.UMAP()\n",
    "embedding = reducer.fit_transform(chosen_embs)\n",
    "chosen_colors = ['red' if label == 0 else 'blue' for label in chosen_labels]\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(embedding[:, 0], embedding[:, 1], c=chosen_colors, s=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"/data/zhihao/TU/Watchog/verification\", exist_ok=True)\n",
    "torch.save({\"data\": train_data, \"label\": train_label, \"logits\": train_logits, \"cls_indexes\": train_cls_indexes, \"embs\": train_embs, \"target_embs\":train_target_embs , \"col_num\": train_col_num}, f\"/data/zhihao/TU/Watchog/verification/{args.task}_veri_data_6.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randperm(len(train_labels_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# brute force perumate in validation TODO: restrict mamimus length of permutation\n",
    "import itertools\n",
    "\n",
    "def is_sublist(A, B):\n",
    "    it = iter(B)\n",
    "    return all(x in it for x in A)\n",
    "def get_permutation(x):\n",
    "    new = []\n",
    "    x = x.tolist()\n",
    "    if len(x) == 1:\n",
    "        x = x[0]\n",
    "    for k in x:\n",
    "        if k not in new:\n",
    "            new.append(k)\n",
    "    return new\n",
    "\n",
    "import torch.nn.functional as F\n",
    "veri_data = defaultdict(list)\n",
    "veri_logits = defaultdict(list)\n",
    "veri_cls_indexes = defaultdict(list)\n",
    "veri_target_col_mask = defaultdict(list)\n",
    "veri_embs = defaultdict(list)\n",
    "veri_col_num = defaultdict(list)\n",
    "veri_label = defaultdict(list)\n",
    "veri_class = defaultdict(list)\n",
    "veri_target_embs = defaultdict(list)\n",
    "    \n",
    "model.load_state_dict(best_state_dict, strict=False)\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "change_log = []\n",
    "score_init = []\n",
    "score_best = [] \n",
    "for threshold in [1.5]:\n",
    "    print(f\"*********************Threshold: {threshold}****************************\")\n",
    "    ft_embs_test = []\n",
    "    labels_test = []\n",
    "    logits_test = []\n",
    "    log = defaultdict(list)\n",
    "    num_cols = []\n",
    "    corrected = 0\n",
    "    total_mistakes = 0\n",
    "    num_permutations = {}\n",
    "    init_permutation = {}\n",
    "    init_correctness = {}\n",
    "    score_init = {}\n",
    "    score_permutation = defaultdict(list)\n",
    "    permutation_correctness = defaultdict(list)\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(valid_dataloader_iter):\n",
    "            cls_indexes = torch.LongTensor([[0, batch[\"cls_indexes\"].cpu().item()]]).to(device)\n",
    "            target_col_mask = batch[\"target_col_mask\"].T\n",
    "            logits = model(batch[\"data\"].T, cls_indexes=cls_indexes,)\n",
    "            score_init[batch_idx] = F.softmax(logits.detach()).max().item()\n",
    "            label_i = batch[\"label\"].item()\n",
    "            predict_init = logits.argmax().item()\n",
    "            init_permutation_i = get_permutation(target_col_mask)\n",
    "            init_permutation[batch_idx] = init_permutation_i\n",
    "            if predict_init == label_i:\n",
    "                init_correctness[batch_idx] = True\n",
    "            else:\n",
    "                init_correctness[batch_idx] = False\n",
    "            num_permutations[batch_idx] = 0\n",
    "\n",
    "            x = [0]\n",
    "            new_batch_data = []\n",
    "            for col_i in x:\n",
    "                if col_i == 0:\n",
    "                    if len(new_batch_data) == 0:\n",
    "                        cls_indexes_value = 0\n",
    "                    else:\n",
    "                        cls_indexes_value = sum([len(new_batch_data[i]) for i in range(len(new_batch_data))])\n",
    "                new_batch_data.append(batch[\"data\"].T[target_col_mask==col_i])\n",
    "            new_batch_data = torch.cat(new_batch_data, dim=-1).reshape(1, -1)\n",
    "            cls_indexes = torch.tensor([0, cls_indexes_value]).reshape(1, -1).to(device)\n",
    "            logits_temp, embs_temp = model(new_batch_data, cls_indexes=cls_indexes, get_enc=True)\n",
    "            veri_target_embs[batch_idx].append(embs_temp.cpu())\n",
    "\n",
    "            num_cols.append(len(init_permutation_i))\n",
    "            labels_test.append(batch[\"label\"].cpu().item())\n",
    "            col_idx_set = target_col_mask.unique().tolist()\n",
    "            assert -1 not in col_idx_set\n",
    "            for r in range(len(init_permutation_i), max(len(init_permutation_i)//2, len(init_permutation_i)-3), -1): # not \n",
    "                for x in itertools.combinations(init_permutation_i, r):\n",
    "                    if 0 not in x:\n",
    "                        continue\n",
    "\n",
    "                    num_permutations[batch_idx] += 1\n",
    "                    new_batch_data = []\n",
    "                    for col_i in x:\n",
    "                        if col_i == 0:\n",
    "                            if len(new_batch_data) == 0:\n",
    "                                cls_indexes_value = 0\n",
    "                            else:\n",
    "                                cls_indexes_value = sum([len(new_batch_data[i]) for i in range(len(new_batch_data))])\n",
    "                        new_batch_data.append(batch[\"data\"].T[target_col_mask==col_i])\n",
    "                    new_batch_data = torch.cat(new_batch_data, dim=-1).reshape(1, -1)\n",
    "                    cls_indexes = torch.tensor([0, cls_indexes_value]).reshape(1, -1).to(device)\n",
    "                    logits_temp, embs_temp = model(new_batch_data, cls_indexes=cls_indexes, get_enc=True)\n",
    "                    ood_score_temp = F.softmax(logits_temp.detach()).max().item()\n",
    "                    score_permutation[batch_idx].append(ood_score_temp)\n",
    "                    predict_temp = logits_temp.argmax().item()\n",
    "                    permutation_correctness[batch_idx].append(predict_temp == label_i)\n",
    "                    \n",
    "                    veri_data[batch_idx].append(new_batch_data.cpu())\n",
    "                    veri_logits[batch_idx].append(logits_temp.detach().cpu())\n",
    "                    veri_cls_indexes[batch_idx].append(cls_indexes_value)\n",
    "                    veri_embs[batch_idx].append(embs_temp.cpu())\n",
    "                    veri_col_num[batch_idx].append(len(x))\n",
    "                    veri_label[batch_idx].append(torch.tensor(predict_temp == label_i).long()) # indicate whether the permutation is correct or not\n",
    "                    veri_class[batch_idx].append(label_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop one column for column correlation\n",
    "import itertools\n",
    "\n",
    "def is_sublist(A, B):\n",
    "    it = iter(B)\n",
    "    return all(x in it for x in A)\n",
    "def get_permutation(x):\n",
    "    new = []\n",
    "    x = x.tolist()\n",
    "    if len(x) == 1:\n",
    "        x = x[0]\n",
    "    for k in x:\n",
    "        if k not in new:\n",
    "            new.append(k)\n",
    "    return new\n",
    "\n",
    "import torch.nn.functional as F\n",
    "veri_data = defaultdict(list)\n",
    "veri_logits = defaultdict(list)\n",
    "veri_cls_indexes = defaultdict(list)\n",
    "veri_target_col_mask = defaultdict(list)\n",
    "veri_embs = defaultdict(list)\n",
    "veri_col_num = defaultdict(list)\n",
    "veri_label = defaultdict(list)\n",
    "veri_class = defaultdict(list)\n",
    "veri_target_embs = defaultdict(list)\n",
    "    \n",
    "model.load_state_dict(best_state_dict, strict=False)\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "change_log = []\n",
    "score_init = []\n",
    "score_best = [] \n",
    "for threshold in [1.5]:\n",
    "    print(f\"*********************Threshold: {threshold}****************************\")\n",
    "    ft_embs_test = []\n",
    "    labels_test = []\n",
    "    logits_test = []\n",
    "    log = defaultdict(list)\n",
    "    num_cols = []\n",
    "    corrected = 0\n",
    "    total_mistakes = 0\n",
    "    num_permutations = {}\n",
    "    init_permutation = {}\n",
    "    init_correctness = {}\n",
    "    score_init = {}\n",
    "    score_permutation = defaultdict(list)\n",
    "    permutation_correctness = defaultdict(list)\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(valid_dataloader_iter):\n",
    "            cls_indexes = torch.LongTensor([[0, batch[\"cls_indexes\"].cpu().item()]]).to(device)\n",
    "            target_col_mask = batch[\"target_col_mask\"].T\n",
    "            logits = model(batch[\"data\"].T, cls_indexes=cls_indexes,)\n",
    "            score_init[batch_idx] = F.softmax(logits.detach()).max().item()\n",
    "            label_i = batch[\"label\"].item()\n",
    "            predict_init = logits.argmax().item()\n",
    "            init_permutation_i = get_permutation(target_col_mask)\n",
    "            init_permutation[batch_idx] = init_permutation_i\n",
    "            if predict_init == label_i:\n",
    "                init_correctness[batch_idx] = True\n",
    "            else:\n",
    "                init_correctness[batch_idx] = False\n",
    "            num_permutations[batch_idx] = 0\n",
    "\n",
    "            x = [0]\n",
    "            new_batch_data = []\n",
    "            for col_i in x:\n",
    "                if col_i == 0:\n",
    "                    if len(new_batch_data) == 0:\n",
    "                        cls_indexes_value = 0\n",
    "                    else:\n",
    "                        cls_indexes_value = sum([len(new_batch_data[i]) for i in range(len(new_batch_data))])\n",
    "                new_batch_data.append(batch[\"data\"].T[target_col_mask==col_i])\n",
    "            new_batch_data = torch.cat(new_batch_data, dim=-1).reshape(1, -1)\n",
    "            cls_indexes = torch.tensor([0, cls_indexes_value]).reshape(1, -1).to(device)\n",
    "            logits_temp, embs_temp = model(new_batch_data, cls_indexes=cls_indexes, get_enc=True)\n",
    "            veri_target_embs[batch_idx].append(embs_temp.cpu())\n",
    "\n",
    "            num_cols.append(len(init_permutation_i))\n",
    "            labels_test.append(batch[\"label\"].cpu().item())\n",
    "            col_idx_set = target_col_mask.unique().tolist()\n",
    "            assert -1 not in col_idx_set\n",
    "            for r in range(len(init_permutation_i), len(init_permutation_i)-2, -1): # not \n",
    "                for x in itertools.combinations(init_permutation_i, r):\n",
    "                    if 0 not in x:\n",
    "                        continue\n",
    "\n",
    "                    num_permutations[batch_idx] += 1\n",
    "                    new_batch_data = []\n",
    "                    for col_i in x:\n",
    "                        if col_i == 0:\n",
    "                            if len(new_batch_data) == 0:\n",
    "                                cls_indexes_value = 0\n",
    "                            else:\n",
    "                                cls_indexes_value = sum([len(new_batch_data[i]) for i in range(len(new_batch_data))])\n",
    "                        new_batch_data.append(batch[\"data\"].T[target_col_mask==col_i])\n",
    "                    new_batch_data = torch.cat(new_batch_data, dim=-1).reshape(1, -1)\n",
    "                    cls_indexes = torch.tensor([0, cls_indexes_value]).reshape(1, -1).to(device)\n",
    "                    logits_temp, embs_temp = model(new_batch_data, cls_indexes=cls_indexes, get_enc=True)\n",
    "                    ood_score_temp = F.softmax(logits_temp.detach()).max().item()\n",
    "                    score_permutation[batch_idx].append(ood_score_temp)\n",
    "                    predict_temp = logits_temp.argmax().item()\n",
    "                    permutation_correctness[batch_idx].append(predict_temp == label_i)\n",
    "                    \n",
    "                    veri_data[batch_idx].append(new_batch_data.cpu())\n",
    "                    veri_logits[batch_idx].append(logits_temp.detach().cpu())\n",
    "                    veri_cls_indexes[batch_idx].append(cls_indexes_value)\n",
    "                    veri_embs[batch_idx].append(embs_temp.cpu())\n",
    "                    veri_col_num[batch_idx].append(len(x))\n",
    "                    if predict_init != label_i and predict_temp == label_i:\n",
    "                        veri_label[batch_idx].append(torch.tensor(1).long())\n",
    "                    elif predict_init == label_i and predict_temp != label_i:\n",
    "                        veri_label[batch_idx].append(torch.tensor(0).long())\n",
    "                    elif predict_init == label_i and predict_temp == label_i:\n",
    "                        veri_label[batch_idx].append(torch.tensor(2).long())\n",
    "                    else:\n",
    "                        veri_label[batch_idx].append(torch.tensor(3).long())\n",
    "                    veri_class[batch_idx].append(label_i)\n",
    "            assert len(veri_label[batch_idx]) == len(init_permutation_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"/data/zhihao/TU/Watchog/verification\", exist_ok=True)\n",
    "torch.save({\"data\": veri_data, \"label\": veri_label, \"logits\": veri_logits, \"cls_indexes\": veri_cls_indexes, \"embs\": veri_embs, \"target_embs\":veri_target_embs , \"col_num\": veri_col_num}, f\"/data/zhihao/TU/Watchog/verification/{args.task}_veri_drop_data_0.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for veri_i in veri_label:\n",
    "    context_embs = veri_embs[veri_i][0].reshape(-1)\n",
    "    for veri_j in range(1, len(veri_label[veri_i])):\n",
    "        if label_version == 0:\n",
    "            if veri_label[veri_i][veri_j] != 0 or veri_label[veri_i][veri_j-1] != 1:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "veri_labels_all = torch.cat([torch.tensor(veri_label[i]) for i in veri_label], dim=0)\n",
    "print(len(veri_labels_all))\n",
    "for i in range(4):\n",
    "    print((veri_labels_all==i).sum(), (veri_labels_all==i).sum()/len(veri_labels_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "veri_labels_all = torch.cat([torch.tensor(veri_label[i])[1:] for i in veri_label], dim=0)\n",
    "print(len(veri_labels_all))\n",
    "for i in range(4):\n",
    "    print((veri_labels_all==i).sum(), (veri_labels_all==i).sum()/len(veri_labels_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "        self.pos_iter[anchor_idx] = iter(self.veri_pos_embs[anchor_idx])\n",
    "        self.neg_iter[anchor_idx] = iter(self.veri_neg_embs[anchor_idx])\n",
    "        # Shuffle for randomness\n",
    "        random.shuffle(self.pos_embeddings[anchor_idx])\n",
    "        random.shuffle(self.neg_embeddings[anchor_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"/data/zhihao/TU/Watchog/verification\", exist_ok=True)\n",
    "torch.save({\"data\": veri_data, \"label\": veri_label, \"logits\": veri_logits, \"cls_indexes\": veri_cls_indexes, \"embs\": veri_embs, \"target_embs\":veri_target_embs , \"col_num\": veri_col_num}, f\"/data/zhihao/TU/Watchog/verification/{args.task}_veri_data.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = torch.load(f\"/data/zhihao/TU/Watchog/verification/SOTAB_veri_data.pth\")\n",
    "veri_label_sotab = res[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "veri_label_sotab_all = []\n",
    "for i in veri_label_sotab:\n",
    "    veri_label_sotab_all += veri_label_sotab[i]\n",
    "veri_label_sotab_all = torch.tensor(veri_label_sotab_all).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(veri_label_sotab_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "veri_label_sotab_all.sum()/len(veri_label_sotab_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = torch.load(f\"/data/zhihao/TU/Watchog/verification/{args.task}_veri_data.pth\")\n",
    "veri_label_old = res[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res1 = torch.load(f\"/data/zhihao/TU/Watchog/verification/{args.task}_veri_data_1.pth\")\n",
    "veri_label_1 = res1[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "veri_label_old_all = []\n",
    "for i in veri_label_old:\n",
    "    veri_label_old_all += veri_label_old[i]\n",
    "veri_label_old_all = torch.tensor(veri_label_old_all).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "veri_label_1_all = []\n",
    "for i in veri_label_1:\n",
    "    veri_label_1_all += veri_label_1[i]\n",
    "veri_label_1_all = torch.tensor(veri_label_1_all).reshape(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(veri_label_old_all))\n",
    "print(veri_label_old_all.sum()/len(veri_label_old_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(veri_label_1_all))\n",
    "print(veri_label_1_all.sum()/len(veri_label_1_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def veri_collate_fn(pad_token_id, binary=False):\n",
    "    '''padder for input batch'''\n",
    "    \n",
    "    def padder(samples):    \n",
    "        batch = {}\n",
    "        if \"embs\" in samples[0]:\n",
    "            embs = torch.stack([sample[\"embs\"] for sample in samples], dim=0)\n",
    "            batch[\"embs\"] = embs\n",
    "        \n",
    "        if \"data\" in samples[0]:\n",
    "            data = []\n",
    "            for sample in samples:\n",
    "                data.extend(sample[\"data\"])\n",
    "            data =torch.nn.utils.rnn.pad_sequence(\n",
    "                data, padding_value=pad_token_id)\n",
    "            batch[\"data\"] = data\n",
    "        if \"label\" in samples[0]:\n",
    "            label = torch.cat([sample[\"label\"] for sample in samples], dim=-1)\n",
    "            batch[\"label\"] = label\n",
    "        if \"cls_indexes\" in samples[0]:\n",
    "            i = 0\n",
    "            cls_indexes = []\n",
    "            for sample in samples:\n",
    "                for value in sample[\"cls_indexes\"]:\n",
    "                    cls_indexes.append(torch.tensor([i, value]))\n",
    "                    i += 1\n",
    "            cls_indexes = torch.stack(cls_indexes, dim=0).long()\n",
    "            batch[\"cls_indexes\"] = cls_indexes\n",
    "        return batch\n",
    "    \n",
    "    def padder_binary(samples):   \n",
    "\n",
    "        label = torch.cat([sample[\"label\"] for sample in samples], dim=-1)\n",
    "        batch = {\"label\": label}\n",
    "        if  \"data\" in samples[0]:\n",
    "            data = torch.nn.utils.rnn.pad_sequence(\n",
    "                [sample[\"data\"] for sample in samples], padding_value=pad_token_id)\n",
    "            batch[\"data\"] = data\n",
    "        if \"cls_indexes\" in samples[0]:\n",
    "            i = 0\n",
    "            cls_indexes = []\n",
    "            for sample in samples:\n",
    "                value =  sample[\"cls_indexes\"]\n",
    "                cls_indexes.append(torch.tensor([i, value]))\n",
    "                i += 1\n",
    "            cls_indexes = torch.stack(cls_indexes, dim=0).long()\n",
    "            batch[\"cls_indexes\"] = cls_indexes\n",
    "        if \"embs\" in samples[0]:\n",
    "            embs = torch.stack([sample[\"embs\"] for sample in samples], dim=0)\n",
    "            batch[\"embs\"] = embs\n",
    "        if \"logits\" in samples[0]:\n",
    "            logits = torch.stack([sample[\"logits\"] for sample in samples], dim=0)\n",
    "            batch[\"logits\"] = logits\n",
    "        return batch\n",
    "    if binary:\n",
    "        return padder_binary\n",
    "    else:\n",
    "        return padder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VerificationCompareDataset(data.Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            data_path: str = \"/data/zhihao/TU/Watchog/verification/gt-semtab22-dbpedia-all0_veri_data.pth\",\n",
    "            pos_ratio = None, # clone the negative samples\n",
    "            context = None,\n",
    "            ): \n",
    "        \n",
    "        self.num_neg = int((1-pos_ratio)/pos_ratio)\n",
    "        self.context = context\n",
    "        \n",
    "        data_raw = torch.load(data_path)\n",
    "        veri_label = data_raw[\"label\"]\n",
    "        veri_data = data_raw[\"data\"]\n",
    "        veri_cls_indexes = data_raw[\"cls_indexes\"]\n",
    "        veri_embs = data_raw[\"embs\"]\n",
    "        veri_target_embs = data_raw[\"target_embs\"]\n",
    "        veri_logits = data_raw[\"logits\"]\n",
    "        \n",
    "        \n",
    "        self.veri_pos_embs = defaultdict(list)\n",
    "        self.veri_neg_embs = defaultdict(list)\n",
    "        self.veri_anchor_embs = defaultdict(list)\n",
    "        \n",
    "        # self.veri_label = {}\n",
    "        # self.veri_logits = {}\n",
    "        # self.veri_cls_indexes = {}\n",
    "        # self.veri_logits = {}\n",
    "        i = 0\n",
    "        for veri_i in veri_label:\n",
    "            if 1 not in veri_label[veri_i] or 0 not in veri_label[veri_i]:\n",
    "                continue\n",
    "            # save anchor emb\n",
    "            if context is None or context == \"None\" or context == \"init\":\n",
    "                self.veri_anchor_embs[i].append(veri_embs[veri_i][0])\n",
    "            elif context == \"target\":\n",
    "                self.veri_anchor_embs[i].append(veri_target_embs[veri_i][0])        \n",
    "            else:\n",
    "                raise ValueError(\"context {} is not supported\".format(context))\n",
    "            # save pos and neg embs\n",
    "            for veri_j in range(len(veri_label[veri_i])):\n",
    "                if veri_label[veri_i][veri_j] == 1:\n",
    "                    self.veri_pos_embs[i].append(veri_embs[veri_i][veri_j])\n",
    "                else:\n",
    "                    self.veri_neg_embs[i].append(veri_embs[veri_i][veri_j])\n",
    "            i += 1\n",
    "\n",
    "        # Maintain the iteration state for each anchor\n",
    "        self.pos_iter = {anchor_idx: iter(pos_embs) for anchor_idx, pos_embs in self.veri_pos_embs.items()}\n",
    "        self.neg_iter = {anchor_idx: iter(neg_embs) for anchor_idx, neg_embs in self.veri_neg_embs.items()}\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.veri_anchor_embs)\n",
    "\n",
    "    def reset_iterators(self, anchor_idx, type=\"pos\"):\n",
    "        \"\"\"Reset the iterator for the given anchor when all positives or negatives are iterated through.\"\"\"\n",
    "        if type == \"pos\":\n",
    "            self.pos_iter[anchor_idx] = iter(self.veri_pos_embs[anchor_idx])\n",
    "            random.shuffle(self.veri_pos_embs[anchor_idx])\n",
    "        else:\n",
    "            self.neg_iter[anchor_idx] = iter(self.veri_neg_embs[anchor_idx])\n",
    "            random.shuffle(self.veri_neg_embs[anchor_idx])\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        anchor_emb = self.veri_anchor_embs[idx]\n",
    "\n",
    "        # Get one positive embedding\n",
    "        try:\n",
    "            pos_emb = next(self.pos_iter[idx])\n",
    "        except StopIteration:\n",
    "            # If we've exhausted all positives, reset and shuffle\n",
    "            self.reset_iterators(idx, type=\"pos\")\n",
    "            pos_emb = next(self.pos_iter[idx])\n",
    "\n",
    "        # Get N negative embeddings\n",
    "        neg_embs = []\n",
    "        for _ in range(self.num_neg):\n",
    "            try:\n",
    "                neg_emb = next(self.neg_iter[idx])\n",
    "                neg_embs.append(neg_emb)\n",
    "            except StopIteration:\n",
    "                # If we've exhausted all negatives, reset and shuffle\n",
    "                self.reset_iterators(idx, type=\"neg\")\n",
    "                neg_emb = next(self.neg_iter[idx])\n",
    "                neg_embs.append(neg_emb)\n",
    "        if self.context is None or self.context == \"None\":\n",
    "            embs = [pos_emb] + neg_embs\n",
    "        else:\n",
    "            embs = [anchor_emb, pos_emb] + neg_embs\n",
    "        embs = torch.stack(embs, dim=0)\n",
    "        \n",
    "        \n",
    "        \n",
    "        return {\n",
    "            # \"data\": self.veri_data[idx].reshape(-1),\n",
    "            \"embs\": embs,\n",
    "            # \"label\": self.veri_label[idx],\n",
    "            # \"logits\": self.veri_logits[idx],\n",
    "            # \"cls_indexes\": self.veri_cls_indexes[idx], \n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_comp = VerificationCompareDataset(pos_ratio=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "veri_comp_padder = veri_collate_fn(0)\n",
    "veri_comp_dataloader = data.DataLoader(\n",
    "    dataset_comp, batch_size=5, shuffle=True, num_workers=4, collate_fn=veri_comp_padder\n",
    ")\n",
    "num = 0\n",
    "for batch_idx, batch in enumerate(veri_comp_dataloader):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"embs\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = nn.MultiheadAttention(embed_dim=128, num_heads=2, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = torch.randn(32, 2, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = torch.randn(32, 1, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = attention(q, embs, embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = ffn(batch[\"embs\"])\n",
    "pos_scores, neg_scores = scores[:, 0], scores[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "margin = 0.1\n",
    "loss = torch.clamp(pos_scores - neg_scores + margin, min=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_comp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "labels_test = []\n",
    "for batch in veri_class:\n",
    "    labels_test.append(batch[0])\n",
    "sns.histplot(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, test_counts = np.unique(labels_test, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, veri_counts = np.unique(labels_all, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(test_counts)/min(test_counts), )\n",
    "veri_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "labels_all = []\n",
    "for batch in veri_class:\n",
    "    for label in veri_class[batch]:\n",
    "        labels_all.append(label)\n",
    "sns.histplot(labels_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.histplot(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(labels_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs_temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " torch.LongTensor([[0, batch[\"cls_indexes\"].cpu().item()]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_batch_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_indexes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"data\"].T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in veri_data:\n",
    "    assert len(veri_data[i]) == len(veri_label[i]) == len(veri_cls_indexes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# brute force perumate in validation TODO: restrict mamimus length of permutation\n",
    "import itertools\n",
    "\n",
    "def is_sublist(A, B):\n",
    "    it = iter(B)\n",
    "    return all(x in it for x in A)\n",
    "def get_permutation(x):\n",
    "    new = []\n",
    "    x = x.tolist()\n",
    "    if len(x) == 1:\n",
    "        x = x[0]\n",
    "    for k in x:\n",
    "        if k not in new:\n",
    "            new.append(k)\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# brute force perumate in validation TODO: restrict mamimus length of permutation\n",
    "import itertools\n",
    "\n",
    "def is_sublist(A, B):\n",
    "    it = iter(B)\n",
    "    return all(x in it for x in A)\n",
    "def get_permutation(x):\n",
    "    new = []\n",
    "    x = x.tolist()\n",
    "    if len(x) == 1:\n",
    "        x = x[0]\n",
    "    for k in x:\n",
    "        if k not in new:\n",
    "            new.append(k)\n",
    "    return new\n",
    "\n",
    "import torch.nn.functional as F\n",
    "test_data = defaultdict(list)\n",
    "test_logits = defaultdict(list)\n",
    "test_cls_indexes = defaultdict(list)\n",
    "test_target_col_mask = defaultdict(list)\n",
    "test_embs = defaultdict(list)\n",
    "test_col_num = defaultdict(list)\n",
    "test_label = defaultdict(list)\n",
    "test_class = defaultdict(list)\n",
    "test_target_embs = defaultdict(list)\n",
    "test_drop_idx = defaultdict(list)\n",
    "    \n",
    "model.load_state_dict(best_state_dict, strict=False)\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "change_log = []\n",
    "score_init = []\n",
    "score_best = [] \n",
    "for threshold in [1.5]:\n",
    "    print(f\"*********************Threshold: {threshold}****************************\")\n",
    "    ft_embs_test = []\n",
    "    labels_test = []\n",
    "    logits_test = []\n",
    "    log = defaultdict(list)\n",
    "    num_cols = []\n",
    "    corrected = 0\n",
    "    total_mistakes = 0\n",
    "    num_permutations = {}\n",
    "    init_permutation = {}\n",
    "    init_correctness = {}\n",
    "    score_init = {}\n",
    "    score_permutation = defaultdict(list)\n",
    "    permutation_correctness = defaultdict(list)\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(test_dataloader_iter):\n",
    "            cls_indexes = torch.LongTensor([[0, batch[\"cls_indexes\"].cpu().item()]]).to(device)\n",
    "            target_col_mask = batch[\"target_col_mask\"].T\n",
    "            logits = model(batch[\"data\"].T, cls_indexes=cls_indexes,)\n",
    "            score_init[batch_idx] = F.softmax(logits.detach()).max().item()\n",
    "            label_i = batch[\"label\"].item()\n",
    "            predict_init = logits.argmax().item()\n",
    "            init_permutation_i = get_permutation(target_col_mask)\n",
    "            init_permutation[batch_idx] = init_permutation_i\n",
    "            if predict_init == label_i:\n",
    "                init_correctness[batch_idx] = True\n",
    "            else:\n",
    "                init_correctness[batch_idx] = False\n",
    "            num_permutations[batch_idx] = 0\n",
    "\n",
    "\n",
    "            x = [0]\n",
    "            new_batch_data = []\n",
    "            for col_i in x:\n",
    "                if col_i == 0:\n",
    "                    if len(new_batch_data) == 0:\n",
    "                        cls_indexes_value = 0\n",
    "                    else:\n",
    "                        cls_indexes_value = sum([len(new_batch_data[i]) for i in range(len(new_batch_data))])\n",
    "                new_batch_data.append(batch[\"data\"].T[target_col_mask==col_i])\n",
    "            new_batch_data = torch.cat(new_batch_data, dim=-1).reshape(1, -1)\n",
    "            cls_indexes = torch.tensor([0, cls_indexes_value]).reshape(1, -1).to(device)\n",
    "            logits_temp, embs_temp = model(new_batch_data, cls_indexes=cls_indexes, get_enc=True)\n",
    "            test_target_embs[batch_idx].append(embs_temp.cpu())\n",
    "\n",
    "\n",
    "            num_cols.append(len(init_permutation_i))\n",
    "            labels_test.append(batch[\"label\"].cpu())\n",
    "            col_idx_set = target_col_mask.unique().tolist()\n",
    "            assert -1 not in col_idx_set\n",
    "            for r in range(len(init_permutation_i), len(init_permutation_i)-2, -1): # not \n",
    "                for x in itertools.combinations(init_permutation_i, r):\n",
    "                    if 0 not in x:\n",
    "                        continue\n",
    "\n",
    "                    num_permutations[batch_idx] += 1\n",
    "                    new_batch_data = []\n",
    "                    if len(x) != len(init_permutation_i):\n",
    "                        drop_idx = (set(init_permutation_i)-set(x)).pop()\n",
    "                    else:\n",
    "                        drop_idx = -1\n",
    "                    for col_i in x:\n",
    "                        if col_i == 0:\n",
    "                            if len(new_batch_data) == 0:\n",
    "                                cls_indexes_value = 0\n",
    "                            else:\n",
    "                                cls_indexes_value = sum([len(new_batch_data[i]) for i in range(len(new_batch_data))])\n",
    "                        new_batch_data.append(batch[\"data\"].T[target_col_mask==col_i])\n",
    "                    new_batch_data = torch.cat(new_batch_data, dim=-1).reshape(1, -1)\n",
    "                    cls_indexes = torch.tensor([0, cls_indexes_value]).reshape(1, -1).to(device)\n",
    "                    logits_temp, embs_temp = model(new_batch_data, cls_indexes=cls_indexes, get_enc=True)\n",
    "                    ood_score_temp = F.softmax(logits_temp.detach()).max().item()\n",
    "                    score_permutation[batch_idx].append(ood_score_temp)\n",
    "                    predict_temp = logits_temp.argmax().item()\n",
    "                    permutation_correctness[batch_idx].append(predict_temp == label_i)\n",
    "                    \n",
    "                    test_data[batch_idx].append(new_batch_data.cpu())\n",
    "                    test_logits[batch_idx].append(logits_temp.detach().cpu())\n",
    "                    test_cls_indexes[batch_idx].append(cls_indexes_value)\n",
    "                    test_embs[batch_idx].append(embs_temp.cpu())\n",
    "                    test_col_num[batch_idx].append(len(x))\n",
    "                    test_label[batch_idx].append(torch.tensor(predict_temp == label_i).long()) # indicate whether the permutation is correct or not\n",
    "                    test_class[batch_idx].append(label_i)\n",
    "                    test_drop_idx[batch_idx].append(drop_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols_iter = []\n",
    "for batch_idx, batch in enumerate(test_dataloader_iter):\n",
    "    cls_indexes = torch.LongTensor([[0, batch[\"cls_indexes\"].cpu().item()]]).to(device)\n",
    "    target_col_mask = batch[\"target_col_mask\"].T\n",
    "    init_permutation_i = get_permutation(target_col_mask)\n",
    "    num_cols_iter.append(len(init_permutation_i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: ylabel='Count'>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.histplot(num_cols_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1085"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(num_cols_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"/data/zhihao/TU/Watchog/verification\", exist_ok=True)\n",
    "torch.save({\"data\": test_data, \"logits\": test_logits, \"cls_indexes\": test_cls_indexes, \n",
    "            \"embs\": test_embs, \"target_embs\":test_target_embs, \"col_num\": test_col_num, \"label\": test_label, \"class\": test_class}, f\"/data/zhihao/TU/Watchog/verification/{args.task}_test_data_drop_0.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3970067/872819949.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  test_veri = torch.load(f\"/data/zhihao/TU/Watchog/verification/{args.task}_test_data.pth\")\n"
     ]
    }
   ],
   "source": [
    "test_veri = torch.load(f\"/data/zhihao/TU/Watchog/verification/{args.task}_test_data.pth\")\n",
    "test_data = test_veri[\"data\"]\n",
    "test_logits = test_veri[\"logits\"]\n",
    "test_cls_indexes = test_veri[\"cls_indexes\"]\n",
    "test_embs = test_veri[\"embs\"]\n",
    "test_col_num = test_veri[\"col_num\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_col_num[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_test = []\n",
    "logits_test = []\n",
    "col_scores = defaultdict(list)\n",
    "verifier = Verifier(norm=\"batch_norm\").to(device)\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch in enumerate(test_dataloader_iter):\n",
    "        embs = test_embs[batch_idx][0].reshape(-1).to(device)\n",
    "        logits = test_logits[batch_idx][0].reshape(-1).to(device)\n",
    "        # scores_init = verifier(embs)\n",
    "        max_score = -float(\"inf\")\n",
    "        target_col_mask = batch[\"target_col_mask\"].T\n",
    "        init_permutation_i = get_permutation(target_col_mask)\n",
    "        num_cols_to_drop = min(2, len(init_permutation_i)//2)\n",
    "        if len(init_permutation_i) ==1:\n",
    "            labels_test.append(batch[\"label\"].cpu())\n",
    "            logits_test.append(logits.detach().cpu())\n",
    "            continue\n",
    "        \n",
    "        for i in range(1, len(test_embs[batch_idx])):\n",
    "            logits_temp = test_logits[batch_idx][i].reshape(-1).to(device)\n",
    "            embs_temp = test_embs[batch_idx][i].reshape(-1).to(device)\n",
    "\n",
    "            scores_temp = np.random.rand()\n",
    "            predict_temp = logits_temp.argmax().item()\n",
    "            col_scores[batch_idx].append(scores_temp)\n",
    "            \n",
    "            \n",
    "        cols_to_drop = torch.tensor(col_scores[batch_idx]).argsort()[:num_cols_to_drop]\n",
    "        x = deepcopy(init_permutation_i)\n",
    "        for col_i in cols_to_drop:\n",
    "            x.remove(col_i)\n",
    "        new_batch_data = []\n",
    "        if len(x) != len(init_permutation_i):\n",
    "            drop_idx = (set(init_permutation_i)-set(x)).pop()\n",
    "        else:\n",
    "            drop_idx = -1\n",
    "        for col_i in x:\n",
    "            if col_i == 0:\n",
    "                if len(new_batch_data) == 0:\n",
    "                    cls_indexes_value = 0\n",
    "                else:\n",
    "                    cls_indexes_value = sum([len(new_batch_data[i]) for i in range(len(new_batch_data))])\n",
    "            new_batch_data.append(batch[\"data\"].T[target_col_mask==col_i])\n",
    "        new_batch_data = torch.cat(new_batch_data, dim=-1).reshape(1, -1)\n",
    "        cls_indexes = torch.tensor([0, cls_indexes_value]).reshape(1, -1).to(device)\n",
    "        logits_topk, embs_topk = model(new_batch_data, cls_indexes=cls_indexes, get_enc=True)\n",
    "        if len(init_permutation_i) > 4: \n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_permutation_i = torch.tensor(init_permutation_i)\n",
    "init_permutation_i[init_permutation_i!=0][:num_cols_to_drop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(col_scores[batch_idx]).argsort(descending=True)[: ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(col_scores[batch_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(init_permutation_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_idx = (set(init_permutation_i)-set(x)).pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"/data/zhihao/TU/Watchog/verification\", exist_ok=True)\n",
    "torch.save({\"data\": test_data, \"logits\": test_logits, \"cls_indexes\": test_cls_indexes, \n",
    "            \"embs\": test_embs, \"target_embs\":test_target_embs, \"col_num\": test_col_num, \"label\": test_label, \"class\": test_class}, f\"/data/zhihao/TU/Watchog/verification/{args.task}_test_data_2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, r in enumerate(test_dataloader_iter):\n",
    "    try:\n",
    "        res = test_embs[i][0]\n",
    "    except:\n",
    "        print(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = torch.load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = []\n",
    "for i in veri_label:\n",
    "    all_labels.extend(veri_label[i])\n",
    "all_labels = torch.tensor(all_labels).reshape(-1)\n",
    "all_labels.sum()/len(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "veri_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VerificationDataset(data.Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            data_path: str = \"/data/zhihao/TU/Watchog/verification/veri_data.pth\",\n",
    "            max_list_length: int = 10,\n",
    "            pos_ratio : int = 0.5, # None: only control pos_ratio to be less than 0.5\n",
    "            label_padding_value: int = -1,\n",
    "            data_padding_value: int = 0,\n",
    "            ): \n",
    "        data_raw = torch.load(data_path)\n",
    "        veri_label = data_raw[\"label\"]\n",
    "        veri_data = data_raw[\"data\"]\n",
    "        veri_cls_indexes = data_raw[\"cls_indexes\"]\n",
    "        self.label_padding_value = label_padding_value\n",
    "        self.data_padding_value = data_padding_value\n",
    "        self.max_list_length = max_list_length\n",
    "        self.pos_ratio = pos_ratio\n",
    "        self.veri_label = {}\n",
    "        self.veri_data = {}\n",
    "        self.veri_cls_indexes = {}\n",
    "        i = 0\n",
    "        for veri_i in veri_label:\n",
    "            labels_i = torch.tensor(veri_label[veri_i]).reshape(-1)\n",
    "            \n",
    "            if 0 not in labels_i or 1 not in labels_i:\n",
    "                continue\n",
    "            self.veri_data[i] = veri_data[veri_i]\n",
    "            self.veri_label[i] = labels_i\n",
    "            self.veri_cls_indexes[i] = veri_cls_indexes[veri_i]\n",
    "            i += 1\n",
    "    def sample(self, labels):\n",
    "        labels = labels.tolist()  # Convert tensor to list for easier manipulation\n",
    "        positive_indices = [i for i, label in enumerate(labels) if label == 1]\n",
    "        negative_indices = [i for i, label in enumerate(labels) if label == 0]\n",
    "\n",
    "        # Determine how many positives we can sample, respecting the ratio requirement\n",
    "        max_num_positives = max_num_negatives = min(len(positive_indices), len(negative_indices), self.max_list_length // 2)\n",
    "        \n",
    "        # Randomly sample the positives and negatives\n",
    "        sampled_positives = random.sample(positive_indices, max_num_positives)\n",
    "        sampled_negatives = random.sample(negative_indices, max_num_negatives)\n",
    "\n",
    "        # Combine and shuffle the indices\n",
    "        sampled_indices = sampled_positives + sampled_negatives\n",
    "\n",
    "        return sampled_indices\n",
    "        # {\"data\": veri_data, \"label\": veri_label, \"cls_indexes\": veri_cls_indexes}\n",
    "    def __len__(self):\n",
    "        return len(self.veri_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        labels = self.veri_label[idx]\n",
    "        sampled_indices = self.sample(labels)\n",
    "        sampled_labels = torch.tensor([labels[i] for i in sampled_indices]).reshape(-1)\n",
    "        sampled_data = [self.veri_data[idx][i].reshape(-1) for i in sampled_indices]\n",
    "        sampled_cls_indexes = torch.tensor([self.veri_cls_indexes[idx][i] for i in sampled_indices], dtype=torch.long)\n",
    "        if len(sampled_indices) < self.max_list_length:\n",
    "            sampled_labels = torch.cat([sampled_labels, torch.ones(self.max_list_length - len(sampled_labels))*self.label_padding_value])\n",
    "            sampled_data.extend([torch.tensor([self.data_padding_value]) for _ in range(self.max_list_length - len(sampled_data))])\n",
    "            sampled_cls_indexes = torch.cat([sampled_cls_indexes, torch.zeros(self.max_list_length - len(sampled_cls_indexes))])\n",
    "        return {\n",
    "            \"data\": sampled_data,\n",
    "            \"label\": sampled_labels,\n",
    "            \"cls_indexes\": sampled_cls_indexes, \n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VerificationBinaryDataset(data.Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            data_path: str = \"/data/zhihao/TU/Watchog/verification/veri_data.pth\",\n",
    "            pos_ratio = None, # clone the negative samples\n",
    "            ): \n",
    "        data_raw = torch.load(data_path)\n",
    "        veri_label = data_raw[\"label\"]\n",
    "        veri_data = data_raw[\"data\"]\n",
    "        veri_cls_indexes = data_raw[\"cls_indexes\"]\n",
    "        veri_embs = data_raw[\"embs\"]\n",
    "        veri_logits = data_raw[\"logits\"]\n",
    "        self.neg_expand = int(1/pos_ratio)-1 if pos_ratio is not None else 1\n",
    "        self.veri_label = {}\n",
    "        self.veri_data = {}\n",
    "        self.veri_embs = {}\n",
    "        self.veri_cls_indexes = {}\n",
    "        self.veri_logits = {}\n",
    "        i = 0\n",
    "        for veri_i in veri_label:\n",
    "            for veri_j in range(len(veri_label[veri_i])):\n",
    "                if veri_label[veri_i][veri_j] == 1:\n",
    "                    labels_i = torch.tensor([1]).reshape(-1)\n",
    "                    self.veri_data[i] = veri_data[veri_i][veri_j]\n",
    "                    self.veri_label[i] = labels_i\n",
    "                    self.veri_cls_indexes[i] = veri_cls_indexes[veri_i][veri_j]\n",
    "                    self.veri_embs[i] = veri_embs[veri_i][veri_j]\n",
    "                    self.veri_logits[i] = veri_logits[veri_i][veri_j]\n",
    "                    i += 1\n",
    "                else:\n",
    "                    for _ in range(self.neg_expand):\n",
    "                        labels_i = torch.tensor([0]).reshape(-1)\n",
    "                        self.veri_data[i] = veri_data[veri_i][veri_j]\n",
    "                        self.veri_label[i] = labels_i\n",
    "                        self.veri_cls_indexes[i] = veri_cls_indexes[veri_i][veri_j]\n",
    "                        self.veri_embs[i] = veri_embs[veri_i][veri_j]\n",
    "                        self.veri_logits[i] = veri_logits[veri_i][veri_j]\n",
    "                        i += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.veri_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            # \"data\": self.veri_data[idx].reshape(-1),\n",
    "            \"embs\": self.veri_embs[idx].reshape(-1),\n",
    "            \"label\": self.veri_label[idx],\n",
    "            \"logits\": self.veri_logits[idx],\n",
    "            # \"cls_indexes\": self.veri_cls_indexes[idx], \n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(veri_dataset), len(veri_dataset.veri_data), len(veri_dataset.veri_label), len(veri_dataset.veri_cls_indexes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verit_binary_dataset = VerificationBinaryDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(verit_binary_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.nn.utils.rnn.pad_sequence(\n",
    "            [verit_binary_dataset[i][\"data\"] for i in range(10)], padding_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verit_binary_dataset[0][\"embs\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "verit_labels = []\n",
    "for i in range(len(verit_binary_dataset)):\n",
    "    verit_labels.append(verit_binary_dataset[i][\"label\"].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(verit_labels).sum()/len(verit_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(veri_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "veri_dataset = VerificationDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(veri_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = veri_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(res[\"data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[\"data\"][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[\"label\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = []\n",
    "for i in range(len(veri_dataset)):\n",
    "    data_test.extend(veri_dataset[i][\"data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def veri_collate_fn(pad_token_id, binary=False):\n",
    "    '''padder for input batch'''\n",
    "    \n",
    "    def padder(samples):    \n",
    "        data = []\n",
    "        for sample in samples:\n",
    "            data.extend(sample[\"data\"])\n",
    "        data =torch.nn.utils.rnn.pad_sequence(\n",
    "            data, padding_value=pad_token_id)\n",
    "\n",
    "        label = torch.cat([sample[\"label\"] for sample in samples], dim=-1)\n",
    "        batch = {\"data\": data, \"label\": label}\n",
    "        if \"cls_indexes\" in samples[0]:\n",
    "            i = 0\n",
    "            cls_indexes = []\n",
    "            for sample in samples:\n",
    "                for value in sample[\"cls_indexes\"]:\n",
    "                    cls_indexes.append(torch.tensor([i, value]))\n",
    "                    i += 1\n",
    "            cls_indexes = torch.stack(cls_indexes, dim=0).long()\n",
    "            batch[\"cls_indexes\"] = cls_indexes\n",
    "        return batch\n",
    "    \n",
    "    def padder_binary(samples):   \n",
    "\n",
    "        label = torch.cat([sample[\"label\"] for sample in samples], dim=-1)\n",
    "        batch = {\"label\": label}\n",
    "        if  \"data\" in samples[0]:\n",
    "            data = torch.nn.utils.rnn.pad_sequence(\n",
    "                [sample[\"data\"] for sample in samples], padding_value=pad_token_id)\n",
    "            batch[\"data\"] = data\n",
    "        if \"cls_indexes\" in samples[0]:\n",
    "            i = 0\n",
    "            cls_indexes = []\n",
    "            for sample in samples:\n",
    "                value =  sample[\"cls_indexes\"]\n",
    "                cls_indexes.append(torch.tensor([i, value]))\n",
    "                i += 1\n",
    "            cls_indexes = torch.stack(cls_indexes, dim=0).long()\n",
    "            batch[\"cls_indexes\"] = cls_indexes\n",
    "        if \"embs\" in samples[0]:\n",
    "            embs = torch.stack([sample[\"embs\"] for sample in samples], dim=0)\n",
    "            batch[\"embs\"] = embs\n",
    "        if \"logits\" in samples[0]:\n",
    "            logits = torch.stack([sample[\"logits\"] for sample in samples], dim=0)\n",
    "            batch[\"logits\"] = logits\n",
    "        return batch\n",
    "    if binary:\n",
    "        return padder_binary\n",
    "    else:\n",
    "        return padder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "veri_padder = veri_collate_fn(0)\n",
    "veri_dataloader = data.DataLoader(\n",
    "    veri_dataset, batch_size=args.batch_size, shuffle=True, num_workers=4, collate_fn=veri_padder\n",
    ")\n",
    "for batch in veri_dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "veri_binary_padder = veri_collate_fn(0, binary=True)\n",
    "veri_binary_dataloader = data.DataLoader(\n",
    "    verit_binary_dataset, batch_size=args.batch_size, shuffle=True, num_workers=4, collate_fn=veri_binary_padder\n",
    ")\n",
    "for batch_idx, batch in enumerate(veri_binary_dataloader):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"embs\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"logits\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"label\"].to(device).squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model = model.to(device)\n",
    "verifier.train()\n",
    "verifier = verifier.to(device)\n",
    "pos_ratio = 0.1\n",
    "pos_weight = torch.tensor([(1-pos_ratio)/pos_ratio]).to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "for batch_idx, batch in enumerate(veri_binary_dataloader ):\n",
    "    cls_indexes = batch[\"cls_indexes\"].to(device)\n",
    "    input_data = batch[\"data\"].T.to(device)\n",
    "    logits, embs = model(input_data, cls_indexes=cls_indexes, get_enc=True)\n",
    "    scores = verifier(embs).squeeze()\n",
    "    loss = loss_fn(scores, batch[\"label\"].to(device).squeeze().float())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " batch[\"label\"].to(device).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"data\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"data\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"label\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"cls_indexes\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "verifier = Verifier(norm=\"batch_norm\", num_layers=None).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3970067/89480059.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  veri_state_dict = torch.load(\"/data/zhihao/TU/Watchog/outputs/gt-semtab22-dbpedia-all0/bert-base-uncased-fromscratch-semi1-mode@ffn-context@None-data@5-lr@5e-05-warmup@0.0-dp@0.0-norm@batch_norm-pos@0.2-poolv0-unlabeled8-randFalse-bs64-ml128-ne200-do0.0_verifier_binary_best_f1_micro.pt\")\n"
     ]
    }
   ],
   "source": [
    "# veri_state_dict = torch.load(\"/data/zhihao/TU/Watchog/outputs/gt-semtab22-dbpedia-all0/bert-base-uncased-fromscratch-semi1-mode@ffn-context@None-data@5-lr@5e-05-warmup@0.0-dp@0.0-norm@batch_norm-pos@0.1-poolv0-unlabeled8-randFalse-bs64-ml128-ne200-do0.0_verifier_binary_best_f1_macro.pt\")\n",
    "# veri_state_dict = torch.load(\"/data/zhihao/TU/Watchog/outputs/gt-semtab22-dbpedia-all0/bert-base-uncased-fromscratch-semi1-mode@ffn-context@None-data@5-lr@5e-05-warmup@0.0-dp@0.0-norm@batch_norm-pos@0.2-poolv0-unlabeled8-randFalse-bs64-ml128-ne200-do0.0_verifier_binary_best_f1_micro.pt\")\n",
    "# veri_state_dict = torch.load(\"/data/zhihao/TU/Watchog/outputs/gt-semtab22-dbpedia-all0/bert-base-uncased-fromscratch-semi1-mode@ffn-context@None-data@5-lr@5e-05-warmup@0.0-dp@0.0-norm@batch_norm-pos@0.1-poolv0-unlabeled8-randFalse-bs64-ml128-ne200-do0.0_verifier_binary_best_f1_micro.pt\")\n",
    "# veri_state_dict = torch.load(\"/data/zhihao/TU/Watchog/outputs/gt-semtab22-dbpedia-all0/bert-base-uncased-fromscratch-semi1-mode@ffn-search@0-context@None-data@5-lr@5e-05-warmup@0.0-dp@0.0-norm@batch_norm-pos@0.2-poolv0-unlabeled8-randFalse-bs64-ml128-ne200-do0.0_verifier_binary_best_f1_micro.pt\")\n",
    "veri_state_dict = torch.load(\"/data/zhihao/TU/Watchog/outputs/gt-semtab22-dbpedia-all0/bert-base-uncased-fromscratch-semi1-mode@ffn-context@None-data@5-lr@5e-05-warmup@0.0-dp@0.0-norm@batch_norm-pos@0.2-poolv0-unlabeled8-randFalse-bs64-ml128-ne200-do0.0_verifier_binary_best_f1_micro.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verifier.load_state_dict(veri_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verifier.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(verifier.parameters(), lr=args.lr, eps=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.to(device)\n",
    "verifier.train()\n",
    "tr_loss = 0.0\n",
    "for batch_idx, batch in enumerate(veri_dataloader):\n",
    "    cls_indexes = batch[\"cls_indexes\"].to(device)\n",
    "    input_data = batch[\"data\"].T.to(device)\n",
    "    logits, embs = model(input_data, cls_indexes=cls_indexes, get_enc=True)\n",
    "    scores = verifier(embs)\n",
    "    loss = listMLE(scores.reshape(args.batch_size, -1), batch[\"label\"].to(device).reshape(args.batch_size, -1))\n",
    "\n",
    "    accelerator.backward(loss)\n",
    "    # loss.backward()\n",
    "    tr_loss += loss.cpu().detach().item()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def listMLE(y_pred, y_true, eps=1e-10, padded_value_indicator=-1):\n",
    "    \"\"\"\n",
    "    ListMLE loss introduced in \"Listwise Approach to Learning to Rank - Theory and Algorithm\".\n",
    "    :param y_pred: predictions from the model, shape [batch_size, slate_length]\n",
    "    :param y_true: ground truth labels, shape [batch_size, slate_length]\n",
    "    :param eps: epsilon value, used for numerical stability\n",
    "    :param padded_value_indicator: an indicator of the y_true index containing a padded item, e.g. -1\n",
    "    :return: loss value, a torch.Tensor\n",
    "    \"\"\"\n",
    "    # shuffle for randomised tie resolution\n",
    "    random_indices = torch.randperm(y_pred.shape[-1])\n",
    "    y_pred_shuffled = y_pred[:, random_indices]\n",
    "    y_true_shuffled = y_true[:, random_indices]\n",
    "\n",
    "    y_true_sorted, indices = y_true_shuffled.sort(descending=True, dim=-1)\n",
    "\n",
    "    mask = y_true_sorted == padded_value_indicator\n",
    "\n",
    "    preds_sorted_by_true = torch.gather(y_pred_shuffled, dim=1, index=indices)\n",
    "    preds_sorted_by_true[mask] = float(\"-inf\")\n",
    "\n",
    "    max_pred_values, _ = preds_sorted_by_true.max(dim=1, keepdim=True)\n",
    "\n",
    "    preds_sorted_by_true_minus_max = preds_sorted_by_true - max_pred_values\n",
    "\n",
    "    cumsums = torch.cumsum(preds_sorted_by_true_minus_max.exp().flip(dims=[1]), dim=1).flip(dims=[1])\n",
    "\n",
    "    observation_loss = torch.log(cumsums + eps) - preds_sorted_by_true_minus_max\n",
    "\n",
    "    observation_loss[mask] = 0.0\n",
    "\n",
    "    return torch.mean(torch.sum(observation_loss, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import itertools\n",
    "\n",
    "def is_sublist(A, B):\n",
    "    it = iter(B)\n",
    "    return all(x in it for x in A)\n",
    "def get_permutation(x):\n",
    "    new = []\n",
    "    x = x.tolist()\n",
    "    if len(x) == 1:\n",
    "        x = x[0]\n",
    "    for k in x:\n",
    "        if k not in new:\n",
    "            new.append(k)\n",
    "    return new\n",
    "\n",
    "\n",
    "model.load_state_dict(best_state_dict, strict=False)\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "verifier.eval()\n",
    "verifier = verifier.to(device)\n",
    "labels_test = []\n",
    "logits_test = []\n",
    "init_scores = []\n",
    "init_correctness = []\n",
    "final_scores = []\n",
    "final_correctness = []\n",
    "\n",
    "for batch_idx, batch in enumerate(test_dataloader_iter):\n",
    "    cls_indexes = torch.LongTensor([[0, batch[\"cls_indexes\"].cpu().item()]]).to(device)\n",
    "    target_col_mask = batch[\"target_col_mask\"].T\n",
    "    logits, embs = model(batch[\"data\"].T, cls_indexes=cls_indexes, get_enc=True)\n",
    "    scores_init = verifier(embs)\n",
    "    max_score = -float(\"inf\")\n",
    "    if 1 in target_col_mask:\n",
    "        init_scores.append(scores_init.item())\n",
    "        init_correctness.append(logits.argmax().item() == batch[\"label\"].item())\n",
    "        init_permutation_i = get_permutation(target_col_mask)\n",
    "        col_idx_set = target_col_mask.unique().tolist()\n",
    "        early_stop = False\n",
    "        assert -1 not in col_idx_set\n",
    "        for r in range(len(init_permutation_i), max(len(init_permutation_i)//2, len(init_permutation_i)-3), -1): # not \n",
    "            for x in itertools.combinations(init_permutation_i, r):\n",
    "                if 0 not in x:\n",
    "                    continue\n",
    "                new_batch_data = []\n",
    "                for col_i in x:\n",
    "                    if col_i == 0:\n",
    "                        if len(new_batch_data) == 0:\n",
    "                            cls_indexes_value = 0\n",
    "                        else:\n",
    "                            cls_indexes_value = sum([len(new_batch_data[i]) for i in range(len(new_batch_data))])\n",
    "                    new_batch_data.append(batch[\"data\"].T[target_col_mask==col_i])\n",
    "                if 0 not in x:\n",
    "                    cls_indexes_value = 0\n",
    "                new_batch_data = torch.cat(new_batch_data, dim=-1).reshape(1, -1)\n",
    "                cls_indexes = torch.tensor([0, cls_indexes_value]).reshape(1, -1).to(device)\n",
    "                logits_temp, embs = model(new_batch_data, cls_indexes=cls_indexes, get_enc=True)\n",
    "                scores_temp = verifier(embs).item()\n",
    "                predict_temp = logits_temp.argmax().item()\n",
    "                    # if len(x) == 1 and 0 in x:\n",
    "                    #     predict_target = predict_temp\n",
    "                    #     msp_target = msp_temp\n",
    "                    # # print(x, msp_temp, predict_temp)\n",
    "                    # if 0 not in x and msp_temp > debias_threshold and (predict_temp != predict_target):\n",
    "                    #     debias_classes.append(predict_temp)\n",
    "                    #     continue\n",
    "                if scores_temp > max_score:\n",
    "                    max_score = scores_temp\n",
    "                    logits = logits_temp.clone()\n",
    "        final_scores.append(max_score)\n",
    "        final_correctness.append(logits.argmax().item() == batch[\"label\"].item())\n",
    "    labels_test.append(batch[\"label\"].cpu())\n",
    "    logits_test.append(logits.detach().cpu())\n",
    "labels_test = torch.cat(labels_test, dim=0)\n",
    "logits_test = torch.stack(logits_test, dim=0)\n",
    "preds_test = torch.argmax(logits_test, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "ts_pred_list = logits_test.argmax(\n",
    "                            1).cpu().detach().numpy().tolist()\n",
    "ts_micro_f1 = f1_score(labels_test.reshape(-1).numpy().tolist(),\n",
    "                    ts_pred_list,\n",
    "                    average=\"micro\")\n",
    "ts_macro_f1 = f1_score(labels_test.reshape(-1).numpy().tolist(),\n",
    "                    ts_pred_list,\n",
    "                    average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_correctness = torch.tensor(init_correctness)\n",
    "final_correctness = torch.tensor(final_correctness)\n",
    "init_scores = torch.tensor(init_scores)\n",
    "final_scores = torch.tensor(final_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(init_correctness | final_correctness).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.histplot(init_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(F.sigmoid(init_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.sigmoid(init_scores)[init_correctness].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.sigmoid(init_scores)[~init_correctness].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.sigmoid(final_scores)[init_correctness].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.sigmoid(final_scores)[~init_correctness].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(final_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(F.sigmoid(final_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_macro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_init_mask = init_correctness\n",
    "correct_msp_mask = final_correctness\n",
    "for threshold in [ 0.9, 0.99, 0.999]:\n",
    "    uncertain_init_mask = F.sigmoid(init_scores) < threshold\n",
    "    condition_mask = ~uncertain_init_mask\n",
    "    print(f\"*********************Threshold: {threshold}****************************\")\n",
    "    print(\"Init\")\n",
    "    # init prediction\n",
    "    print((condition_mask&correct_init_mask).sum().item(), \n",
    "        (condition_mask).sum().item(), \n",
    "        (condition_mask&correct_init_mask).sum().item()/(condition_mask).sum().item())\n",
    "    # init prediction\n",
    "    print((~condition_mask&correct_init_mask).sum().item(), \n",
    "        (~condition_mask).sum().item(), \n",
    "       (~condition_mask&correct_init_mask).sum().item()/(~condition_mask).sum().item())\n",
    "    print(\"MSP\")\n",
    "    # msp prediction\n",
    "    print((condition_mask&correct_msp_mask).sum().item(), \n",
    "        (condition_mask).sum().item(), \n",
    "        (condition_mask&correct_msp_mask).sum().item()/(condition_mask).sum().item())\n",
    "    # msp prediction\n",
    "    print((~condition_mask&correct_msp_mask).sum().item(), \n",
    "        (~condition_mask).sum().item(), \n",
    "        (~condition_mask&correct_msp_mask).sum().item()/(~condition_mask).sum().item())\n",
    "    print(\"MSP & Init\")\n",
    "    print(((condition_mask&correct_init_mask)&(condition_mask&correct_msp_mask)).sum().item())\n",
    "    # print(((~(init_target_mask&(~uncertain_init_mask & ~uncertain_target_mask))& correct_msp_mask)&(~(init_target_mask&(~uncertain_init_mask & ~uncertain_target_mask))& correct_init_mask)).sum().item())\n",
    "    print(((condition_mask&correct_init_mask)|(condition_mask&correct_msp_mask)).sum().item(),\n",
    "          (condition_mask).sum().item(), \n",
    "          ((condition_mask&correct_init_mask)|(condition_mask&correct_msp_mask)).sum().item()/(condition_mask).sum().item())\n",
    "    print(((~condition_mask&correct_init_mask)|(~condition_mask&correct_msp_mask)).sum().item(),\n",
    "          (~condition_mask).sum().item(), \n",
    "          ((~condition_mask&correct_init_mask)|(~condition_mask&correct_msp_mask)).sum().item()/(~condition_mask).sum().item())\n",
    "    # print(\"Permutation\")\n",
    "    # print((condition_mask&correct_permutation_mask).sum().item(), \n",
    "    #     (condition_mask).sum().item(), \n",
    "    #     (condition_mask&correct_permutation_mask).sum().item()/(condition_mask).sum().item())\n",
    "    # print((~condition_mask&correct_permutation_mask).sum().item(), \n",
    "    #      (~condition_mask).sum().item(), \n",
    "    #     (~condition_mask&correct_permutation_mask).sum().item()/(~condition_mask).sum().item())\n",
    "    # print(\"Permutation Confident\")\n",
    "    # print((condition_mask&correct_permutation_confident_mask).sum().item(), \n",
    "    #     (condition_mask).sum().item(), \n",
    "    #     (condition_mask&correct_permutation_confident_mask).sum().item()/(condition_mask).sum().item())\n",
    "    # print((~condition_mask&correct_permutation_confident_mask).sum().item(),\n",
    "    #         (~condition_mask).sum().item(), \n",
    "    #         (~condition_mask&correct_permutation_confident_mask).sum().item()/(~condition_mask).sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ts_micro_f1=0.5631, ts_macro_f1=0.3018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3970067/3186715187.py:102: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  final_scores = F.sigmoid(torch.tensor(final_scores))\n",
      "/tmp/ipykernel_3970067/3186715187.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  init_scores = F.sigmoid(torch.tensor(init_scores))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import itertools\n",
    "\n",
    "def is_sublist(A, B):\n",
    "    it = iter(B)\n",
    "    return all(x in it for x in A)\n",
    "def get_permutation(x):\n",
    "    new = []\n",
    "    x = x.tolist()\n",
    "    if len(x) == 1:\n",
    "        x = x[0]\n",
    "    for k in x:\n",
    "        if k not in new:\n",
    "            new.append(k)\n",
    "    return new\n",
    "\n",
    "\n",
    "model.load_state_dict(best_state_dict, strict=False)\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "verifier.load_state_dict(veri_state_dict)\n",
    "verifier.eval()\n",
    "verifier = verifier.to(device)\n",
    "labels_test = []\n",
    "logits_test = []\n",
    "init_scores = []\n",
    "init_correctness = []\n",
    "final_scores = []\n",
    "final_correctness = []\n",
    "num_cols = []\n",
    "total_steps = []\n",
    "final_steps = []\n",
    "correct_scores = defaultdict(list)\n",
    "correct_steps = defaultdict(list)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch in enumerate(test_dataloader_iter):\n",
    "        target_col_mask = batch[\"target_col_mask\"].T\n",
    "        embs = test_embs[batch_idx][0].reshape(1,-1).to(device)\n",
    "        logits = test_logits[batch_idx][0].reshape(-1).to(device)\n",
    "        scores_init = verifier(embs)\n",
    "        max_score = -float(\"inf\")\n",
    "\n",
    "        init_scores.append(scores_init.item())\n",
    "        init_correctness.append(logits.argmax().item() == batch[\"label\"].item())\n",
    "        init_permutation_i = get_permutation(target_col_mask)\n",
    "        col_idx_set = target_col_mask.unique().tolist()\n",
    "        early_stop = False\n",
    "        i = 0\n",
    "        final_step = 0\n",
    "        total_steps.append(len(init_permutation_i) - max(len(init_permutation_i)//2, len(init_permutation_i)-3) -1)\n",
    "        num_cols.append(len(init_permutation_i))\n",
    "        for r in range(len(init_permutation_i), max(len(init_permutation_i)//2, len(init_permutation_i)-3), -1): # not \n",
    "            for x in itertools.combinations(init_permutation_i, r):\n",
    "                if 0 not in x:\n",
    "                    continue\n",
    "                logits_temp = test_logits[batch_idx][i].reshape(-1).to(device)\n",
    "                embs_temp = test_embs[batch_idx][i].reshape(1,-1).to(device)\n",
    "                scores_temp = verifier(embs_temp).item()\n",
    "                predict_temp = logits_temp.argmax().item()\n",
    "                i += 1\n",
    "                if scores_temp > max_score:\n",
    "                    max_score = scores_temp\n",
    "                    logits = logits_temp.clone()\n",
    "                    final_step = len(init_permutation_i) - r\n",
    "                if logits_temp.argmax().item() == batch[\"label\"].item():\n",
    "                    correct_scores[batch_idx].append(scores_temp)\n",
    "                    correct_steps[batch_idx].append(len(init_permutation_i) - r)\n",
    "                    \n",
    "        final_scores.append(max_score)\n",
    "        final_correctness.append(logits.argmax().item() == batch[\"label\"].item())\n",
    "        final_steps.append(final_step)\n",
    "        labels_test.append(batch[\"label\"].cpu())\n",
    "        logits_test.append(logits.detach().cpu())\n",
    "    labels_test = torch.cat(labels_test, dim=0)\n",
    "    logits_test = torch.stack(logits_test, dim=0)\n",
    "    preds_test = torch.argmax(logits_test, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "ts_pred_list = logits_test.argmax(\n",
    "                            1).cpu().detach().numpy().tolist()\n",
    "ts_micro_f1 = f1_score(labels_test.reshape(-1).numpy().tolist(),\n",
    "                    ts_pred_list,\n",
    "                    average=\"micro\")\n",
    "ts_macro_f1 = f1_score(labels_test.reshape(-1).numpy().tolist(),\n",
    "                    ts_pred_list,\n",
    "                    average=\"macro\")\n",
    "ts_f1_full = f1_score(labels_test.reshape(-1).numpy().tolist(),\n",
    "                    ts_pred_list,\n",
    "                    average=None)\n",
    "# full_f1_init\n",
    "print(\"ts_micro_f1={:.4f}, ts_macro_f1={:.4f}\".format(ts_micro_f1, ts_macro_f1))\n",
    "\n",
    "final_correctness = torch.tensor(final_correctness)\n",
    "init_correctness = torch.tensor(init_correctness)\n",
    "init_scores = torch.tensor(init_scores)\n",
    "final_scores = torch.tensor(final_scores)\n",
    "final_steps = torch.tensor(final_steps)\n",
    "total_steps = torch.tensor(total_steps)\n",
    "final_scores = F.sigmoid(torch.tensor(final_scores))\n",
    "init_scores = F.sigmoid(torch.tensor(init_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import itertools\n",
    "\n",
    "def is_sublist(A, B):\n",
    "    it = iter(B)\n",
    "    return all(x in it for x in A)\n",
    "def get_permutation(x):\n",
    "    new = []\n",
    "    x = x.tolist()\n",
    "    if len(x) == 1:\n",
    "        x = x[0]\n",
    "    for k in x:\n",
    "        if k not in new:\n",
    "            new.append(k)\n",
    "    return new\n",
    "\n",
    "\n",
    "model.load_state_dict(best_state_dict, strict=False)\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "verifier.load_state_dict(veri_state_dict)\n",
    "verifier.eval()\n",
    "verifier = verifier.to(device)\n",
    "labels_test = []\n",
    "logits_test = []\n",
    "init_scores = []\n",
    "init_correctness = []\n",
    "final_scores = []\n",
    "final_correctness = []\n",
    "num_cols = []\n",
    "total_steps = []\n",
    "final_steps = []\n",
    "correct_scores = defaultdict(list)\n",
    "correct_steps = defaultdict(list)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch in enumerate(test_dataloader_iter):\n",
    "        target_col_mask = batch[\"target_col_mask\"].T\n",
    "        embs = test_embs[batch_idx][0].reshape(1,-1).to(device)\n",
    "        logits = test_logits[batch_idx][0].reshape(-1).to(device)\n",
    "        scores_init = verifier(embs)\n",
    "        max_score = -float(\"inf\")\n",
    "\n",
    "        init_scores.append(scores_init.item())\n",
    "        init_correctness.append(logits.argmax().item() == batch[\"label\"].item())\n",
    "        init_permutation_i = get_permutation(target_col_mask)\n",
    "        col_idx_set = target_col_mask.unique().tolist()\n",
    "        early_stop = False\n",
    "        i = 0\n",
    "        final_step = 0\n",
    "        total_steps.append(len(init_permutation_i) - max(len(init_permutation_i)//2, len(init_permutation_i)-3) -1)\n",
    "        num_cols.append(len(init_permutation_i))\n",
    "        if len(init_permutation_i) <= 1:\n",
    "            continue\n",
    "        for r in range(len(init_permutation_i), min(3, len(init_permutation_i)//2-1), -1): # not \n",
    "            for x in itertools.combinations(init_permutation_i, r):\n",
    "                if 0 not in x:\n",
    "                    continue\n",
    "                logits_temp = test_logits[batch_idx][i].reshape(-1).to(device)\n",
    "                embs_temp = test_embs[batch_idx][i].reshape(1,-1).to(device)\n",
    "                scores_temp = verifier(embs_temp).item()\n",
    "                predict_temp = logits_temp.argmax().item()\n",
    "                i += 1\n",
    "                if scores_temp > max_score:\n",
    "                    max_score = scores_temp\n",
    "                    logits = logits_temp.clone()\n",
    "                    final_step = len(init_permutation_i) - r\n",
    "                if logits_temp.argmax().item() == batch[\"label\"].item():\n",
    "                    correct_scores[batch_idx].append(scores_temp)\n",
    "                    correct_steps[batch_idx].append(len(init_permutation_i) - r)\n",
    "                    \n",
    "        final_scores.append(max_score)\n",
    "        final_correctness.append(logits.argmax().item() == batch[\"label\"].item())\n",
    "        final_steps.append(final_step)\n",
    "        labels_test.append(batch[\"label\"].cpu())\n",
    "        logits_test.append(logits.detach().cpu())\n",
    "    labels_test = torch.cat(labels_test, dim=0)\n",
    "    logits_test = torch.stack(logits_test, dim=0)\n",
    "    preds_test = torch.argmax(logits_test, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "ts_pred_list = logits_test.argmax(\n",
    "                            1).cpu().detach().numpy().tolist()\n",
    "ts_micro_f1 = f1_score(labels_test.reshape(-1).numpy().tolist(),\n",
    "                    ts_pred_list,\n",
    "                    average=\"micro\")\n",
    "ts_macro_f1 = f1_score(labels_test.reshape(-1).numpy().tolist(),\n",
    "                    ts_pred_list,\n",
    "                    average=\"macro\")\n",
    "ts_f1_full = f1_score(labels_test.reshape(-1).numpy().tolist(),\n",
    "                    ts_pred_list,\n",
    "                    average=None)\n",
    "# full_f1_init\n",
    "print(\"ts_micro_f1={:.4f}, ts_macro_f1={:.4f}\".format(ts_micro_f1, ts_macro_f1))\n",
    "\n",
    "final_correctness = torch.tensor(final_correctness)\n",
    "init_correctness = torch.tensor(init_correctness)\n",
    "init_scores = torch.tensor(init_scores)\n",
    "final_scores = torch.tensor(final_scores)\n",
    "final_steps = torch.tensor(final_steps)\n",
    "total_steps = torch.tensor(total_steps)\n",
    "final_scores = F.sigmoid(torch.tensor(final_scores))\n",
    "init_scores = F.sigmoid(torch.tensor(init_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_check_idx = []\n",
    "for i in (torch.tensor(final_correctness) == False).nonzero().reshape(-1).tolist():\n",
    "    if len(correct_scores[i])>0:\n",
    "        to_check_idx.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_check_idx[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_check_idx_init = []\n",
    "for i in (torch.tensor(final_correctness) == False).nonzero().reshape(-1).tolist():\n",
    "    if init_correctness[i] == True:\n",
    "        to_check_idx_init.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_check_idx_init[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[34, 52] in to_check_idx_init[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set([34, 52]).issubset(set(to_check_idx_init[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(to_check_idx_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_idx = 53\n",
    "print(final_scores[target_idx], final_correctness[target_idx], init_correctness[target_idx], init_scores[target_idx], num_cols[target_idx], \n",
    "      final_steps[target_idx], total_steps[target_idx], labels_test[target_idx], preds_test[target_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.sigmoid(torch.tensor(correct_scores[target_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(final_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(np.array(final_steps)/np.array(total_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(correct_scores), len(test_dataloader_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(torch.tensor(final_steps)[final_correctness==True]/torch.tensor(total_steps)[final_correctness==True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(torch.tensor(num_cols)[final_scores>0.999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = torch.tensor(num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num_col in num_cols.unique():\n",
    "    print(num_col, ((final_scores>0.999)&(num_cols==num_col)).sum()/(num_cols==num_col).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(torch.tensor(num_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(torch.tensor(num_cols)<8).sum()/len(num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(torch.tensor(final_steps)[final_scores>0.999]/torch.tensor(total_steps)[final_scores>0.999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(final_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(F.sigmoid(final_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_f1_full - full_f1_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_init_mask = torch.tensor(init_correctness)\n",
    "correct_msp_mask = torch.tensor(final_correctness)\n",
    "init_scores = torch.tensor(init_scores)\n",
    "final_scores = torch.tensor(final_scores)\n",
    "for threshold in [0.9, 0.99, 0.999, 0.9999, 0.99999]:\n",
    "    uncertain_init_mask = init_scores < threshold\n",
    "    condition_mask = ~uncertain_init_mask\n",
    "    print(f\"*********************Threshold: {threshold}****************************\")\n",
    "    print(\"Init\")\n",
    "    # init prediction\n",
    "    print((condition_mask&correct_init_mask).sum().item(), \n",
    "        (condition_mask).sum().item(), \n",
    "        (condition_mask&correct_init_mask).sum().item()/(condition_mask).sum().item())\n",
    "    # init prediction\n",
    "    print((~condition_mask&correct_init_mask).sum().item(), \n",
    "        (~condition_mask).sum().item(), \n",
    "       (~condition_mask&correct_init_mask).sum().item()/(~condition_mask).sum().item())\n",
    "    print(\"MSP\")\n",
    "    # msp prediction\n",
    "    print((condition_mask&correct_msp_mask).sum().item(), \n",
    "        (condition_mask).sum().item(), \n",
    "        (condition_mask&correct_msp_mask).sum().item()/(condition_mask).sum().item())\n",
    "    # msp prediction\n",
    "    print((~condition_mask&correct_msp_mask).sum().item(), \n",
    "        (~condition_mask).sum().item(), \n",
    "        (~condition_mask&correct_msp_mask).sum().item()/(~condition_mask).sum().item())\n",
    "    print(\"MSP & Init\")\n",
    "    print(((condition_mask&correct_init_mask)&(condition_mask&correct_msp_mask)).sum().item())\n",
    "    # print(((~(init_target_mask&(~uncertain_init_mask & ~uncertain_target_mask))& correct_msp_mask)&(~(init_target_mask&(~uncertain_init_mask & ~uncertain_target_mask))& correct_init_mask)).sum().item())\n",
    "    print(((condition_mask&correct_init_mask)|(condition_mask&correct_msp_mask)).sum().item(),\n",
    "          (condition_mask).sum().item(), \n",
    "          ((condition_mask&correct_init_mask)|(condition_mask&correct_msp_mask)).sum().item()/(condition_mask).sum().item())\n",
    "    print(((~condition_mask&correct_init_mask)|(~condition_mask&correct_msp_mask)).sum().item(),\n",
    "          (~condition_mask).sum().item(), \n",
    "          ((~condition_mask&correct_init_mask)|(~condition_mask&correct_msp_mask)).sum().item()/(~condition_mask).sum().item())\n",
    "    # print(\"Permutation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((final_scores>0.999).sum(), (final_scores>0.999).sum()/len(final_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import itertools\n",
    "\n",
    "def is_sublist(A, B):\n",
    "    it = iter(B)\n",
    "    return all(x in it for x in A)\n",
    "def get_permutation(x):\n",
    "    new = []\n",
    "    x = x.tolist()\n",
    "    if len(x) == 1:\n",
    "        x = x[0]\n",
    "    for k in x:\n",
    "        if k not in new:\n",
    "            new.append(k)\n",
    "    return new\n",
    "\n",
    "for threshold in [0.9, 0.99, 0.999]:\n",
    "    print(f\"*********************Threshold: {threshold}****************************\")\n",
    "    model.load_state_dict(best_state_dict, strict=False)\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    verifier.load_state_dict(veri_state_dict)\n",
    "    verifier.eval()\n",
    "    verifier = verifier.to(device)\n",
    "    labels_test = []\n",
    "    logits_test = []\n",
    "    init_scores = []\n",
    "    init_correctness = []\n",
    "    final_scores = []\n",
    "    final_correctness = []\n",
    "    num_cols = []\n",
    "    total_steps = []\n",
    "    final_steps = []\n",
    "    correct_scores = defaultdict(list)\n",
    "    correct_steps = defaultdict(list)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(test_dataloader_iter):\n",
    "            target_col_mask = batch[\"target_col_mask\"].T\n",
    "            embs = test_embs[batch_idx][0].reshape(1,-1).to(device)\n",
    "            logits = test_logits[batch_idx][0].reshape(-1).to(device)\n",
    "            scores_init = verifier(embs)\n",
    "            max_score = -float(\"inf\")\n",
    "\n",
    "            init_scores.append(scores_init.item())\n",
    "            init_correctness.append(logits.argmax().item() == batch[\"label\"].item())\n",
    "            init_permutation_i = get_permutation(target_col_mask)\n",
    "            col_idx_set = target_col_mask.unique().tolist()\n",
    "            early_stop = False\n",
    "            i = 0\n",
    "            final_step = 0\n",
    "            total_steps.append(len(init_permutation_i) - max(len(init_permutation_i)//2, len(init_permutation_i)-3) -1)\n",
    "            num_cols.append(len(init_permutation_i))\n",
    "            enough = False\n",
    "            for r in range(len(init_permutation_i),  max(len(init_permutation_i)//2, len(init_permutation_i)-3), -1): # not \n",
    "                for x in itertools.combinations(init_permutation_i, r):\n",
    "                    if 0 not in x:\n",
    "                        continue\n",
    "                    logits_temp = test_logits[batch_idx][i].reshape(-1).to(device)\n",
    "                    embs_temp = test_embs[batch_idx][i].reshape(1,-1).to(device)\n",
    "                    scores_temp = verifier(embs_temp).item()\n",
    "                    predict_temp = logits_temp.argmax().item()\n",
    "                    i += 1\n",
    "                    if scores_temp > max_score:\n",
    "                        max_score = scores_temp\n",
    "                        logits = logits_temp.clone()\n",
    "                        final_step = len(init_permutation_i) - r\n",
    "                    if logits_temp.argmax().item() == batch[\"label\"].item():\n",
    "                        correct_scores[batch_idx].append(scores_temp)\n",
    "                        correct_steps[batch_idx].append(len(init_permutation_i) - r)\n",
    "                    if F.sigmoid(torch.tensor(max_score)).item() > threshold:\n",
    "                        enough = True\n",
    "                if enough:\n",
    "                    break  \n",
    "                \n",
    "                \n",
    "            final_scores.append(max_score)\n",
    "            final_correctness.append(logits.argmax().item() == batch[\"label\"].item())\n",
    "            final_steps.append(final_step)\n",
    "            labels_test.append(batch[\"label\"].cpu())\n",
    "            logits_test.append(logits.detach().cpu())\n",
    "        labels_test = torch.cat(labels_test, dim=0)\n",
    "        logits_test = torch.stack(logits_test, dim=0)\n",
    "        preds_test = torch.argmax(logits_test, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    from sklearn.metrics import confusion_matrix, f1_score\n",
    "    ts_pred_list = logits_test.argmax(\n",
    "                                1).cpu().detach().numpy().tolist()\n",
    "    ts_micro_f1 = f1_score(labels_test.reshape(-1).numpy().tolist(),\n",
    "                        ts_pred_list,\n",
    "                        average=\"micro\")\n",
    "    ts_macro_f1 = f1_score(labels_test.reshape(-1).numpy().tolist(),\n",
    "                        ts_pred_list,\n",
    "                        average=\"macro\")\n",
    "    ts_f1_full = f1_score(labels_test.reshape(-1).numpy().tolist(),\n",
    "                        ts_pred_list,\n",
    "                        average=None)\n",
    "    # full_f1_init\n",
    "    print(\"ts_micro_f1={:.4f}, ts_macro_f1={:.4f}\".format(ts_micro_f1, ts_macro_f1))\n",
    "    final_correctness = torch.tensor(final_correctness)\n",
    "    init_correctness = torch.tensor(init_correctness)\n",
    "    init_scores = torch.tensor(init_scores)\n",
    "    final_scores = torch.tensor(final_scores)\n",
    "    final_steps = torch.tensor(final_steps)\n",
    "    total_steps = torch.tensor(total_steps)\n",
    "    final_scores = F.sigmoid(torch.tensor(final_scores))\n",
    "    init_scores = F.sigmoid(torch.tensor(init_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.999999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((final_scores>0.999).sum(), len(final_scores), (final_scores>0.999).sum()/len(final_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = None\n",
    "test_dataset_iter_extra = GittablesTablewiseIterateDataset(cv=cv,\n",
    "                            split=\"test\", src=src,\n",
    "                            tokenizer=tokenizer,\n",
    "                            max_length=max_length,\n",
    "                            gt_only='all' not in task,\n",
    "                            device=device,\n",
    "                            base_dirpath=os.path.join(args.data_path, \"GitTables/semtab_gittables/2022\"),\n",
    "                            small_tag=\"semi1\",\n",
    "                            max_unlabeled=16,\n",
    "                            adaptive_max_length=False,)\n",
    "padder = collate_fn(tokenizer.pad_token_id)\n",
    "# test_dataloader_iter_extra = DataLoader(test_dataset_iter_extra,\n",
    "#                                 batch_size=1,\n",
    "#                             #   collate_fn=collate_fn)\n",
    "#                             collate_fn=padder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_iter_extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols_extra = []\n",
    "for i in range(len(test_dataset_iter_extra)):\n",
    "    init_permutation_i = get_permutation(test_dataset_iter_extra[i][\"target_col_mask\"].T)\n",
    "    num_cols_extra.append(len(init_permutation_i))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_permutation_i, init_permutation_i_extra = init_permutation_i[:8], init_permutation_i[8:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(num_cols_extra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import itertools\n",
    "\n",
    "def is_sublist(A, B):\n",
    "    it = iter(B)\n",
    "    return all(x in it for x in A)\n",
    "def get_permutation(x):\n",
    "    new = []\n",
    "    x = x.tolist()\n",
    "    if len(x) == 1:\n",
    "        x = x[0]\n",
    "    for k in x:\n",
    "        if k not in new:\n",
    "            new.append(k)\n",
    "    return new\n",
    "\n",
    "for max_unlabeled in [16]:\n",
    "    print(f\"============================max_unlabeled={max_unlabeled}===============================\")\n",
    "    test_dataset_iter_extra = GittablesTablewiseIterateDataset(cv=cv,\n",
    "                                split=\"test\", src=src,\n",
    "                                tokenizer=tokenizer,\n",
    "                                max_length=max_length,\n",
    "                                gt_only='all' not in task,\n",
    "                                device=device,\n",
    "                                base_dirpath=os.path.join(args.data_path, \"GitTables/semtab_gittables/2022\"),\n",
    "                                small_tag=\"semi1\",\n",
    "                                max_unlabeled=max_unlabeled,\n",
    "                                adaptive_max_length=False,)\n",
    "    model.load_state_dict(best_state_dict, strict=False)\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    verifier.load_state_dict(veri_state_dict)\n",
    "    verifier.eval()\n",
    "    verifier = verifier.to(device)\n",
    "    labels_test = []\n",
    "    logits_test = []\n",
    "    init_scores = []\n",
    "    init_correctness = []\n",
    "    final_scores = []\n",
    "    final_correctness = []\n",
    "    num_cols = []\n",
    "    total_steps = []\n",
    "    final_steps = []\n",
    "    correct_scores = defaultdict(list)\n",
    "    correct_steps = defaultdict(list)\n",
    "    init_max_col_length = 8\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(test_dataset_iter_extra):\n",
    "            target_col_mask = batch[\"target_col_mask\"].T\n",
    "            embs = test_embs[batch_idx][0].reshape(1,-1).to(device)\n",
    "            logits = test_logits[batch_idx][0].reshape(-1).to(device)\n",
    "            scores_init = verifier(embs)\n",
    "            max_score = -float(\"inf\")\n",
    "\n",
    "            init_scores.append(scores_init.item())\n",
    "            init_correctness.append(logits.argmax().item() == batch[\"label\"].item())\n",
    "            init_permutation_i = get_permutation(target_col_mask)\n",
    "            init_permutation_i, init_permutation_i_extra = init_permutation_i[:init_max_col_length], init_permutation_i[init_max_col_length:]\n",
    "            \n",
    "            if 0 not in init_permutation_i:\n",
    "                col_to_exclude = init_permutation_i[-1]\n",
    "                init_permutation_i = init_permutation_i[:-1] + [0]\n",
    "                init_permutation_i_extra.remove(0)\n",
    "                init_permutation_i_extra = [col_to_exclude] + init_permutation_i_extra\n",
    "                assert 0 in init_permutation_i and 0 not in init_permutation_i_extra\n",
    "            \n",
    "            col_idx_set = target_col_mask.unique().tolist()\n",
    "            early_stop = False\n",
    "            i = 0\n",
    "            final_step = 0\n",
    "            total_steps.append(len(init_permutation_i) - max(len(init_permutation_i)//2, len(init_permutation_i)-3) -1)\n",
    "            num_cols.append(len(init_permutation_i))\n",
    "            enough = False\n",
    "            for r in range(len(init_permutation_i),  max(len(init_permutation_i)//2, len(init_permutation_i)-3), -1): # not \n",
    "                for x in itertools.combinations(init_permutation_i, r):\n",
    "                    if 0 not in x:\n",
    "                        continue\n",
    "                    logits_temp = test_logits[batch_idx][i].reshape(-1).to(device)\n",
    "                    embs_temp = test_embs[batch_idx][i].reshape(1,-1).to(device)\n",
    "                    scores_temp = verifier(embs_temp).item()\n",
    "                    predict_temp = logits_temp.argmax().item()\n",
    "                    i += 1\n",
    "                    if scores_temp > max_score:\n",
    "                        max_score = scores_temp\n",
    "                        logits = logits_temp.clone()\n",
    "                        final_step = len(init_permutation_i) - r\n",
    "                    if logits_temp.argmax().item() == batch[\"label\"].item():\n",
    "                        correct_scores[batch_idx].append(scores_temp)\n",
    "                        correct_steps[batch_idx].append(len(init_permutation_i) - r)\n",
    "                    if F.sigmoid(torch.tensor(max_score)).item() > threshold:\n",
    "                        enough = True\n",
    "                if enough:\n",
    "                    break  \n",
    "            if not enough and len(init_permutation_i_extra)>0:\n",
    "                for new_col_idx in init_permutation_i_extra:\n",
    "                    init_permutation_i_new = init_permutation_i + [new_col_idx]\n",
    "                    for r in range(len(init_permutation_i),  max(len(init_permutation_i)//2, len(init_permutation_i)-3), -1): # not \n",
    "                        for x in itertools.combinations(init_permutation_i_new, r):\n",
    "                            if 0 not in x or new_col_idx not in x:\n",
    "                                continue\n",
    "                            new_batch_data = []\n",
    "                            for col_i in x:\n",
    "                                if col_i == 0:\n",
    "                                    if len(new_batch_data) == 0:\n",
    "                                        cls_indexes_value = 0\n",
    "                                    else:\n",
    "                                        cls_indexes_value = sum([len(new_batch_data[i]) for i in range(len(new_batch_data))])\n",
    "                                new_batch_data.append(batch[\"data\"].T[target_col_mask==col_i])\n",
    "                            new_batch_data = torch.cat(new_batch_data, dim=-1).reshape(1, -1)\n",
    "                            cls_indexes = torch.tensor([0, cls_indexes_value]).reshape(1, -1).to(device)\n",
    "                            logits_temp, embs_temp = model(new_batch_data, cls_indexes=cls_indexes, get_enc=True)\n",
    "                            scores_temp = verifier(embs_temp.reshape(1,-1)).item()\n",
    "                            predict_temp = logits_temp.argmax().item()\n",
    "                            \n",
    "                            if scores_temp > max_score:\n",
    "                                max_score = scores_temp\n",
    "                                logits = logits_temp.clone()\n",
    "                                final_step = len(init_permutation_i) - r\n",
    "                            if logits_temp.argmax().item() == batch[\"label\"].item():\n",
    "                                correct_scores[batch_idx].append(scores_temp)\n",
    "                                correct_steps[batch_idx].append(len(init_permutation_i) - r)\n",
    "                    #         if F.sigmoid(torch.tensor(max_score)).item() > threshold:\n",
    "                    #             enough = True\n",
    "                    #     if enough:\n",
    "                    #         break\n",
    "                    # if enough:\n",
    "                    #     break\n",
    "                    \n",
    "                    \n",
    "            final_scores.append(max_score)\n",
    "            final_correctness.append(logits.argmax().item() == batch[\"label\"].item())\n",
    "            final_steps.append(final_step)\n",
    "            labels_test.append(batch[\"label\"].cpu())\n",
    "            logits_test.append(logits.detach().cpu())\n",
    "        labels_test = torch.cat(labels_test, dim=0)\n",
    "        logits_test = torch.stack(logits_test, dim=0)\n",
    "        preds_test = torch.argmax(logits_test, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    from sklearn.metrics import confusion_matrix, f1_score\n",
    "    ts_pred_list = logits_test.argmax(\n",
    "                                1).cpu().detach().numpy().tolist()\n",
    "    ts_micro_f1 = f1_score(labels_test.reshape(-1).numpy().tolist(),\n",
    "                        ts_pred_list,\n",
    "                        average=\"micro\")\n",
    "    ts_macro_f1 = f1_score(labels_test.reshape(-1).numpy().tolist(),\n",
    "                        ts_pred_list,\n",
    "                        average=\"macro\")\n",
    "    ts_f1_full = f1_score(labels_test.reshape(-1).numpy().tolist(),\n",
    "                        ts_pred_list,\n",
    "                        average=None)\n",
    "    # full_f1_init\n",
    "    print(\"ts_micro_f1={:.4f}, ts_macro_f1={:.4f}\".format(ts_micro_f1, ts_macro_f1))\n",
    "    final_correctness = torch.tensor(final_correctness)\n",
    "    init_correctness = torch.tensor(init_correctness)\n",
    "    init_scores = torch.tensor(init_scores)\n",
    "    final_scores = torch.tensor(final_scores)\n",
    "    final_steps = torch.tensor(final_steps)\n",
    "    total_steps = torch.tensor(total_steps)\n",
    "    final_scores = F.sigmoid(torch.tensor(final_scores))\n",
    "    init_scores = F.sigmoid(torch.tensor(init_scores))\n",
    "    print((final_scores>threshold).sum(), len(final_scores), (final_scores>threshold).sum()/len(final_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_micro_f1=0.5567, ts_macro_f1=0.2983"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import itertools\n",
    "\n",
    "def is_sublist(A, B):\n",
    "    it = iter(B)\n",
    "    return all(x in it for x in A)\n",
    "def get_permutation(x):\n",
    "    new = []\n",
    "    x = x.tolist()\n",
    "    if len(x) == 1:\n",
    "        x = x[0]\n",
    "    for k in x:\n",
    "        if k not in new:\n",
    "            new.append(k)\n",
    "    return new\n",
    "\n",
    "for max_unlabeled in [32, 64, 128]:\n",
    "    print(f\"============================max_unlabeled={max_unlabeled}===============================\")\n",
    "    test_dataset_iter_extra = GittablesTablewiseIterateDataset(cv=cv,\n",
    "                                split=\"test\", src=src,\n",
    "                                tokenizer=tokenizer,\n",
    "                                max_length=max_length,\n",
    "                                gt_only='all' not in task,\n",
    "                                device=device,\n",
    "                                base_dirpath=os.path.join(args.data_path, \"GitTables/semtab_gittables/2022\"),\n",
    "                                small_tag=\"semi1\",\n",
    "                                max_unlabeled=max_unlabeled,\n",
    "                                adaptive_max_length=False,)\n",
    "    model.load_state_dict(best_state_dict, strict=False)\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    verifier.load_state_dict(veri_state_dict)\n",
    "    verifier.eval()\n",
    "    verifier = verifier.to(device)\n",
    "    labels_test = []\n",
    "    logits_test = []\n",
    "    init_scores = []\n",
    "    init_correctness = []\n",
    "    final_scores = []\n",
    "    final_correctness = []\n",
    "    num_cols = []\n",
    "    total_steps = []\n",
    "    final_steps = []\n",
    "    correct_scores = defaultdict(list)\n",
    "    correct_steps = defaultdict(list)\n",
    "    init_max_col_length = 8\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(test_dataset_iter_extra):\n",
    "            target_col_mask = batch[\"target_col_mask\"].T\n",
    "            embs = test_embs[batch_idx][0].reshape(1,-1).to(device)\n",
    "            logits = test_logits[batch_idx][0].reshape(-1).to(device)\n",
    "            scores_init = verifier(embs)\n",
    "            max_score = -float(\"inf\")\n",
    "\n",
    "            init_scores.append(scores_init.item())\n",
    "            init_correctness.append(logits.argmax().item() == batch[\"label\"].item())\n",
    "            init_permutation_i = get_permutation(target_col_mask)\n",
    "            init_permutation_i, init_permutation_i_extra = init_permutation_i[:init_max_col_length], init_permutation_i[init_max_col_length:]\n",
    "            \n",
    "            if 0 not in init_permutation_i:\n",
    "                col_to_exclude = init_permutation_i[-1]\n",
    "                init_permutation_i = init_permutation_i[:-1] + [0]\n",
    "                init_permutation_i_extra.remove(0)\n",
    "                init_permutation_i_extra = [col_to_exclude] + init_permutation_i_extra\n",
    "                assert 0 in init_permutation_i and 0 not in init_permutation_i_extra\n",
    "            \n",
    "            col_idx_set = target_col_mask.unique().tolist()\n",
    "            early_stop = False\n",
    "            i = 0\n",
    "            final_step = 0\n",
    "            total_steps.append(len(init_permutation_i) - max(len(init_permutation_i)//2, len(init_permutation_i)-3) -1)\n",
    "            num_cols.append(len(init_permutation_i))\n",
    "            enough = False\n",
    "            for r in range(len(init_permutation_i),  max(len(init_permutation_i)//2, len(init_permutation_i)-3), -1): # not \n",
    "                for x in itertools.combinations(init_permutation_i, r):\n",
    "                    if 0 not in x:\n",
    "                        continue\n",
    "                    logits_temp = test_logits[batch_idx][i].reshape(-1).to(device)\n",
    "                    embs_temp = test_embs[batch_idx][i].reshape(1,-1).to(device)\n",
    "                    scores_temp = verifier(embs_temp).item()\n",
    "                    predict_temp = logits_temp.argmax().item()\n",
    "                    i += 1\n",
    "                    if scores_temp > max_score:\n",
    "                        max_score = scores_temp\n",
    "                        logits = logits_temp.clone()\n",
    "                        final_step = len(init_permutation_i) - r\n",
    "                    if logits_temp.argmax().item() == batch[\"label\"].item():\n",
    "                        correct_scores[batch_idx].append(scores_temp)\n",
    "                        correct_steps[batch_idx].append(len(init_permutation_i) - r)\n",
    "                    if F.sigmoid(torch.tensor(max_score)).item() > 0.999:\n",
    "                        enough = True\n",
    "                if enough:\n",
    "                    break  \n",
    "            if not enough and len(init_permutation_i_extra)>0:\n",
    "                for new_col_idx in init_permutation_i_extra:\n",
    "                    init_permutation_i_new = init_permutation_i + [new_col_idx]\n",
    "                    for r in range(len(init_permutation_i),  max(len(init_permutation_i)//2, len(init_permutation_i)-3), -1): # not \n",
    "                        for x in itertools.combinations(init_permutation_i_new, r):\n",
    "                            if 0 not in x or new_col_idx not in x:\n",
    "                                continue\n",
    "                            new_batch_data = []\n",
    "                            for col_i in x:\n",
    "                                if col_i == 0:\n",
    "                                    if len(new_batch_data) == 0:\n",
    "                                        cls_indexes_value = 0\n",
    "                                    else:\n",
    "                                        cls_indexes_value = sum([len(new_batch_data[i]) for i in range(len(new_batch_data))])\n",
    "                                new_batch_data.append(batch[\"data\"].T[target_col_mask==col_i])\n",
    "                            new_batch_data = torch.cat(new_batch_data, dim=-1).reshape(1, -1)\n",
    "                            cls_indexes = torch.tensor([0, cls_indexes_value]).reshape(1, -1).to(device)\n",
    "                            logits_temp, embs_temp = model(new_batch_data, cls_indexes=cls_indexes, get_enc=True)\n",
    "                            scores_temp = verifier(embs_temp.reshape(1,-1)).item()\n",
    "                            predict_temp = logits_temp.argmax().item()\n",
    "                            \n",
    "                            if scores_temp > max_score:\n",
    "                                max_score = scores_temp\n",
    "                                logits = logits_temp.clone()\n",
    "                                final_step = len(init_permutation_i) - r\n",
    "                            if logits_temp.argmax().item() == batch[\"label\"].item():\n",
    "                                correct_scores[batch_idx].append(scores_temp)\n",
    "                                correct_steps[batch_idx].append(len(init_permutation_i) - r)\n",
    "                            if F.sigmoid(torch.tensor(max_score)).item() > 0.999:\n",
    "                                enough = True\n",
    "                        if enough:\n",
    "                            break\n",
    "                    if enough:\n",
    "                        break\n",
    "                    \n",
    "                    \n",
    "            final_scores.append(max_score)\n",
    "            final_correctness.append(logits.argmax().item() == batch[\"label\"].item())\n",
    "            final_steps.append(final_step)\n",
    "            labels_test.append(batch[\"label\"].cpu())\n",
    "            logits_test.append(logits.detach().cpu())\n",
    "        labels_test = torch.cat(labels_test, dim=0)\n",
    "        logits_test = torch.stack(logits_test, dim=0)\n",
    "        preds_test = torch.argmax(logits_test, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    from sklearn.metrics import confusion_matrix, f1_score\n",
    "    ts_pred_list = logits_test.argmax(\n",
    "                                1).cpu().detach().numpy().tolist()\n",
    "    ts_micro_f1 = f1_score(labels_test.reshape(-1).numpy().tolist(),\n",
    "                        ts_pred_list,\n",
    "                        average=\"micro\")\n",
    "    ts_macro_f1 = f1_score(labels_test.reshape(-1).numpy().tolist(),\n",
    "                        ts_pred_list,\n",
    "                        average=\"macro\")\n",
    "    ts_f1_full = f1_score(labels_test.reshape(-1).numpy().tolist(),\n",
    "                        ts_pred_list,\n",
    "                        average=None)\n",
    "    # full_f1_init\n",
    "    print(\"ts_micro_f1={:.4f}, ts_macro_f1={:.4f}\".format(ts_micro_f1, ts_macro_f1))\n",
    "    final_correctness = torch.tensor(final_correctness)\n",
    "    init_correctness = torch.tensor(init_correctness)\n",
    "    init_scores = torch.tensor(init_scores)\n",
    "    final_scores = torch.tensor(final_scores)\n",
    "    final_steps = torch.tensor(final_steps)\n",
    "    total_steps = torch.tensor(total_steps)\n",
    "    final_scores = F.sigmoid(torch.tensor(final_scores))\n",
    "    init_scores = F.sigmoid(torch.tensor(init_scores))\n",
    "    print((final_scores>0.999).sum(), len(final_scores), (final_scores>0.999).sum()/len(final_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import itertools\n",
    "\n",
    "def is_sublist(A, B):\n",
    "    it = iter(B)\n",
    "    return all(x in it for x in A)\n",
    "def get_permutation(x):\n",
    "    new = []\n",
    "    x = x.tolist()\n",
    "    if len(x) == 1:\n",
    "        x = x[0]\n",
    "    for k in x:\n",
    "        if k not in new:\n",
    "            new.append(k)\n",
    "    return new\n",
    "\n",
    "for max_unlabeled in [16]:\n",
    "    print(f\"============================max_unlabeled={max_unlabeled}===============================\")\n",
    "    test_dataset_iter_extra = GittablesTablewiseIterateDataset(cv=cv,\n",
    "                                split=\"test\", src=src,\n",
    "                                tokenizer=tokenizer,\n",
    "                                max_length=max_length,\n",
    "                                gt_only='all' not in task,\n",
    "                                device=device,\n",
    "                                base_dirpath=os.path.join(args.data_path, \"GitTables/semtab_gittables/2022\"),\n",
    "                                small_tag=\"semi1\",\n",
    "                                max_unlabeled=max_unlabeled,\n",
    "                                adaptive_max_length=False,)\n",
    "    model.load_state_dict(best_state_dict, strict=False)\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    verifier.load_state_dict(veri_state_dict)\n",
    "    verifier.eval()\n",
    "    verifier = verifier.to(device)\n",
    "    labels_test = []\n",
    "    logits_test = []\n",
    "    init_scores = []\n",
    "    init_correctness = []\n",
    "    final_scores = []\n",
    "    final_correctness = []\n",
    "    num_cols = []\n",
    "    total_steps = []\n",
    "    final_steps = []\n",
    "    correct_scores = defaultdict(list)\n",
    "    correct_steps = defaultdict(list)\n",
    "    init_max_col_length = 8\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(test_dataset_iter_extra):\n",
    "            target_col_mask = batch[\"target_col_mask\"].T\n",
    "            embs = test_embs[batch_idx][0].reshape(1,-1).to(device)\n",
    "            logits = test_logits[batch_idx][0].reshape(-1).to(device)\n",
    "            scores_init = verifier(embs)\n",
    "            max_score = -float(\"inf\")\n",
    "\n",
    "            init_scores.append(scores_init.item())\n",
    "            init_correctness.append(logits.argmax().item() == batch[\"label\"].item())\n",
    "            init_permutation_i = get_permutation(target_col_mask)\n",
    "            init_permutation_i, init_permutation_i_extra = init_permutation_i[:init_max_col_length], init_permutation_i[init_max_col_length:]\n",
    "            \n",
    "            col_idx_set = target_col_mask.unique().tolist()\n",
    "            early_stop = False\n",
    "            i = 0\n",
    "            final_step = 0\n",
    "            total_steps.append(len(init_permutation_i) - max(len(init_permutation_i)//2, len(init_permutation_i)-3) -1)\n",
    "            num_cols.append(len(init_permutation_i))\n",
    "            enough = False\n",
    "            for r in range(len(init_permutation_i),  max(len(init_permutation_i)//2, len(init_permutation_i)-3), -1): # not \n",
    "                for x in itertools.combinations(init_permutation_i, r):\n",
    "                    if 0 not in x:\n",
    "                        continue\n",
    "                    logits_temp = test_logits[batch_idx][i].reshape(-1).to(device)\n",
    "                    embs_temp = test_embs[batch_idx][i].reshape(1,-1).to(device)\n",
    "                    scores_temp = verifier(embs_temp).item()\n",
    "                    predict_temp = logits_temp.argmax().item()\n",
    "                    i += 1\n",
    "                    if scores_temp > max_score:\n",
    "                        max_score = scores_temp\n",
    "                        logits = logits_temp.clone()\n",
    "                        final_step = len(init_permutation_i) - r\n",
    "                    if logits_temp.argmax().item() == batch[\"label\"].item():\n",
    "                        correct_scores[batch_idx].append(scores_temp)\n",
    "                        correct_steps[batch_idx].append(len(init_permutation_i) - r)\n",
    "                    if F.sigmoid(torch.tensor(max_score)).item() > 0.999:\n",
    "                        enough = True\n",
    "                if enough:\n",
    "                    break  \n",
    "                \n",
    "            if not enough and len(init_permutation_i_extra)>0:\n",
    "                for z in range(1, len(init_permutation_i_extra)+1):\n",
    "                    for new_col_idx in itertools.combinations(init_permutation_i_extra, z):\n",
    "                        init_permutation_i_new = init_permutation_i + list(new_col_idx)\n",
    "                        for r in range(len(init_permutation_i),  max(len(init_permutation_i)//2, len(init_permutation_i)-3), -1):\n",
    "                            for x in itertools.combinations(init_permutation_i_new, r):\n",
    "                                if not set(list(new_col_idx)+[0]).issubset(init_permutation_i_new):\n",
    "                                    continue\n",
    "                                new_batch_data = []\n",
    "                                for col_i in x:\n",
    "                                    if col_i == 0:\n",
    "                                        if len(new_batch_data) == 0:\n",
    "                                            cls_indexes_value = 0\n",
    "                                        else:\n",
    "                                            cls_indexes_value = sum([len(new_batch_data[i]) for i in range(len(new_batch_data))])\n",
    "                                    new_batch_data.append(batch[\"data\"].T[target_col_mask==col_i])\n",
    "                                new_batch_data = torch.cat(new_batch_data, dim=-1).reshape(1, -1)\n",
    "                                cls_indexes = torch.tensor([0, cls_indexes_value]).reshape(1, -1).to(device)\n",
    "                                logits_temp, embs_temp = model(new_batch_data, cls_indexes=cls_indexes, get_enc=True)\n",
    "                                scores_temp = verifier(embs_temp.reshape(1,-1)).item()\n",
    "                                predict_temp = logits_temp.argmax().item()\n",
    "                                \n",
    "                                if scores_temp > max_score:\n",
    "                                    max_score = scores_temp\n",
    "                                    logits = logits_temp.clone()\n",
    "                                    final_step = len(init_permutation_i) - r\n",
    "                                if logits_temp.argmax().item() == batch[\"label\"].item():\n",
    "                                    correct_scores[batch_idx].append(scores_temp)\n",
    "                                    correct_steps[batch_idx].append(len(init_permutation_i) - r)\n",
    "                                if F.sigmoid(torch.tensor(max_score)).item() > 0.999:\n",
    "                                    enough = True\n",
    "                            if enough:\n",
    "                                break\n",
    "                        if enough:\n",
    "                            break\n",
    "                        \n",
    "                    \n",
    "            final_scores.append(max_score)\n",
    "            final_correctness.append(logits.argmax().item() == batch[\"label\"].item())\n",
    "            final_steps.append(final_step)\n",
    "            labels_test.append(batch[\"label\"].cpu())\n",
    "            logits_test.append(logits.detach().cpu())\n",
    "        labels_test = torch.cat(labels_test, dim=0)\n",
    "        logits_test = torch.stack(logits_test, dim=0)\n",
    "        preds_test = torch.argmax(logits_test, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    from sklearn.metrics import confusion_matrix, f1_score\n",
    "    ts_pred_list = logits_test.argmax(\n",
    "                                1).cpu().detach().numpy().tolist()\n",
    "    ts_micro_f1 = f1_score(labels_test.reshape(-1).numpy().tolist(),\n",
    "                        ts_pred_list,\n",
    "                        average=\"micro\")\n",
    "    ts_macro_f1 = f1_score(labels_test.reshape(-1).numpy().tolist(),\n",
    "                        ts_pred_list,\n",
    "                        average=\"macro\")\n",
    "    ts_f1_full = f1_score(labels_test.reshape(-1).numpy().tolist(),\n",
    "                        ts_pred_list,\n",
    "                        average=None)\n",
    "    # full_f1_init\n",
    "    print(\"ts_micro_f1={:.4f}, ts_macro_f1={:.4f}\".format(ts_micro_f1, ts_macro_f1))\n",
    "    final_correctness = torch.tensor(final_correctness)\n",
    "    init_correctness = torch.tensor(init_correctness)\n",
    "    init_scores = torch.tensor(init_scores)\n",
    "    final_scores = torch.tensor(final_scores)\n",
    "    final_steps = torch.tensor(final_steps)\n",
    "    total_steps = torch.tensor(total_steps)\n",
    "    final_scores = F.sigmoid(torch.tensor(final_scores))\n",
    "    init_scores = F.sigmoid(torch.tensor(init_scores))\n",
    "    print((final_scores>0.999).sum(), len(final_scores), (final_scores>0.999).sum()/len(final_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "        for batch_idx, batch in enumerate(test_dataset_iter_extra):\n",
    "            target_col_mask = batch[\"target_col_mask\"].T\n",
    "            embs = test_embs[batch_idx][0].reshape(1,-1).to(device)\n",
    "            logits = test_logits[batch_idx][0].reshape(-1).to(device)\n",
    "            scores_init = verifier(embs)\n",
    "            max_score = -float(\"inf\")\n",
    "\n",
    "            init_permutation_i = get_permutation(target_col_mask)\n",
    "            init_permutation_i, init_permutation_i_extra = init_permutation_i[:init_max_col_length], init_permutation_i[init_max_col_length:]\n",
    "            if len(init_permutation_i_extra) == 2:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_permutation_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0 not in init_permutation_i:\n",
    "    col_to_exclude = init_permutation_i[-1]\n",
    "    init_permutation_i = init_permutation_i[:-1] + [0]\n",
    "    init_permutation_i_extra.remove(0)\n",
    "    init_permutation_i_extra = [col_to_exclude] + init_permutation_i_extra\n",
    "    assert 0 in init_permutation_i and 0 not in init_permutation_i_extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_permutation_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_permutation_i_extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                for z in range(1, len(init_permutation_i_extra)+1):\n",
    "                    for new_col_idx in itertools.combinations(init_permutation_i_extra, z):\n",
    "                        init_permutation_i_new = init_permutation_i + list(new_col_idx)\n",
    "                        for r in range(len(init_permutation_i),  max(len(init_permutation_i)//2, len(init_permutation_i)-3), -1):\n",
    "                            for x in itertools.combinations(init_permutation_i_new, r):\n",
    "                                if not set(list(new_col_idx)+[0]).issubset(init_permutation_i_new):\n",
    "                                    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_permutation_i_extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = []\n",
    "for batch_idx, batch in enumerate(test_dataset_iter_extra):\n",
    "        target_col_mask = batch[\"target_col_mask\"].T\n",
    "        init_permutation_i = get_permutation(target_col_mask)\n",
    "        num_cols.append(len(init_permutation_i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# table search STARMIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_list_1 = [\n",
    "    \"Name: Philip Duffy; Jeremy Oppenheim; Mark Sedwill;\",\n",
    "    \"Mode of Travel: Air; Taxi; Air;\",\n",
    "    \"Purpose: Regional Meeting; Exchange Visit; Evening Meal;\",\n",
    "    \"Destination: London; Ottawa; Bristol;\",\n",
    "    \"Day: 10; 30; 02;\",\n",
    "    \"Month: April; July; September;\",\n",
    "    \"Year: 2019; 2019; 2019;\",\n",
    "    \"Expense: 189.06; 8.08; 50.00;\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cur_maxlen = min(max_length, 512 // len(column_list_1) - 1)\n",
    "\n",
    "# print(tokenizer.cls_token)\n",
    "# print(group_df[\"data\"])\n",
    "token_ids_list = [tokenizer.encode(\n",
    "                tokenizer.cls_token + \" \" + str(x), add_special_tokens=False, max_length=cur_maxlen, truncation=True) for x in column_list_1]\n",
    "token_ids = torch.LongTensor(reduce(operator.add,\n",
    "                                    token_ids_list)).to(device)\n",
    "cls_index_list = [0] + np.cumsum(\n",
    "    np.array([len(x) for x in token_ids_list])).tolist()[:-1]\n",
    "cls_indexes = torch.LongTensor(cls_index_list).to(device).reshape(-1, 1)\n",
    "cls_indexes = torch.hstack([torch.zeros_like(cls_indexes), cls_indexes])\n",
    "logits, embs_1 = model(token_ids.reshape(1, -1), cls_indexes=cls_indexes, get_enc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "cur_maxlen = min(max_length, 512 // len(column_list_1) - 1)\n",
    "\n",
    "# print(tokenizer.cls_token)\n",
    "# print(group_df[\"data\"])\n",
    "token_ids_list = [tokenizer.encode(\n",
    "                tokenizer.cls_token + \" \" + str(x), add_special_tokens=False, max_length=cur_maxlen, truncation=True) for x in column_list_1]\n",
    "token_ids = nn.utils.rnn.pad_sequence(\n",
    "      [torch.tensor(x) for x in token_ids_list], padding_value=tokenizer.pad_token_id).to(device)\n",
    "cls_indexes = torch.nonzero(\n",
    "                token_ids == tokenizer.cls_token_id)\n",
    "logits, embs_1_seperate = model(token_ids, cls_indexes=cls_indexes, get_enc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_list_2 = [\n",
    "    \"Name: Clark; Gyimah; Harrington;\",\n",
    "    \"Date: 23/07; 03/09; 05/08;\",\n",
    "    \"Destination: France; Belgium; China;\",\n",
    "    \"Purpose: Discuss EU; Build Relations; Discuss Productivity;\",\n",
    "]\n",
    "cur_maxlen = min(max_length, 512 // len(column_list_2) - 1)\n",
    "\n",
    "# print(tokenizer.cls_token)\n",
    "# print(group_df[\"data\"])\n",
    "token_ids_list = [tokenizer.encode(\n",
    "                tokenizer.cls_token + \" \" + str(x), add_special_tokens=False, max_length=cur_maxlen, truncation=True) \n",
    "                  for x in column_list_2]\n",
    "token_ids = torch.LongTensor(reduce(operator.add,\n",
    "                                    token_ids_list)).to(device)\n",
    "cls_index_list = [0] + np.cumsum(\n",
    "    np.array([len(x) for x in token_ids_list])).tolist()[:-1]\n",
    "cls_indexes = torch.LongTensor(cls_index_list).to(device).reshape(-1, 1)\n",
    "cls_indexes = torch.hstack([torch.zeros_like(cls_indexes), cls_indexes])\n",
    "logits, embs_2 = model(token_ids.reshape(1, -1), cls_indexes=cls_indexes, get_enc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = token_ids.reshape(1, -1)\n",
    "attention_mask = token_ids != 0\n",
    "res = model.bert(token_ids, attention_mask=attention_mask, return_dict=True, output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(res['attentions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res['attentions'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res['attentions'][0].squeeze().mean(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res['attentions'][0].squeeze().mean(0)[cls_mask][:, cls_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_mask = (token_ids==tokenizer.cls_token_id).reshape(-1)\n",
    "acc_attention = []\n",
    "for i in range(len(res['attentions'])):\n",
    "    attn_i = res['attentions'][i].squeeze().mean(0)[cls_mask][:, cls_mask].mean(0)\n",
    "    acc_attention.append(attn_i)\n",
    "    print(attn_i)\n",
    "acc_attention = torch.stack(acc_attention, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_mask = (token_ids==tokenizer.cls_token_id).reshape(-1)\n",
    "acc_attention = torch.ones_like(res['attentions'][0].squeeze().mean(0)[cls_mask][:, cls_mask].mean(0))\n",
    "for i in range(len(res['attentions'])-1, -1, -1):\n",
    "    attn_i = res['attentions'][i].squeeze().mean(0)[cls_mask][:, cls_mask].mean(0)\n",
    "    acc_attention = acc_attention * attn_i\n",
    "    print(acc_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res['attentions'][0].squeeze().mean(0)[cls_mask][:, cls_mask].fill_diagonal_(0).mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "cur_maxlen = min(max_length, 512 // len(column_list_2) - 1)\n",
    "\n",
    "# print(tokenizer.cls_token)\n",
    "# print(group_df[\"data\"])\n",
    "token_ids_list = [tokenizer.encode(\n",
    "                tokenizer.cls_token + \" \" + str(x), add_special_tokens=False, max_length=cur_maxlen, truncation=True) \n",
    "                  for x in column_list_2]\n",
    "token_ids = nn.utils.rnn.pad_sequence(\n",
    "      [torch.tensor(x) for x in token_ids_list], padding_value=tokenizer.pad_token_id).to(device)\n",
    "cls_indexes = torch.nonzero(\n",
    "                token_ids == tokenizer.cls_token_id)\n",
    "logits, embs_2_seperate = model(token_ids, cls_indexes=cls_indexes, get_enc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_list_3 = [\n",
    "    'Bird Name: Pine Siskin; American Robin; Northern Flicker;',\n",
    "    'Scientific Name: Carduelis Pinus; Turdus migratorius; Colaptes auratus;',\n",
    "    'Date: 2019; 2019; 2019;',\n",
    "    'Location: Ottawa; Ottawa; London;'\n",
    "]\n",
    "\n",
    "cur_maxlen = min(max_length, 512 // len(column_list_3) - 1)\n",
    "\n",
    "# print(tokenizer.cls_token)\n",
    "# print(group_df[\"data\"])\n",
    "token_ids_list = [tokenizer.encode(\n",
    "                tokenizer.cls_token + \" \" + str(x), add_special_tokens=False, max_length=cur_maxlen, truncation=True) \n",
    "                  for x in column_list_3]\n",
    "token_ids = torch.LongTensor(reduce(operator.add,\n",
    "                                    token_ids_list)).to(device)\n",
    "cls_index_list = [0] + np.cumsum(\n",
    "    np.array([len(x) for x in token_ids_list])).tolist()[:-1]\n",
    "cls_indexes = torch.LongTensor(cls_index_list).to(device).reshape(-1, 1)\n",
    "cls_indexes = torch.hstack([torch.zeros_like(cls_indexes), cls_indexes])\n",
    "logits, embs_3 = model(token_ids.reshape(1, -1), cls_indexes=cls_indexes, get_enc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "cur_maxlen = min(max_length, 512 // len(column_list_3) - 1)\n",
    "\n",
    "# print(tokenizer.cls_token)\n",
    "# print(group_df[\"data\"])\n",
    "token_ids_list = [tokenizer.encode(\n",
    "                tokenizer.cls_token + \" \" + str(x), add_special_tokens=False, max_length=cur_maxlen, truncation=True) \n",
    "                  for x in column_list_3]\n",
    "token_ids = nn.utils.rnn.pad_sequence(\n",
    "      [torch.tensor(x) for x in token_ids_list], padding_value=tokenizer.pad_token_id).to(device)\n",
    "cls_indexes = torch.nonzero(\n",
    "                token_ids == tokenizer.cls_token_id)\n",
    "logits, embs_3_seperate = model(token_ids, cls_indexes=cls_indexes, get_enc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosine_similarity(embs_1.detach().cpu().numpy(), embs_2.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cosine_similarity(embs_1.detach().cpu().numpy(), embs_2.detach().cpu().numpy()).max(0))\n",
    "cosine_similarity(embs_1.detach().cpu().numpy(), embs_2.detach().cpu().numpy()).max(0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cosine_similarity(embs_1_seperate.detach().cpu().numpy(), embs_2_seperate.detach().cpu().numpy()).max(0))\n",
    "cosine_similarity(embs_1_seperate.detach().cpu().numpy(), embs_2_seperate.detach().cpu().numpy()).max(0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((final_scores>0.999).sum(), len(final_scores), (final_scores>0.999).sum()/len(final_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4026465/1559134942.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  test_dataset = torch.load(f\"/data/zhihao/TU/Watchog/verification/{args.task}_test_data.pth\")\n"
     ]
    }
   ],
   "source": [
    "test_dataset = torch.load(f\"/data/zhihao/TU/Watchog/verification/{args.task}_test_data.pth\")\n",
    "test_embs = test_dataset['embs']\n",
    "test_class = test_dataset['class']\n",
    "test_label = test_dataset['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'logits', 'cls_indexes', 'embs', 'target_embs', 'col_num', 'label', 'class'])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 3, 3, 3]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12501"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_embs_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11892"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_embs_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embs_pos = []\n",
    "test_embs_neg = []\n",
    "for batch_idx in range(len(test_embs)):\n",
    "    for i in range(len(test_embs[batch_idx])):\n",
    "        if test_labels[batch_idx][i].item() == 1:\n",
    "            test_embs_pos.append(test_embs[batch_idx][i])\n",
    "        else:\n",
    "            test_embs_neg.append(test_embs[batch_idx][i])\n",
    "test_embs_pos = torch.stack(test_embs_pos, dim=0)\n",
    "test_embs_neg = torch.stack(test_embs_neg, dim=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Pairwise cosine similarity\n",
    "test_similarity_matrix = cosine_similarity(test_embs_pos.numpy(), test_embs_neg.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12501, 768])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_embs_pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94780064"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_similarity_matrix.max(1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.111379325"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_similarity_matrix.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12501, 84031)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpvp_similarity_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4026465/557746203.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  veri_results = torch.load(f\"/data/zhihao/TU/Watchog/verification/{args.task}_veri_data_5.pth\")\n"
     ]
    }
   ],
   "source": [
    "veri_results = torch.load(f\"/data/zhihao/TU/Watchog/verification/{args.task}_veri_data_5.pth\")\n",
    "veri_label = veri_results['label']\n",
    "veri_embs = veri_results['embs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "veri_embs_pos = []\n",
    "veri_embs_neg = []\n",
    "for batch_idx in range(len(veri_embs)):\n",
    "    for i in range(len(veri_embs[batch_idx])):\n",
    "        if veri_label[batch_idx][i].item() == 1:\n",
    "            veri_embs_pos.append(veri_embs[batch_idx][i])\n",
    "        else:\n",
    "            veri_embs_neg.append(veri_embs[batch_idx][i])\n",
    "veri_embs_pos = torch.stack(veri_embs_pos, dim=0)\n",
    "veri_embs_neg = torch.stack(veri_embs_neg, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpvp_similarity_matrix = cosine_similarity(test_embs_pos.numpy(), veri_embs_pos.numpy())\n",
    "tpvn_similarity_matrix = cosine_similarity(test_embs_pos.numpy(), veri_embs_neg.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tnvp_similarity_matrix = cosine_similarity(test_embs_neg.numpy(), veri_embs_pos.numpy())\n",
    "tnvn_similarity_matrix = cosine_similarity(test_embs_neg.numpy(), veri_embs_neg.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9657009 0.17322846\n"
     ]
    }
   ],
   "source": [
    "print(tpvp_similarity_matrix.max(1).mean(), tpvp_similarity_matrix.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9559124 0.14575109\n"
     ]
    }
   ],
   "source": [
    "print(tpvn_similarity_matrix.max(1).mean(), tpvn_similarity_matrix.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8502803 0.08830399\n",
      "0.8699045 0.081692524\n"
     ]
    }
   ],
   "source": [
    "print(tnvp_similarity_matrix.max(1).mean(), tnvp_similarity_matrix.mean())\n",
    "print(tnvn_similarity_matrix.max(1).mean(), tnvn_similarity_matrix.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4026465/1451746444.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  veri_results = torch.load(f\"/data/zhihao/TU/Watchog/verification/{args.task}_veri_data_4.pth\")\n"
     ]
    }
   ],
   "source": [
    "veri_results = torch.load(f\"/data/zhihao/TU/Watchog/verification/{args.task}_veri_data_4.pth\")\n",
    "veri_label = veri_results['label']\n",
    "veri_embs = veri_results['embs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "veri_embs_pos = []\n",
    "veri_embs_neg = []\n",
    "for batch_idx in range(len(veri_embs)):\n",
    "    for i in range(len(veri_embs[batch_idx])):\n",
    "        if veri_label[batch_idx][i].item() == 1:\n",
    "            veri_embs_pos.append(veri_embs[batch_idx][i])\n",
    "        else:\n",
    "            veri_embs_neg.append(veri_embs[batch_idx][i])\n",
    "veri_embs_pos = torch.stack(veri_embs_pos, dim=0)\n",
    "veri_embs_neg = torch.stack(veri_embs_neg, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8687,  0.9182, -0.8263,  ...,  0.7522,  0.8715,  0.4586],\n",
       "        [ 0.8572,  0.9041, -0.7880,  ...,  0.8075,  0.8726,  0.5800],\n",
       "        [ 0.8715,  0.9171, -0.8301,  ...,  0.7428,  0.8663,  0.4430],\n",
       "        ...,\n",
       "        [ 0.7532,  0.9257, -0.8568,  ...,  0.7762,  0.8702,  0.7592],\n",
       "        [ 0.7319,  0.9229, -0.8659,  ...,  0.7707,  0.8672,  0.7369],\n",
       "        [ 0.6951,  0.9259, -0.8172,  ...,  0.8195,  0.8404,  0.7706]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_embs_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpvp_similarity_matrix = cosine_similarity(test_embs_pos.numpy(), veri_embs_pos.numpy())\n",
    "tpvn_similarity_matrix = cosine_similarity(test_embs_pos.numpy(), veri_embs_neg.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "tnvp_similarity_matrix = cosine_similarity(test_embs_neg.numpy(), veri_embs_pos.numpy())\n",
    "tnvn_similarity_matrix = cosine_similarity(test_embs_neg.numpy(), veri_embs_neg.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9656329 0.1726674\n",
      "0.9511409 0.1378151\n"
     ]
    }
   ],
   "source": [
    "print(tpvp_similarity_matrix.max(1).mean(), tpvp_similarity_matrix.mean())\n",
    "print(tpvn_similarity_matrix.max(1).mean(), tpvn_similarity_matrix.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "tnvp_similarity_matrix = cosine_similarity(test_embs_neg.numpy(), veri_embs_pos.numpy())\n",
    "tnvn_similarity_matrix = cosine_similarity(test_embs_neg.numpy(), veri_embs_neg.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.85004056 0.08802708\n",
      "0.86059237 0.07896456\n"
     ]
    }
   ],
   "source": [
    "print(tnvp_similarity_matrix.max(1).mean(), tnvp_similarity_matrix.mean())\n",
    "print(tnvn_similarity_matrix.max(1).mean(), tnvn_similarity_matrix.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.036336727\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "print(silhouette_score(torch.cat([test_embs_pos, veri_embs_pos], dim=0).numpy(), torch.cat([torch.zeros(test_embs_pos.shape[0]), torch.ones(veri_embs_pos.shape[0])]).numpy(), metric=\"cosine\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.015460037\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "print(silhouette_score(torch.cat([test_embs_pos, veri_embs_neg], dim=0).numpy(), torch.cat([torch.zeros(test_embs_pos.shape[0]), torch.ones(veri_embs_neg.shape[0])]).numpy(), metric=\"cosine\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.014984836\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "print(silhouette_score(torch.cat([test_embs_neg, veri_embs_neg], dim=0).numpy(), torch.cat([torch.zeros(test_embs_neg.shape[0]), torch.ones(veri_embs_neg.shape[0])]).numpy(), metric=\"cosine\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.045245163\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "print(silhouette_score(torch.cat([test_embs_neg, veri_embs_pos], dim=0).numpy(), torch.cat([torch.zeros(test_embs_neg.shape[0]), torch.ones(veri_embs_pos.shape[0])]).numpy(), metric=\"cosine\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
