diff --git a/data_viewer.ipynb b/data_viewer.ipynb
index c2dcbbd..4d2c02f 100644
--- a/data_viewer.ipynb
+++ b/data_viewer.ipynb
@@ -2,7 +2,7 @@
  "cells": [
   {
    "cell_type": "code",
-   "execution_count": 1,
+   "execution_count": 5,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -28,12 +28,25 @@
     "\n",
     "from watchog.dataset import TableDataset, SupCLTableDataset\n",
     "from watchog.model import SupCLforTable, UnsupCLforTable, SupclLoss\n",
-    "import wandb"
+    "import wandb\n",
+    "\n",
+    "import numpy as np\n",
+    "import pandas as pd\n",
+    "from sklearn.metrics import f1_score\n",
+    "import torch\n",
+    "import seaborn as sns"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 3,
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 2,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -66,7 +79,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 63,
+   "execution_count": 3,
    "metadata": {},
    "outputs": [
     {
@@ -76,7 +89,7 @@
      "traceback": [
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
       "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
-      "Input \u001b[0;32mIn [63]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mBertTokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(shortcut_name)\n",
+      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mBertTokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(shortcut_name)\n",
       "\u001b[0;31mNameError\u001b[0m: name 'BertTokenizer' is not defined"
      ]
     }
@@ -554,9 +567,29 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 69,
+   "execution_count": 6,
    "metadata": {},
-   "outputs": [],
+   "outputs": [
+    {
+     "ename": "FileNotFoundError",
+     "evalue": "[Errno 2] No such file or directory: '/data/zhihao/TU/GitTables/semtab_gittables/2022/dbpedia_property_train.csv'",
+     "output_type": "error",
+     "traceback": [
+      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
+      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
+      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_target_train \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/data/zhihao/TU/GitTables/semtab_gittables/2022/dbpedia_property_train.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m df_target_test \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/data/zhihao/TU/GitTables/semtab_gittables/2022/dbpedia_property_targets.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m df_gt_2021 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/data/zhihao/TU/GitTables/semtab_gittables/2021/dbpedia_gt.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
+      "File \u001b[0;32m~/jupyterprojects/jupyter/lib/python3.8/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
+      "File \u001b[0;32m~/jupyterprojects/jupyter/lib/python3.8/site-packages/pandas/io/parsers/readers.py:586\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    571\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    572\u001b[0m     dialect,\n\u001b[1;32m    573\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    582\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    583\u001b[0m )\n\u001b[1;32m    584\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 586\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
+      "File \u001b[0;32m~/jupyterprojects/jupyter/lib/python3.8/site-packages/pandas/io/parsers/readers.py:482\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    479\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    481\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 482\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    485\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
+      "File \u001b[0;32m~/jupyterprojects/jupyter/lib/python3.8/site-packages/pandas/io/parsers/readers.py:811\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwds:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 811\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
+      "File \u001b[0;32m~/jupyterprojects/jupyter/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1040\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1037\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown engine: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mengine\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (valid options are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmapping\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1038\u001b[0m     )\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;66;03m# error: Too many arguments for \"ParserBase\"\u001b[39;00m\n\u001b[0;32m-> 1040\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
+      "File \u001b[0;32m~/jupyterprojects/jupyter/lib/python3.8/site-packages/pandas/io/parsers/c_parser_wrapper.py:51\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     48\u001b[0m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musecols\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39musecols\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# open handles\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open_handles\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Have to pass int, would break tests using TextReader directly otherwise :(\u001b[39;00m\n",
+      "File \u001b[0;32m~/jupyterprojects/jupyter/lib/python3.8/site-packages/pandas/io/parsers/base_parser.py:222\u001b[0m, in \u001b[0;36mParserBase._open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_handles\u001b[39m(\u001b[38;5;28mself\u001b[39m, src: FilePathOrBuffer, kwds: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;124;03m    Let the readers open IOHandles after they are done with their potential raises.\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 222\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
+      "File \u001b[0;32m~/jupyterprojects/jupyter/lib/python3.8/site-packages/pandas/io/common.py:702\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    698\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    701\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 702\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    710\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    711\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
+      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/data/zhihao/TU/GitTables/semtab_gittables/2022/dbpedia_property_train.csv'"
+     ]
+    }
+   ],
    "source": [
     "df_target_train = pd.read_csv(\"/data/zhihao/TU/GitTables/semtab_gittables/2022/dbpedia_property_train.csv\")\n",
     "df_target_test = pd.read_csv(\"/data/zhihao/TU/GitTables/semtab_gittables/2022/dbpedia_property_targets.csv\")\n",
diff --git a/run_finetune_gittables22_pool.py b/run_finetune_gittables22_pool.py
index 585c331..b34e82a 100644
--- a/run_finetune_gittables22_pool.py
+++ b/run_finetune_gittables22_pool.py
@@ -22,7 +22,6 @@ from_scratch = True
 # from_scratch = True # True means using Huggingface's pre-trained language model's checkpoint
 eval_test = True
 colpair = False
-gpus = '1'
 small_tag = 'semi'
 max_unlabeled = 2
 comment = "max-unlabeled@{}".format(max_unlabeled)
@@ -70,14 +69,16 @@ comment = "max-unlabeled@{}".format(max_unlabeled)
 
 
     
-max_unlabeled = 2
-pool = 'v2'
-comment = f"pool@{pool}-max-unlabeled@{max_unlabeled}"
-for task in [ 'gt-semtab22-dbpedia1']:
+max_unlabeled = 8
+gpus = '1'
+pool = 'v0'
+rand = True
+comment = f"rand_pool@{pool}-max-unlabeled@{max_unlabeled}"
+for task in [ 'gt-semtab22-dbpedia-all0', 'gt-semtab22-dbpedia-all1']:
     cmd = '''CUDA_VISIBLE_DEVICES={} python supcl_ft.py \
-                --shortcut_name {} --task {} --max_length {} --max_unlabeled {} --pool_version {} --batch_size {} --epoch {} \
+                --shortcut_name {} --task {} --max_length {} --max_unlabeled {} --pool_version {} --random_sample {} --batch_size {} --epoch {} \
                 --dropout_prob {} --pretrained_ckpt_path "{}" --cl_tag {} --small_tag "{}" --comment "{}" {} {} {}'''.format(
-        gpus, base_model, task, ml, max_unlabeled, pool, bs, n_epochs, dropout_prob,
+        gpus, base_model, task, ml, max_unlabeled, pool, rand, bs, n_epochs, dropout_prob,
         ckpt_path, cl_tag, small_tag, comment,
         '--colpair' if colpair else '',
         '--from_scratch' if from_scratch else '',        
diff --git a/supcl_ft.py b/supcl_ft.py
index a818d7d..73f19d4 100644
--- a/supcl_ft.py
+++ b/supcl_ft.py
@@ -49,6 +49,7 @@ if __name__ == "__main__":
     parser = argparse.ArgumentParser()
     parser.add_argument("--model", type=str, default="Watchog")
     parser.add_argument("--pool_version", type=str, default="v0")
+    parser.add_argument("--random_sample", type=bool, default=False)
     parser.add_argument("--comment", type=str, default="debug", help="to distinguish the runs")
     parser.add_argument(
         "--shortcut_name",
@@ -202,8 +203,8 @@ if __name__ == "__main__":
 
     if args.from_scratch:
         if "gt" in task:
-            tag_name = "{}/{}-{}-pool{}-unlabeled{}-bs{}-ml{}-ne{}-do{}{}".format(
-                taskname,  "{}-fromscratch".format(shortcut_name), args.small_tag, args.pool_version, args.max_unlabeled,
+            tag_name = "{}/{}-{}-pool{}-unlabeled{}-rand{}-bs{}-ml{}-ne{}-do{}{}".format(
+                taskname,  "{}-fromscratch".format(shortcut_name), args.small_tag, args.pool_version, args.max_unlabeled, args.random_sample,
                 batch_size, max_length, num_train_epochs, args.dropout_prob, 
                 '-rs{}'.format(args.random_seed) if args.random_seed != 4649 else '')
         else:
@@ -214,8 +215,8 @@ if __name__ == "__main__":
         
     else:
         if "gt" in task:
-            tag_name = "{}/{}_{}-pool{}-unlabeled{}-bs{}-ml{}-ne{}-do{}{}".format(
-                taskname, args.cl_tag.replace('/', '-'),  shortcut_name, args.small_tag, args.pool_version, args.max_unlabeled,
+            tag_name = "{}/{}_{}-pool{}-unlabeled{}-rand{}-bs{}-ml{}-ne{}-do{}{}".format(
+                taskname, args.cl_tag.replace('/', '-'),  shortcut_name, args.small_tag, args.pool_version, args.max_unlabeled, args.random_sample,
                 batch_size, max_length, num_train_epochs, args.dropout_prob,
                 '-rs{}'.format(args.random_seed) if args.random_seed != 4649 else '')
         else:
@@ -257,9 +258,9 @@ if __name__ == "__main__":
     setattr(ckpt_hp, 'shortcut_name', args.shortcut_name)
     setattr(ckpt_hp, 'num_labels', args.num_classes)
     
-    pre_model, trainset = load_checkpoint(ckpt)
-    tokenizer = trainset.tokenizer
-
+    
+    
+    tokenizer = BertTokenizer.from_pretrained(shortcut_name)
     if task == "turl-re" and args.colpair:
         model = BertForMultiOutputClassification(ckpt_hp, device=device, lm=ckpt['hp'].lm, col_pair='Pair')
     elif "col-popl" in task:
@@ -269,17 +270,19 @@ if __name__ == "__main__":
         
 
     if not args.from_scratch:
+        pre_model, trainset = load_checkpoint(ckpt)
         model.bert = pre_model.bert
-
+        tokenizer = trainset.tokenizer
+        del pre_model
     if task == "turl-re" and args.colpair and ckpt['hp'].lm != 'distilbert':
         config = BertConfig.from_pretrained(lm_mp[ckpt['hp'].lm])
         model.bert.pooler = BertMultiPairPooler(config).to(device)
         print("Use column-pair pooling")
         # print(type(model.bert.pooler), model.bert.pooler.hidden_size)
 
-    del pre_model
+    
         
-    padder = collate_fn(trainset.tokenizer.pad_token_id)
+    padder = collate_fn(tokenizer.pad_token_id)
     with accelerator.main_process_first():
         if task in [
                 "sato0", "sato1", "sato2", "sato3", "sato4", "msato0",
@@ -301,7 +304,8 @@ if __name__ == "__main__":
                                         train_ratio=1.0,
                                         device=device,
                                         small_tag=args.small_tag,
-                                        base_dirpath=os.path.join(args.data_path, "doduo", "data"))
+                                        base_dirpath=os.path.join(args.data_path, "doduo", "data"), 
+                                        random_sample=args.random_sample)
             valid_dataset = dataset_cls(cv=cv,
                                         split="valid",
                                         tokenizer=tokenizer,
diff --git a/watchog/__pycache__/__init__.cpython-38.pyc b/watchog/__pycache__/__init__.cpython-38.pyc
index 7350ab1..703f85f 100644
Binary files a/watchog/__pycache__/__init__.cpython-38.pyc and b/watchog/__pycache__/__init__.cpython-38.pyc differ
diff --git a/watchog/__pycache__/augment.cpython-38.pyc b/watchog/__pycache__/augment.cpython-38.pyc
index 0cad96e..bc50cf4 100644
Binary files a/watchog/__pycache__/augment.cpython-38.pyc and b/watchog/__pycache__/augment.cpython-38.pyc differ
diff --git a/watchog/__pycache__/dataset.cpython-38.pyc b/watchog/__pycache__/dataset.cpython-38.pyc
index 2bf2366..6aa1598 100644
Binary files a/watchog/__pycache__/dataset.cpython-38.pyc and b/watchog/__pycache__/dataset.cpython-38.pyc differ
diff --git a/watchog/__pycache__/model.cpython-38.pyc b/watchog/__pycache__/model.cpython-38.pyc
index be65ab8..a8be5da 100644
Binary files a/watchog/__pycache__/model.cpython-38.pyc and b/watchog/__pycache__/model.cpython-38.pyc differ
diff --git a/watchog/__pycache__/preprocessor.cpython-38.pyc b/watchog/__pycache__/preprocessor.cpython-38.pyc
index 9af5857..2b39dab 100644
Binary files a/watchog/__pycache__/preprocessor.cpython-38.pyc and b/watchog/__pycache__/preprocessor.cpython-38.pyc differ
diff --git a/watchog/__pycache__/utils.cpython-38.pyc b/watchog/__pycache__/utils.cpython-38.pyc
index be92911..5358f51 100644
Binary files a/watchog/__pycache__/utils.cpython-38.pyc and b/watchog/__pycache__/utils.cpython-38.pyc differ
diff --git a/watchog/dataset.py b/watchog/dataset.py
index bcadac4..a6f2bb4 100644
--- a/watchog/dataset.py
+++ b/watchog/dataset.py
@@ -1542,7 +1542,9 @@ class GittablesTablewiseDataset(data.Dataset):
             base_tag: str = '', # blank, comma
             small_tag: str = "",
             train_ratio: float = 1.0,
-            max_unlabeled=4):
+            max_unlabeled=8,
+            random_sample=False, # TODO
+            train_only=True): # TODO
         if device is None:
             device = torch.device('cpu')
         basename = small_tag+ "_cv_{}.csv"
@@ -1565,7 +1567,8 @@ class GittablesTablewiseDataset(data.Dataset):
 
         if gt_only:
             df = df[df["class_id"] > -1]
-        
+        if train_only and split != "train":
+            df = df[df["class_id"] > -1]
 
         
         data_list = []
@@ -1589,9 +1592,11 @@ class GittablesTablewiseDataset(data.Dataset):
             #     break
             labeled_columns = group_df[group_df['class_id'] > -1]
             unlabeled_columns = group_df[group_df['class_id'] == -1]
+            num_unlabeled = min(max(max_unlabeled-len(labeled_columns), 0), len(unlabeled_columns))
+            unlabeled_columns = unlabeled_columns.sample(num_unlabeled) if random_sample else unlabeled_columns[0:num_unlabeled]
             # group_df = pd.concat([group_df[group_df['class_id'] > -1], unlabeled_columns.sample(min(10-len(labeled_columns), len(unlabeled_columns)))])
             # group_df = pd.concat([group_df[group_df['class_id'] > -1], unlabeled_columns[0:min(max(10-len(labeled_columns), 0), len(unlabeled_columns))]])
-            group_df = pd.concat([group_df[group_df['class_id'] > -1], unlabeled_columns[0:min(max(max_unlabeled-len(labeled_columns), 0), len(unlabeled_columns))]])
+            group_df = pd.concat([group_df[group_df['class_id'] > -1], unlabeled_columns]) # TODO
             group_df.sort_values(by=['col_idx'], inplace=True)
 
             if max_length <= 128:
diff --git a/watchog/model.py b/watchog/model.py
index 9841af4..7b86cdb 100644
--- a/watchog/model.py
+++ b/watchog/model.py
@@ -480,6 +480,11 @@ class BertMultiPooler(nn.Module):
             pooled_outputs = self.dense(pooled_outputs)
             tab_outputs = pooler_output[cls_indexes[:,0]] # (B, hidden_size)
             pooled_outputs = pooled_outputs + self.dense_tab(tab_outputs)
+        elif self.version == "v4.1": 
+            hidden_states = self.dense(hidden_states)
+            pooled_outputs = pool_sub_sentences(hidden_states, cls_indexes, table_length)
+            tab_outputs = pooler_output[cls_indexes[:,0]] # (B, hidden_size)
+            pooled_outputs = pooled_outputs + self.dense_tab(tab_outputs)            
         else:
             raise ValueError(f"Invalid version: {self.version}")
         pooled_outputs = self.activation(pooled_outputs)
