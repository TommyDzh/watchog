/home/zhihao/jupyterprojects/jupyter/lib/python3.8/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.
Epoch 0 starts
/home/zhihao/jupyterprojects/jupyter/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))
Epoch 0 (gt-semtab22-dbpedia0): tr_loss=3.4676811 tr_macro_f1=0.0147 tr_micro_f1=0.2668  vl_loss=3.1636534 vl_macro_f1=0.0268 vl_micro_f1=0.3091 (32.37 sec.)
Epoch 1 starts
/home/zhihao/jupyterprojects/jupyter/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, "true nor predicted", "F-score is", len(true_sum))
Epoch 1 (gt-semtab22-dbpedia0): tr_loss=2.9669857 tr_macro_f1=0.0443 tr_micro_f1=0.3549  vl_loss=2.8671178 vl_macro_f1=0.0608 vl_micro_f1=0.3575 (29.56 sec.)
Epoch 2 starts
Traceback (most recent call last):
  File "supcl_ft.py", line 705, in <module>
    optimizer.step()
  File "/home/zhihao/jupyterprojects/jupyter/lib/python3.8/site-packages/accelerate/optimizer.py", line 145, in step
    self.optimizer.step(closure)
  File "/home/zhihao/jupyterprojects/jupyter/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 65, in wrapper
    return wrapped(*args, **kwargs)
  File "/home/zhihao/jupyterprojects/jupyter/lib/python3.8/site-packages/torch/optim/optimizer.py", line 109, in wrapper
    return func(*args, **kwargs)
  File "/home/zhihao/jupyterprojects/jupyter/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/home/zhihao/jupyterprojects/jupyter/lib/python3.8/site-packages/transformers/optimization.py", line 494, in step
    p.addcdiv_(exp_avg, denom, value=-step_size)
KeyboardInterrupt