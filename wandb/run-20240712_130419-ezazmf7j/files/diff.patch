diff --git a/run_supcl_train.py b/run_supcl_train.py
index 1ffc274..bbeb962 100644
--- a/run_supcl_train.py
+++ b/run_supcl_train.py
@@ -4,13 +4,12 @@ pretrain_data = 'wikitables'
 ml = 256
 bs = 32
 # cuda_devices = '0,1,2,3'
-cuda_devices = '0'
+cuda_devices = '0,1'
 
 '''unsupervised'''
-# mode = 'simclr'
 task = 'None'
 '''supervised with header'''
-mode = 'supcon' 
+mode = "simclr" # 'supcon' 
 task = 'header'
 
 ao = 'sample_row4,sample_row4'
@@ -20,20 +19,20 @@ lm = 'bert'
 
 gpus = ','.join([str(i) for i in range(len(cuda_devices.split(',')))])
 n_epochs = 10
-size = 100000
+save_model = 10
+size = -1
 cnt = 0
 run_id = 0
 temp = 0.05
 # temp = 0.07
 
-data_path = "/efs/task_datasets/TURL/"
+data_path = "/data/zhihao/TU/TURL/"
 
 if len(cuda_devices.split(',')) > 1:
     cmd = """accelerate launch --config_file accelerate_config.yaml supcl_train.py --fp16 \
         --data_path %s \
         --pretrain_data %s \
         --mode %s \
-        --task %s \
         --batch_size %s \
         --lr 5e-5 \
         --temperature %s \
@@ -41,16 +40,15 @@ if len(cuda_devices.split(',')) > 1:
         --n_epochs %d \
         --max_len %d \
         --size %d \
-        --save_model \
+        --save_model %d \
         --augment_op %s \
         --sample_meth %s \
-        --run_id %d""" % (data_path, pretrain_data, mode, task, bs, temp, lm, n_epochs, ml, size, ao, sm, run_id)
+        --run_id %d""" % (data_path, pretrain_data, mode, bs, temp, lm, n_epochs, ml, size, save_model, ao, sm, run_id)
 else:
     cmd = """python3 supcl_train.py --fp16 \
         --data_path %s \
         --pretrain_data %s \
         --mode %s \
-        --task %s \
         --batch_size %s \
         --lr 5e-5 \
         --temperature %s \
@@ -58,10 +56,10 @@ else:
         --n_epochs %d \
         --max_len %d \
         --size %d \
-        --save_model \
+        --save_model %d \
         --augment_op %s \
         --sample_meth %s \
-        --run_id %d""" % (data_path, pretrain_data, mode, task, bs, temp, lm, n_epochs, ml, size, ao, sm, run_id)
+        --run_id %d""" % (data_path, pretrain_data, mode, bs, temp, lm, n_epochs, ml, size, save_model, ao, sm, run_id)
 print(cmd)
 # os.system('CUDA_VISIBLE_DEVICES={} {}'.format(
 os.system('CUDA_VISIBLE_DEVICES={} {} &> {} &'.format(
diff --git a/supcl_ft.py b/supcl_ft.py
index b5199cc..667ec19 100644
--- a/supcl_ft.py
+++ b/supcl_ft.py
@@ -33,7 +33,7 @@ from watchog.model import SupCLforTable, UnsupCLforTable, lm_mp
 from watchog.utils import load_checkpoint, f1_score_multilabel, collate_fn, get_col_pred, ColPoplEvaluator
 from watchog.utils import task_num_class_dict
 from accelerate import DistributedDataParallelKwargs
-
+import wandb
 
 def set_seed(seed):
     random.seed(seed)
@@ -45,6 +45,7 @@ if __name__ == "__main__":
     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
 
     parser = argparse.ArgumentParser()
+    parser.add_argument("--model", type=str, default="Watchog")
     parser.add_argument(
         "--shortcut_name",
         default="bert-base-uncased",
@@ -226,6 +227,9 @@ if __name__ == "__main__":
     
     if args.fp16:
         torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = True
+        
+      
+        
     # accelerator = Accelerator(mixed_precision="no" if not args.fp16 else "fp16")   
     ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)
     accelerator = Accelerator(mixed_precision="no" if not args.fp16 else "fp16", kwargs_handlers=[ddp_kwargs])
@@ -266,6 +270,7 @@ if __name__ == "__main__":
         
     padder = collate_fn(trainset.tokenizer.pad_token_id)
     with accelerator.main_process_first():
+        
     # if True:
         if task in [
                 "sato0", "sato1", "sato2", "sato3", "sato4", "msato0",
@@ -527,7 +532,15 @@ if __name__ == "__main__":
         else:
             raise ValueError("task name must be either sato or turl.")
 
-
+    if accelerator.is_local_main_process:
+        wandb.init(config=args,
+            project="TableUnderstanding",
+            name=f"{args.model}_DS@{args.task}_scratch@{args.from_scratch}_meta@{args.meta}",
+            group="TU",
+            )
+        wandb.log({
+                f"tag_name": tag_name,
+            }, commit=True)
     t_total = len(train_dataloader) * num_train_epochs
     no_decay = ["bias", "LayerNorm.weight"]
     optimizer_grouped_parameters = [
@@ -576,6 +589,7 @@ if __name__ == "__main__":
     best_vl_macro_f1s_epoch = -1
     loss_info_list = []
     eval_dict = []
+    time_epochs = []
     for epoch in range(num_train_epochs):
         t1 = time()
         print("Epoch", epoch, "starts")
@@ -782,7 +796,7 @@ if __name__ == "__main__":
                 vl_micro_f1, vl_macro_f1, vl_class_f1, _ = f1_score_multilabel(
                     vl_true_list, vl_pred_list)
             
-                
+            t2 = time()
             if not args.eval_test:
                 if vl_micro_f1 > best_vl_micro_f1:
                     best_vl_micro_f1 = vl_micro_f1
@@ -798,13 +812,12 @@ if __name__ == "__main__":
                     tr_loss, tr_macro_f1, tr_micro_f1, vl_loss, vl_macro_f1,
                     vl_micro_f1
                 ])
-                t2 = time()
                 print(
                     "Epoch {} ({}): tr_loss={:.7f} tr_macro_f1={:.4f} tr_micro_f1={:.4f} "
                     .format(epoch, task, tr_loss, tr_macro_f1, tr_micro_f1),
                     "vl_loss={:.7f} vl_macro_f1={:.4f} vl_micro_f1={:.4f} ({:.2f} sec.)"
                     .format(vl_loss, vl_macro_f1, vl_micro_f1, (t2 - t1)))
-            else:
+            else: # TODO
                 if "popl" in task:
                     ts_pred_list = {}
                     ts_true_list = {}
@@ -897,7 +910,6 @@ if __name__ == "__main__":
                     ts_micro_f1, ts_macro_f1, ts_class_f1, ts_conf_mat = f1_score_multilabel(
                         ts_true_list, ts_pred_list)
 
-                t2 = time()
                 if "popl" in task:
                     if vl_map >= best_vl_micro_f1:
                         best_vl_micro_f1 = vl_map
@@ -943,11 +955,12 @@ if __name__ == "__main__":
                         .format(vl_loss, vl_macro_f1, vl_micro_f1, (t2 - t1)),
                         "ts_macro_f1={:.4f} ts_micro_f1={:.4f} ({:.2f} sec.)"
                         .format(ts_macro_f1, ts_micro_f1, t2-t1))
-
+                    time_epoch = t2-t1
+                    time_epochs.append(time_epoch)
                     metrics_dict = {'epoch': epoch, 'tr_loss':tr_loss, 'tr_macro_f1':tr_macro_f1, 'tr_micro_f1': tr_micro_f1,
                         'vl_loss':vl_loss, 'vl_macro_f1':vl_macro_f1, 'vl_micro_f1':vl_micro_f1, #'vl_class_f1':vl_class_f1,
                         'ts_macro_f1':ts_macro_f1, 'ts_micro_f1':ts_micro_f1, #'ts_class_f1':ts_class_f1, 'ts_conf_mat':ts_conf_mat,
-                        'time': t2-t1
+                        'time': time_epoch
                     }
                     mlflow.log_metrics(metrics_dict)
                     eval_dict.append(metrics_dict)
@@ -957,7 +970,16 @@ if __name__ == "__main__":
                     if type(ts_conf_mat) != list:
                         ts_conf_mat = ts_conf_mat.tolist()
                     eval_dict[epoch]["confusion_matrix"] = ts_conf_mat
-            
+                    if accelerator.is_local_main_process:
+                        wandb.log({
+                                f"train/loss": tr_loss,
+                                f"train/macro_f1": tr_macro_f1,
+                                f"train/micro_f1": tr_micro_f1,
+                                f"valid/loss": vl_loss,
+                                f"valid/macro_f1": vl_macro_f1,
+                                f"valid/micro_f1": vl_micro_f1,
+                                f"train/time": time_epoch,
+                            }, step=epoch+1, commit=True)
 
     with accelerator.main_process_first():
         if args.eval_test:
@@ -982,7 +1004,6 @@ if __name__ == "__main__":
             output_filepath = "{}_eval.json".format(tag_name)
             with open(output_filepath, "w") as fout:
                 json.dump(eval_dict, fout)
-
         else:
             loss_info_df = pd.DataFrame(loss_info_list,
                                         columns=[
diff --git a/supcl_train.py b/supcl_train.py
index d6d6972..a885f63 100644
--- a/supcl_train.py
+++ b/supcl_train.py
@@ -20,6 +20,7 @@ from accelerate import DistributedDataParallelKwargs
 
 from watchog.dataset import TableDataset, SupCLTableDataset
 from watchog.model import SupCLforTable, UnsupCLforTable, SupclLoss
+import wandb
 
 def train_step(train_iter, model, optimizer, scheduler, accelerator, criterion, hp):
     """Perform a single training step
@@ -95,7 +96,12 @@ def train_step(train_iter, model, optimizer, scheduler, accelerator, criterion,
 
 
 def train(accelerator, trainset, hp, validset=None):
-
+    if accelerator.is_local_main_process:
+        wandb.init(config=hp,
+            project="TableUnderstanding",
+            name=f"{hp.model}_Pretrain_DS@{hp.pretrain_data}_mode@{hp.mode}_size@{hp.size}",
+            group="TU",
+            )
     # initialize model, optimizer, and LR scheduler
     device = accelerator.device
     if hp.mode in ['simclr']:
@@ -134,7 +140,7 @@ def train(accelerator, trainset, hp, validset=None):
         directory = os.path.join(hp.logdir, hp.pretrain_data, hp.mode)
         if not os.path.exists(directory):
             os.makedirs(directory)
-
+    time_epochs = []
     for epoch in range(1, hp.n_epochs+1):
         # train
         accelerator.print("Epoch {} starts.".format(epoch))
@@ -158,17 +164,31 @@ def train(accelerator, trainset, hp, validset=None):
             accelerator.save(ckpt, ckpt_path)
 
         end_time = time.time()
+        time_epoch = end_time - start_time
+        time_epochs.append(time_epoch)
         accelerator.print("Epoch {} training ends, took {} secs.".format(epoch, end_time - start_time))
         accelerator.print("   Training loss=%f"  %train_loss)
-        
+        if accelerator.is_local_main_process:
+            wandb.log({
+                    f"train/loss": train_loss,
+                    f"train/time": time_epoch,
+                }, step=epoch, commit=True)
+    if accelerator.is_local_main_process:
+        avg_train_time = sum(time_epochs) / len(time_epochs)
+        wandb.log({
+            f"train/avg_time": avg_train_time,
+            "ckpt_path": ckpt_path,
+        }, commit=True)
+        wandb.finish()
     
 if __name__ == '__main__':
     parser = argparse.ArgumentParser()
+    parser.add_argument("--model", type=str, default="Watchog") # simclr for original CL, supcon for CL using metadata
     parser.add_argument("--pretrain_data", type=str, default="wikitables") # dataset for pretraining
     parser.add_argument("--pretrained_model_path", type=str, default="") # pretrained checkpoint 
-    parser.add_argument("--data_path",type=str, default="./data/doduo")
+    parser.add_argument("--data_path",type=str, default="/data/zhihao/TU/TURL/")
     parser.add_argument("--mode", type=str, default="simclr") # simclr for original CL, supcon for CL using metadata
-    parser.add_argument("--logdir", type=str, default="results/") # directory to store model checkpoints
+    parser.add_argument("--logdir", type=str, default="/data/zhihao/TU/Watchog/model/") # directory to store model checkpoints
     parser.add_argument("--run_id", type=int, default=0)
     parser.add_argument("--batch_size", type=int, default=32)
     parser.add_argument("--max_len", type=int, default=128)
@@ -177,16 +197,18 @@ if __name__ == '__main__':
     parser.add_argument("--n_epochs", type=int, default=20)
     parser.add_argument("--lm", type=str, default='bert-base-uncased')
     parser.add_argument("--projector", type=int, default=768)
-    parser.add_argument("--augment_op", type=str, default='sample_row,sample_row')
+    parser.add_argument("--augment_op", type=str, default='sample_row4,sample_row4')
     parser.add_argument("--table_order", type=str, default='column')
-    parser.add_argument("--sample_meth", type=str, default='head')
+    parser.add_argument("--sample_meth", type=str, default='tfidf_entity')
     parser.add_argument("--temperature", type=float, default=0.05)
     parser.add_argument("--save_model", type=int, default=5)
-    parser.add_argument("--fp16", dest="fp16", action="store_true")
+    parser.add_argument("--fp16", dest="fp16", default=True, action="store_true")
     parser.add_argument("--gpus", type=str, default="0")
-    
+    parser.add_argument("--single_column", default=False)
     
     hp = parser.parse_args()
+    if hp.size == -1:
+        hp.size = None
     # set seed
     seed = hp.run_id
     random.seed(seed)
diff --git a/watchog/dataset.py b/watchog/dataset.py
index 76095ca..590df63 100644
--- a/watchog/dataset.py
+++ b/watchog/dataset.py
@@ -236,7 +236,7 @@ class TableDataset(data.Dataset):
             Dictionary: a map from column names to the position of corresponding special tokens
         """
         res = []
-        max_tokens = self.max_len * 2 // len(table.columns)
+        max_tokens = self.max_len * 2 // len(table.columns) # max tokens per column
         budget = max(1, self.max_len // len(table.columns) - 1)
         tfidfDict = computeTfIdf(table) if "tfidf" in self.sample_meth else None # from preprocessor.py
 
@@ -246,7 +246,7 @@ class TableDataset(data.Dataset):
         # column-ordered preprocessing
         if self.table_order == 'column':
             if 'row' in self.sample_meth: 
-                table = tfidfRowSample(table, tfidfDict, max_tokens)
+                table = tfidfRowSample(table, tfidfDict, max_tokens) # TODO
             for column in table.columns:
                 tokens = preprocess(table[column], tfidfDict, max_tokens, self.sample_meth) # from preprocessor.py
                 col_text = self.tokenizer.cls_token + " " + \
@@ -337,7 +337,7 @@ class TableDataset(data.Dataset):
         cls_indices = []
         for col in mp_ori:
             if col in mp_aug:
-                cls_indices.append((mp_ori[col], mp_aug[col]))
+                cls_indices.append((mp_ori[col], mp_aug[col])) 
 
         return x_ori, x_aug, cls_indices
 
