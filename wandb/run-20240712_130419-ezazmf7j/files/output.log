/home/zhihao/jupyterprojects/jupyter/lib/python3.8/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.
Epoch 1 starts.
step: 0, loss: 4.565886974334717
step: 10, loss: 3.0367772579193115
step: 20, loss: 2.792182207107544
step: 30, loss: 2.6816108226776123
step: 40, loss: 2.7991788387298584
step: 50, loss: 2.818354368209839
step: 60, loss: 1.782254695892334
step: 70, loss: 1.0855767726898193
step: 80, loss: 0.5660673975944519
step: 90, loss: 0.2750665247440338
step: 100, loss: 0.2894928753376007
step: 110, loss: 0.6098920106887817
step: 120, loss: 0.2050919532775879
step: 130, loss: 0.2097553163766861
step: 140, loss: 0.29419413208961487
step: 150, loss: 0.4189310073852539
step: 160, loss: 0.04976418614387512
step: 170, loss: 0.13380740582942963
step: 180, loss: 0.10861410200595856
step: 190, loss: 0.04166174307465553
step: 200, loss: 0.09240186214447021
step: 210, loss: 0.1403014361858368
step: 220, loss: 0.07589080929756165
step: 230, loss: 0.06018494814634323
step: 240, loss: 0.04363342374563217
step: 250, loss: 0.4845878481864929
step: 260, loss: 0.2233072817325592
step: 270, loss: 0.13751061260700226
step: 280, loss: 0.10452986508607864
step: 290, loss: 0.10705260932445526
step: 300, loss: 0.05650141462683678
step: 310, loss: 0.08305928111076355
step: 320, loss: 0.1758459359407425
step: 330, loss: 0.03861052915453911
step: 340, loss: 0.10258244723081589
step: 350, loss: 0.008746542036533356
step: 360, loss: 0.030195562168955803
step: 370, loss: 0.12343145906925201
step: 380, loss: 0.033741604536771774
step: 390, loss: 0.14779743552207947
step: 400, loss: 0.15330064296722412
step: 410, loss: 0.01516241766512394
step: 420, loss: 0.11607569456100464
step: 430, loss: 0.03958435356616974
step: 440, loss: 0.13853603601455688
step: 450, loss: 0.1457553505897522
step: 460, loss: 0.12549617886543274
step: 470, loss: 0.01063639298081398
