/home/zhihao/jupyterprojects/jupyter/lib/python3.8/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.
Epoch 0 starts
Epoch 0 (sato0): tr_loss=0.9049515 tr_macro_f1=0.4511 tr_micro_f1=0.7653  vl_loss=0.4770415 vl_macro_f1=0.5990 vl_micro_f1=0.8859 (278.80 sec.)
Epoch 1 starts
Epoch 1 (sato0): tr_loss=0.3348608 tr_macro_f1=0.6826 tr_micro_f1=0.9177  vl_loss=0.3857119 vl_macro_f1=0.7027 vl_micro_f1=0.9108 (276.52 sec.)
Epoch 2 starts
Epoch 2 (sato0): tr_loss=0.2222920 tr_macro_f1=0.7759 tr_micro_f1=0.9437  vl_loss=0.3532177 vl_macro_f1=0.7573 vl_micro_f1=0.9220 (278.23 sec.)
Epoch 3 starts
Epoch 3 (sato0): tr_loss=0.1679481 tr_macro_f1=0.8238 tr_micro_f1=0.9570  vl_loss=0.3517293 vl_macro_f1=0.7667 vl_micro_f1=0.9246 (285.04 sec.)
Epoch 4 starts
Epoch 4 (sato0): tr_loss=0.1297908 tr_macro_f1=0.8613 tr_micro_f1=0.9668  vl_loss=0.3948121 vl_macro_f1=0.7807 vl_micro_f1=0.9208 (278.51 sec.)
Epoch 5 starts
Epoch 5 (sato0): tr_loss=0.1105460 tr_macro_f1=0.8787 tr_micro_f1=0.9706  vl_loss=0.3933545 vl_macro_f1=0.7886 vl_micro_f1=0.9156 (278.87 sec.)
Epoch 6 starts
Epoch 6 (sato0): tr_loss=0.0902326 tr_macro_f1=0.9073 tr_micro_f1=0.9762  vl_loss=0.3653736 vl_macro_f1=0.7815 vl_micro_f1=0.9273 (280.57 sec.)
Epoch 7 starts
Epoch 7 (sato0): tr_loss=0.0752619 tr_macro_f1=0.9161 tr_micro_f1=0.9800  vl_loss=0.3657488 vl_macro_f1=0.8045 vl_micro_f1=0.9334 (278.59 sec.)
Epoch 8 starts
Epoch 8 (sato0): tr_loss=0.0624711 tr_macro_f1=0.9360 tr_micro_f1=0.9840  vl_loss=0.4171216 vl_macro_f1=0.7887 vl_micro_f1=0.9266 (279.13 sec.)
Epoch 9 starts
Epoch 9 (sato0): tr_loss=0.0576773 tr_macro_f1=0.9417 tr_micro_f1=0.9848  vl_loss=0.3941582 vl_macro_f1=0.8005 vl_micro_f1=0.9295 (276.80 sec.)
Epoch 10 starts
Epoch 10 (sato0): tr_loss=0.0516434 tr_macro_f1=0.9481 tr_micro_f1=0.9867  vl_loss=0.4223074 vl_macro_f1=0.8026 vl_micro_f1=0.9290 (267.91 sec.)
Epoch 11 starts
Epoch 11 (sato0): tr_loss=0.0417440 tr_macro_f1=0.9585 tr_micro_f1=0.9893  vl_loss=0.3903512 vl_macro_f1=0.8049 vl_micro_f1=0.9348 (269.70 sec.)
Epoch 12 starts
Epoch 12 (sato0): tr_loss=0.0395722 tr_macro_f1=0.9635 tr_micro_f1=0.9893  vl_loss=0.4062395 vl_macro_f1=0.8014 vl_micro_f1=0.9333 (277.44 sec.)
Epoch 13 starts
Epoch 13 (sato0): tr_loss=0.0334960 tr_macro_f1=0.9660 tr_micro_f1=0.9912  vl_loss=0.4343070 vl_macro_f1=0.8091 vl_micro_f1=0.9324 (273.34 sec.)
Epoch 14 starts
Epoch 14 (sato0): tr_loss=0.0289297 tr_macro_f1=0.9662 tr_micro_f1=0.9919  vl_loss=0.4281069 vl_macro_f1=0.7885 vl_micro_f1=0.9331 (277.53 sec.)
Epoch 15 starts
Epoch 15 (sato0): tr_loss=0.0233940 tr_macro_f1=0.9735 tr_micro_f1=0.9936  vl_loss=0.4298258 vl_macro_f1=0.7991 vl_micro_f1=0.9314 (277.63 sec.)
Epoch 16 starts
Epoch 16 (sato0): tr_loss=0.0252619 tr_macro_f1=0.9762 tr_micro_f1=0.9936  vl_loss=0.4323763 vl_macro_f1=0.8059 vl_micro_f1=0.9339 (277.24 sec.)
Epoch 17 starts
Epoch 17 (sato0): tr_loss=0.0205396 tr_macro_f1=0.9820 tr_micro_f1=0.9943  vl_loss=0.4533622 vl_macro_f1=0.8049 vl_micro_f1=0.9338 (268.08 sec.)
Epoch 18 starts
Epoch 18 (sato0): tr_loss=0.0195354 tr_macro_f1=0.9863 tr_micro_f1=0.9948  vl_loss=0.4623572 vl_macro_f1=0.8133 vl_micro_f1=0.9350 (273.10 sec.)
Epoch 19 starts
Epoch 19 (sato0): tr_loss=0.0164872 tr_macro_f1=0.9868 tr_micro_f1=0.9955  vl_loss=0.4627861 vl_macro_f1=0.8102 vl_micro_f1=0.9359 (282.37 sec.)
Epoch 20 starts
Epoch 20 (sato0): tr_loss=0.0148334 tr_macro_f1=0.9890 tr_micro_f1=0.9958  vl_loss=0.4654083 vl_macro_f1=0.8152 vl_micro_f1=0.9348 (282.94 sec.)
Epoch 21 starts
Epoch 21 (sato0): tr_loss=0.0130933 tr_macro_f1=0.9891 tr_micro_f1=0.9963  vl_loss=0.4657587 vl_macro_f1=0.8133 vl_micro_f1=0.9364 (280.12 sec.)
Epoch 22 starts
Epoch 22 (sato0): tr_loss=0.0108888 tr_macro_f1=0.9916 tr_micro_f1=0.9967  vl_loss=0.4816037 vl_macro_f1=0.8117 vl_micro_f1=0.9370 (281.83 sec.)
Epoch 23 starts
Epoch 23 (sato0): tr_loss=0.0094381 tr_macro_f1=0.9929 tr_micro_f1=0.9970  vl_loss=0.4854140 vl_macro_f1=0.8259 vl_micro_f1=0.9360 (278.16 sec.)
Epoch 24 starts
Epoch 24 (sato0): tr_loss=0.0092212 tr_macro_f1=0.9943 tr_micro_f1=0.9973  vl_loss=0.4863539 vl_macro_f1=0.8099 vl_micro_f1=0.9365 (278.54 sec.)
Epoch 25 starts
Epoch 25 (sato0): tr_loss=0.0078696 tr_macro_f1=0.9942 tr_micro_f1=0.9975  vl_loss=0.4781209 vl_macro_f1=0.8182 vl_micro_f1=0.9370 (266.60 sec.)
Epoch 26 starts
Epoch 26 (sato0): tr_loss=0.0067800 tr_macro_f1=0.9947 tr_micro_f1=0.9978  vl_loss=0.4868207 vl_macro_f1=0.8165 vl_micro_f1=0.9382 (269.65 sec.)
Epoch 27 starts
Epoch 27 (sato0): tr_loss=0.0063193 tr_macro_f1=0.9949 tr_micro_f1=0.9980  vl_loss=0.4968020 vl_macro_f1=0.8237 vl_micro_f1=0.9391 (278.09 sec.)
Epoch 28 starts
Epoch 28 (sato0): tr_loss=0.0058414 tr_macro_f1=0.9947 tr_micro_f1=0.9980  vl_loss=0.4997418 vl_macro_f1=0.8130 vl_micro_f1=0.9392 (286.04 sec.)
Epoch 29 starts
Epoch 29 (sato0): tr_loss=0.0056970 tr_macro_f1=0.9951 tr_micro_f1=0.9981  vl_loss=0.5007991 vl_macro_f1=0.8166 vl_micro_f1=0.9392 (286.05 sec.)