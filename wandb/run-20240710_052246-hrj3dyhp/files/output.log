/home/zhihao/jupyterprojects/jupyter/lib/python3.8/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.
Epoch 1 starts.
step: 0, loss: 5.051030158996582
step: 10, loss: 3.698190450668335
step: 20, loss: 2.874997138977051
step: 30, loss: 3.3739490509033203
step: 40, loss: 1.392288327217102
step: 50, loss: 0.9295406341552734
step: 60, loss: 0.8954694271087646
step: 70, loss: 0.43331828713417053
step: 80, loss: 1.0311479568481445
step: 90, loss: 0.43013739585876465
step: 100, loss: 0.5685591101646423
step: 110, loss: 0.49006298184394836
step: 120, loss: 0.5357679128646851
step: 130, loss: 0.1469159871339798
step: 140, loss: 0.16804952919483185
step: 150, loss: 0.3135055601596832
step: 160, loss: 0.3063255846500397
step: 170, loss: 0.19506068527698517
step: 180, loss: 0.5362381935119629
step: 190, loss: 0.2595285177230835
step: 200, loss: 0.17969317734241486
step: 210, loss: 0.2356913536787033
step: 220, loss: 0.38146504759788513
step: 230, loss: 0.09546620398759842
step: 240, loss: 0.07264307141304016
step: 250, loss: 0.5278574228286743
step: 260, loss: 0.3512592017650604
step: 270, loss: 0.1711900234222412
step: 280, loss: 0.532806932926178
step: 290, loss: 0.14800654351711273
step: 300, loss: 0.06556867808103561
step: 310, loss: 0.1668449491262436