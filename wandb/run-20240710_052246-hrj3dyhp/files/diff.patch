diff --git a/run_supcl_train.py b/run_supcl_train.py
index 1ffc274..605c20e 100644
--- a/run_supcl_train.py
+++ b/run_supcl_train.py
@@ -4,13 +4,12 @@ pretrain_data = 'wikitables'
 ml = 256
 bs = 32
 # cuda_devices = '0,1,2,3'
-cuda_devices = '0'
+cuda_devices = '2'
 
 '''unsupervised'''
-# mode = 'simclr'
 task = 'None'
 '''supervised with header'''
-mode = 'supcon' 
+mode = "simclr" # 'supcon' 
 task = 'header'
 
 ao = 'sample_row4,sample_row4'
@@ -20,20 +19,20 @@ lm = 'bert'
 
 gpus = ','.join([str(i) for i in range(len(cuda_devices.split(',')))])
 n_epochs = 10
+save_model = 10
 size = 100000
 cnt = 0
 run_id = 0
 temp = 0.05
 # temp = 0.07
 
-data_path = "/efs/task_datasets/TURL/"
+data_path = "/data/zhihao/TU/TURL/"
 
 if len(cuda_devices.split(',')) > 1:
     cmd = """accelerate launch --config_file accelerate_config.yaml supcl_train.py --fp16 \
         --data_path %s \
         --pretrain_data %s \
         --mode %s \
-        --task %s \
         --batch_size %s \
         --lr 5e-5 \
         --temperature %s \
@@ -41,16 +40,15 @@ if len(cuda_devices.split(',')) > 1:
         --n_epochs %d \
         --max_len %d \
         --size %d \
-        --save_model \
+        --save_model %d \
         --augment_op %s \
         --sample_meth %s \
-        --run_id %d""" % (data_path, pretrain_data, mode, task, bs, temp, lm, n_epochs, ml, size, ao, sm, run_id)
+        --run_id %d""" % (data_path, pretrain_data, mode, bs, temp, lm, n_epochs, ml, size, save_model, ao, sm, run_id)
 else:
     cmd = """python3 supcl_train.py --fp16 \
         --data_path %s \
         --pretrain_data %s \
         --mode %s \
-        --task %s \
         --batch_size %s \
         --lr 5e-5 \
         --temperature %s \
@@ -58,10 +56,10 @@ else:
         --n_epochs %d \
         --max_len %d \
         --size %d \
-        --save_model \
+        --save_model %d \
         --augment_op %s \
         --sample_meth %s \
-        --run_id %d""" % (data_path, pretrain_data, mode, task, bs, temp, lm, n_epochs, ml, size, ao, sm, run_id)
+        --run_id %d""" % (data_path, pretrain_data, mode, bs, temp, lm, n_epochs, ml, size, save_model, ao, sm, run_id)
 print(cmd)
 # os.system('CUDA_VISIBLE_DEVICES={} {}'.format(
 os.system('CUDA_VISIBLE_DEVICES={} {} &> {} &'.format(
diff --git a/supcl_train.py b/supcl_train.py
index d6d6972..8dcac93 100644
--- a/supcl_train.py
+++ b/supcl_train.py
@@ -20,6 +20,7 @@ from accelerate import DistributedDataParallelKwargs
 
 from watchog.dataset import TableDataset, SupCLTableDataset
 from watchog.model import SupCLforTable, UnsupCLforTable, SupclLoss
+import wandb
 
 def train_step(train_iter, model, optimizer, scheduler, accelerator, criterion, hp):
     """Perform a single training step
@@ -95,7 +96,11 @@ def train_step(train_iter, model, optimizer, scheduler, accelerator, criterion,
 
 
 def train(accelerator, trainset, hp, validset=None):
-
+    wandb.init(config=hp,
+        project="TableUnderstanding",
+        name=f"{hp.model}_Pretrain_DS@{hp.pretrain_data}_mode@{hp.mode}",
+        group="TU",
+        )
     # initialize model, optimizer, and LR scheduler
     device = accelerator.device
     if hp.mode in ['simclr']:
@@ -134,7 +139,7 @@ def train(accelerator, trainset, hp, validset=None):
         directory = os.path.join(hp.logdir, hp.pretrain_data, hp.mode)
         if not os.path.exists(directory):
             os.makedirs(directory)
-
+    time_epochs = []
     for epoch in range(1, hp.n_epochs+1):
         # train
         accelerator.print("Epoch {} starts.".format(epoch))
@@ -158,17 +163,30 @@ def train(accelerator, trainset, hp, validset=None):
             accelerator.save(ckpt, ckpt_path)
 
         end_time = time.time()
+        time_epoch = end_time - start_time
+        time_epochs.append(time_epoch)
         accelerator.print("Epoch {} training ends, took {} secs.".format(epoch, end_time - start_time))
         accelerator.print("   Training loss=%f"  %train_loss)
         
+        wandb.log({
+                f"train/loss": train_loss,
+                f"train/time": time_epoch,
+            }, step=epoch, commit=True)
+    avg_train_time = sum(time_epochs) / len(time_epochs)
+    wandb.log({
+        f"train/avg_time": avg_train_time,
+        "ckpt_path": ckpt_path,
+    }, commit=True)
+    wandb.finish()
     
 if __name__ == '__main__':
     parser = argparse.ArgumentParser()
+    parser.add_argument("--model", type=str, default="simclr") # simclr for original CL, supcon for CL using metadata
     parser.add_argument("--pretrain_data", type=str, default="wikitables") # dataset for pretraining
     parser.add_argument("--pretrained_model_path", type=str, default="") # pretrained checkpoint 
     parser.add_argument("--data_path",type=str, default="./data/doduo")
     parser.add_argument("--mode", type=str, default="simclr") # simclr for original CL, supcon for CL using metadata
-    parser.add_argument("--logdir", type=str, default="results/") # directory to store model checkpoints
+    parser.add_argument("--logdir", type=str, default="/data/zhihao/TU/Watchog/model/") # directory to store model checkpoints
     parser.add_argument("--run_id", type=int, default=0)
     parser.add_argument("--batch_size", type=int, default=32)
     parser.add_argument("--max_len", type=int, default=128)
@@ -182,9 +200,9 @@ if __name__ == '__main__':
     parser.add_argument("--sample_meth", type=str, default='head')
     parser.add_argument("--temperature", type=float, default=0.05)
     parser.add_argument("--save_model", type=int, default=5)
-    parser.add_argument("--fp16", dest="fp16", action="store_true")
+    parser.add_argument("--fp16", dest="fp16", default=True, action="store_true")
     parser.add_argument("--gpus", type=str, default="0")
-    
+    parser.add_argument("--single_column", default=False)
     
     hp = parser.parse_args()
     # set seed
