{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from time import time\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from collections import defaultdict\n",
    "\n",
    "# import pytrec_eval\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss, BCEWithLogitsLoss\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig, AutoTokenizer\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from accelerate import Accelerator\n",
    "from copy import deepcopy\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "from watchog.dataset import (\n",
    "    # collate_fn,\n",
    "    TURLColTypeTablewiseDataset,\n",
    "    TURLRelExtTablewiseDataset,\n",
    "    SatoCVTablewiseDataset,\n",
    "    ColPoplTablewiseDataset\n",
    ")\n",
    "\n",
    "from watchog.dataset import TableDataset, SupCLTableDataset, SemtableCVTablewiseDataset, GittablesColwiseDataset, GittablesTablewiseDataset\n",
    "from watchog.model import BertMultiPairPooler, BertForMultiOutputClassification, BertForMultiOutputClassificationColPopl\n",
    "from watchog.model import SupCLforTable, UnsupCLforTable, lm_mp\n",
    "from watchog.utils import load_checkpoint, f1_score_multilabel, collate_fn, get_col_pred, ColPoplEvaluator\n",
    "from watchog.utils import task_num_class_dict\n",
    "from accelerate import DistributedDataParallelKwargs\n",
    "import wandb\n",
    "\n",
    "from argparse import Namespace\n",
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import re\n",
    "import transformers\n",
    "from torch.utils import data\n",
    "from torch.nn.utils import rnn\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from typing import List\n",
    "from functools import reduce\n",
    "import operator\n",
    "\n",
    "from itertools import chain\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args={\"wandb\": false, \"model\": \"Watchog\", \"unlabeled_train_only\": false, \"context_encoding_type\": \"v0\", \"pool_version\": \"v0.2\", \"random_sample\": false, \"comment\": \"debug\", \"shortcut_name\": \"bert-base-uncased\", \"max_length\": 64, \"adaptive_max_length\": false, \"max_num_col\": 8, \"batch_size\": 16, \"epoch\": 1, \"random_seed\": 4649, \"train_n_seed_cols\": -1, \"num_classes\": 101, \"multi_gpu\": false, \"fp16\": false, \"warmup\": 0.0, \"lr\": 5e-05, \"task\": \"gt-semtab22-dbpedia-all0\", \"colpair\": false, \"metadata\": false, \"from_scratch\": false, \"cl_tag\": \"wikitables/simclr/bert_100000_10_32_256_5e-05_sample_row4,sample_row4_tfidf_entity_column_0.05_0_last.pt\", \"dropout_prob\": 0.5, \"eval_test\": true, \"small_tag\": \"semi1\", \"data_path\": \"/data/zhihao/TU/\", \"pretrained_ckpt_path\": \"/data/zhihao/TU/Watchog/model/\"}\n",
      "gt-semtab22-dbpedia-all0/wikitables-simclr-bert_100000_10_32_256_5e-05_sample_row4,sample_row4_tfidf_entity_column_0.05_0_last.pt_bert-base-uncased-poolsemi1-max_colsv0.2-rand8-bsFalse-ml16-ne64-do10.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3703738/3314320431.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(ckpt_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(augment_op='sample_row4,sample_row4', batch_size=32, data_path='/data/zhihao/TU/TURL/', fp16=True, gpus='0', lm='bert', logdir='/data/zhihao/TU/Watchog/model/', lr=5e-05, max_len=256, mode='simclr', model='Watchog', n_epochs=10, pretrain_data='wikitables', pretrained_model_path='', projector=768, run_id=0, sample_meth='tfidf_entity', save_model=10, single_column=False, size=100000, table_order='column', temperature=0.05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n"
     ]
    }
   ],
   "source": [
    "    device = torch.device(2)\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--wandb\", type=bool, default=False)\n",
    "    parser.add_argument(\"--model\", type=str, default=\"Watchog\")\n",
    "    parser.add_argument(\"--unlabeled_train_only\", type=bool, default=False)\n",
    "    parser.add_argument(\"--context_encoding_type\", type=str, default=\"v0\")\n",
    "    parser.add_argument(\"--pool_version\", type=str, default=\"v0.2\")\n",
    "    parser.add_argument(\"--random_sample\", type=bool, default=False)\n",
    "    parser.add_argument(\"--comment\", type=str, default=\"debug\", help=\"to distinguish the runs\")\n",
    "    parser.add_argument(\n",
    "        \"--shortcut_name\",\n",
    "        default=\"bert-base-uncased\",\n",
    "        type=str,\n",
    "        help=\"Huggingface model shortcut name \",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_length\",\n",
    "        default=64,\n",
    "        type=int,\n",
    "        help=\n",
    "        \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "        \"than this will be truncated, sequences shorter will be padded.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--adaptive_max_length\",\n",
    "        default=False,\n",
    "        type=bool,\n",
    "    )    \n",
    "    parser.add_argument(\n",
    "        \"--max_num_col\",\n",
    "        default=8,\n",
    "        type=int,\n",
    "    )   \n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--batch_size\",\n",
    "        default=16,\n",
    "        type=int,\n",
    "        help=\"Batch size\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--epoch\",\n",
    "        default=1,\n",
    "        type=int,\n",
    "        help=\"Number of epochs for training\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--random_seed\",\n",
    "        default=4649,\n",
    "        type=int,\n",
    "        help=\"Random seed\",\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--train_n_seed_cols\",\n",
    "        default=-1,\n",
    "        type=int,\n",
    "        help=\"number of seeding columns in training\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--num_classes\",\n",
    "        default=78,\n",
    "        type=int,\n",
    "        help=\"Number of classes\",\n",
    "    )\n",
    "    parser.add_argument(\"--multi_gpu\",\n",
    "                        action=\"store_true\",\n",
    "                        default=False,\n",
    "                        help=\"Use multiple GPU\")\n",
    "    parser.add_argument(\"--fp16\",\n",
    "                        action=\"store_true\",\n",
    "                        default=False,\n",
    "                        help=\"Use FP16\")\n",
    "    parser.add_argument(\"--warmup\",\n",
    "                        type=float,\n",
    "                        default=0.,\n",
    "                        help=\"Warmup ratio\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=5e-5, help=\"Learning rate\")\n",
    "    parser.add_argument(\"--task\",\n",
    "                        type=str,\n",
    "                        default='gt-semtab22-dbpedia-all0',\n",
    "                        choices=[\n",
    "                            \"sato0\", \"sato1\", \"sato2\", \"sato3\", \"sato4\",\n",
    "                            \"msato0\", \"msato1\", \"msato2\", \"msato3\", \"msato4\",\n",
    "                            \"gt-dbpedia0\", \"gt-dbpedia1\", \"gt-dbpedia2\", \"gt-dbpedia3\", \"gt-dbpedia4\",\n",
    "                            \"gt-dbpedia-all0\", \"gt-dbpedia-all1\", \"gt-dbpedia-all2\", \"gt-dbpedia-all3\", \"gt-dbpedia-all4\",\n",
    "                            \"gt-schema-all0\", \"gt-schema-all1\", \"gt-schema-all2\", \"gt-schema-all3\", \"gt-schema-all4\",\n",
    "                            \"gt-semtab22-dbpedia\", \"gt-semtab22-dbpedia0\", \"gt-semtab22-dbpedia1\", \"gt-semtab22-dbpedia2\", \"gt-semtab22-dbpedia3\", \"gt-semtab22-dbpedia4\",\n",
    "                            \"gt-semtab22-dbpedia-all\", \"gt-semtab22-dbpedia-all0\", \"gt-semtab22-dbpedia-all1\", \"gt-semtab22-dbpedia-all2\", \"gt-semtab22-dbpedia-all3\", \"gt-semtab22-dbpedia-all4\",\n",
    "                            \"gt-semtab22-schema-class-all\", \"gt-semtab22-schema-property-all\",\n",
    "                            \"turl\", \"turl-re\", \"col-popl-1\", \"col-popl-2\", \"col-popl-3\", \"row-popl\",\n",
    "                            \"col-popl-turl-0\", \"col-popl-turl-1\", \"col-popl-turl-2\",\n",
    "                            \"col-popl-turl-mdonly-0\", \"col-popl-turl-mdonly-1\", \"col-popl-turl-mdonly-2\"\n",
    "                        ],\n",
    "                        help=\"Task names}\")\n",
    "    parser.add_argument(\"--colpair\",\n",
    "                        action=\"store_true\",\n",
    "                        help=\"Use column pair embedding\")\n",
    "    parser.add_argument(\"--metadata\",\n",
    "                        action=\"store_true\",\n",
    "                        help=\"Use column header metadata\")\n",
    "    parser.add_argument(\"--from_scratch\",\n",
    "                        action=\"store_true\",\n",
    "                        help=\"Training from scratch\")\n",
    "    parser.add_argument(\"--cl_tag\",\n",
    "                        type=str,\n",
    "                        default=\"wikitables/simclr/bert_100000_10_32_256_5e-05_sample_row4,sample_row4_tfidf_entity_column_0.05_0_last.pt\",\n",
    "                        help=\"path to the pre-trained file\")\n",
    "    parser.add_argument(\"--dropout_prob\",\n",
    "                        type=float,\n",
    "                        default=0.5)\n",
    "    parser.add_argument(\"--eval_test\",\n",
    "                        action=\"store_true\",\n",
    "                        help=\"evaluate on testset and do not save the model file\")\n",
    "    parser.add_argument(\"--small_tag\",\n",
    "                        type=str,\n",
    "                        default=\"semi1\",\n",
    "                        help=\"e.g., by_table_t5_v1\")\n",
    "    parser.add_argument(\"--data_path\",\n",
    "                        type=str,\n",
    "                        default=\"/data/zhihao/TU/\")\n",
    "    parser.add_argument(\"--pretrained_ckpt_path\",\n",
    "                        type=str,\n",
    "                        default=\"/data/zhihao/TU/Watchog/model/\")    \n",
    "\n",
    "    args = parser.parse_args([])\n",
    "    task = args.task\n",
    "    if args.small_tag != \"\":\n",
    "        args.eval_test = True\n",
    "    \n",
    "    args.num_classes = task_num_class_dict[task]\n",
    "    if args.colpair:\n",
    "        assert \"turl-re\" == task, \"colpair can be only used for Relation Extraction\"\n",
    "    if args.metadata:\n",
    "        assert \"turl-re\" == task or \"turl\" == task, \"metadata can be only used for TURL datasets\"\n",
    "    if \"col-popl\":\n",
    "        # metrics = {\n",
    "        #     \"accuracy\": CategoricalAccuracy(tie_break=True),\n",
    "        # }\n",
    "        if args.train_n_seed_cols != -1:\n",
    "            if \"col-popl\" in task:\n",
    "                assert args.train_n_seed_cols == int(task[-1]),  \"# of seed columns must match\"\n",
    "\n",
    "    print(\"args={}\".format(json.dumps(vars(args))))\n",
    "\n",
    "    max_length = args.max_length\n",
    "    batch_size = args.batch_size\n",
    "    num_train_epochs = args.epoch\n",
    "\n",
    "    shortcut_name = args.shortcut_name\n",
    "\n",
    "    if args.colpair and args.metadata:\n",
    "        taskname = \"{}-colpair-metadata\".format(task)\n",
    "    elif args.colpair:\n",
    "        taskname = \"{}-colpair\".format(task)\n",
    "    elif args.metadata:\n",
    "        taskname = \"{}-metadata\".format(task)\n",
    "    elif args.train_n_seed_cols == -1 and 'popl' in task:\n",
    "        taskname = \"{}-mix\".format(task)\n",
    "    else:\n",
    "        taskname = \"\".join(task)\n",
    "    cv = int(task[-1])\n",
    "\n",
    "    if args.from_scratch:\n",
    "        if \"gt\" in task:\n",
    "            tag_name = \"{}/{}-{}-{}-pool{}-max_cols{}-rand{}-bs{}-ml{}-ne{}-do{}{}\".format(\n",
    "                taskname,  \"{}-fromscratch\".format(shortcut_name), args.small_tag, args.comment, args.pool_version, args.max_num_col, args.random_sample,\n",
    "                batch_size, max_length, num_train_epochs, args.dropout_prob, \n",
    "                '-rs{}'.format(args.random_seed) if args.random_seed != 4649 else '')\n",
    "        else:\n",
    "            tag_name = \"{}/{}-{}-{}-bs{}-ml{}-ne{}-do{}{}\".format(\n",
    "                taskname,  \"{}-fromscratch\".format(shortcut_name), args.small_tag, args.comment, \n",
    "                batch_size, max_length, num_train_epochs, args.dropout_prob, \n",
    "                '-rs{}'.format(args.random_seed) if args.random_seed != 4649 else '')\n",
    "        \n",
    "    else:\n",
    "        if \"gt\" in task:\n",
    "            tag_name = \"{}/{}_{}-pool{}-max_cols{}-rand{}-bs{}-ml{}-ne{}-do{}{}\".format(\n",
    "                taskname, args.cl_tag.replace('/', '-'),  shortcut_name, args.small_tag, args.pool_version, args.max_num_col, args.random_sample,\n",
    "                batch_size, max_length, num_train_epochs, args.dropout_prob,\n",
    "                '-rs{}'.format(args.random_seed) if args.random_seed != 4649 else '')\n",
    "        else:\n",
    "            tag_name = \"{}/{}_{}-{}-bs{}-ml{}-ne{}-do{}{}\".format(\n",
    "                taskname, args.cl_tag.replace('/', '-'),  shortcut_name, args.small_tag,\n",
    "                batch_size, max_length, num_train_epochs, args.dropout_prob,\n",
    "                '-rs{}'.format(args.random_seed) if args.random_seed != 4649 else '')\n",
    "\n",
    "    # if args.eval_test:\n",
    "    #     if args.small_tag != '':\n",
    "    #         tag_name = tag_name.replace('outputs', 'small_outputs')\n",
    "    #         tag_name += '-' + args.small_tag\n",
    "    print(tag_name)\n",
    "    file_path = os.path.join(args.data_path, \"Watchog\", \"outputs\", tag_name)\n",
    "\n",
    "    dirpath = os.path.dirname(file_path)\n",
    "    if not os.path.exists(dirpath):\n",
    "        print(\"{} not exists. Created\".format(dirpath))\n",
    "        os.makedirs(dirpath)\n",
    "    \n",
    "    if args.fp16:\n",
    "        torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = True\n",
    "        \n",
    "      \n",
    "        \n",
    "    # accelerator = Accelerator(mixed_precision=\"no\" if not args.fp16 else \"fp16\")   \n",
    "    ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)\n",
    "    accelerator = Accelerator(mixed_precision=\"no\" if not args.fp16 else \"fp16\", kwargs_handlers=[ddp_kwargs])\n",
    "\n",
    "    device = torch.device(2)\n",
    "    ckpt_path = os.path.join(args.pretrained_ckpt_path, args.cl_tag)\n",
    "    # ckpt_path = '/efs/checkpoints/{}.pt'.format(args.cl_tag)\n",
    "    ckpt = torch.load(ckpt_path, map_location=device)\n",
    "    ckpt_hp = ckpt['hp']\n",
    "    print(ckpt_hp)\n",
    " \n",
    "    setattr(ckpt_hp, 'batch_size', args.batch_size)\n",
    "    setattr(ckpt_hp, 'hidden_dropout_prob', args.dropout_prob)\n",
    "    setattr(ckpt_hp, 'shortcut_name', args.shortcut_name)\n",
    "    setattr(ckpt_hp, 'num_labels', args.num_classes)\n",
    "    \n",
    "    \n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained(shortcut_name)\n",
    "    padder = collate_fn(tokenizer.pad_token_id)\n",
    "    if task == \"turl-re\" and args.colpair:\n",
    "        model = BertForMultiOutputClassification(ckpt_hp, device=device, lm=ckpt['hp'].lm, col_pair='Pair')\n",
    "    elif \"col-popl\" in task:\n",
    "        model = BertForMultiOutputClassificationColPopl(ckpt_hp, device=device, lm=ckpt['hp'].lm, n_seed_cols=int(task[i][-1]), cls_for_md=\"md\" in task)\n",
    "    else:\n",
    "        model = BertForMultiOutputClassification(ckpt_hp, device=device, lm=ckpt['hp'].lm, version=\"v0\", use_attention_mask=True)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3703738/307592194.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  best_state_dict = torch.load(\"/data/zhihao/TU/Watchog/outputs/gt-semtab22-dbpedia-all0/bert-base-uncased-fromscratch-semi1-Repeat@5-AttnMask-UnlabelValid-max-unlabeled@8-poolv0-unlabeled8-randFalse-bs16-ml128-ne50-do0.1_best_f1_macro.pt\", map_location=device)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best_state_dict = torch.load(\"/data/zhihao/TU/Watchog/outputs/gt-semtab22-dbpedia-all0/bert-base-uncased-fromscratch-semi1-AttnMask-max-unlabeled@8-poolv0-unlabeled8-randFalse-bs16-ml128-ne50-do0.1_best_f1_micro.pt\", map_location=device)\n",
    "best_state_dict = torch.load(\"/data/zhihao/TU/Watchog/outputs/gt-semtab22-dbpedia-all0/bert-base-uncased-fromscratch-semi1-Repeat@5-AttnMask-UnlabelValid-max-unlabeled@8-poolv0-unlabeled8-randFalse-bs16-ml128-ne50-do0.1_best_f1_macro.pt\", map_location=device)\n",
    "model.load_state_dict(best_state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GittablesTablewiseIterateDataset(data.Dataset):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            cv: int,\n",
    "            split: str,\n",
    "            src: str,  # train or test\n",
    "            tokenizer: AutoTokenizer,\n",
    "            max_length: int = 256,\n",
    "            gt_only: bool = False,\n",
    "            device: torch.device = None,\n",
    "            base_dirpath: str = \"/data/zhihao/TU/GitTables/semtab_gittables/2022\",\n",
    "            base_tag: str = '', # blank, comma\n",
    "            small_tag: str = \"\",\n",
    "            train_ratio: float = 1.0,\n",
    "            max_unlabeled=8,\n",
    "            random_sample=False, # TODO\n",
    "            train_only=False): # TODO\n",
    "        if device is None:\n",
    "            device = torch.device('cpu')\n",
    "        basename = small_tag+ \"_cv_{}.csv\"\n",
    "    \n",
    "        if split in [\"train\", \"valid\"]:\n",
    "            df_list = []\n",
    "            for i in range(5):\n",
    "                if i == cv:\n",
    "                    continue\n",
    "                filepath = os.path.join(base_dirpath, basename.format(i))\n",
    "                df_list.append(pd.read_csv(filepath))\n",
    "                print(split, i)\n",
    "            df = pd.concat(df_list, axis=0)\n",
    "        else:\n",
    "            # test\n",
    "            filepath = os.path.join(base_dirpath, basename.format(cv))\n",
    "            df = pd.read_csv(filepath)\n",
    "            print(split)\n",
    "\n",
    "\n",
    "        if gt_only:\n",
    "            df = df[df[\"class_id\"] > -1]\n",
    "        if train_only and split != \"train\":\n",
    "            df = df[df[\"class_id\"] > -1]\n",
    "\n",
    "        \n",
    "        data_list = []\n",
    "        \n",
    "        df['class_id'] = df['class_id'].astype(int)\n",
    "        df.drop(df[(df['data'].isna()) & (df['class_id'] == -1)].index, inplace=True)\n",
    "        df['col_idx'] = df['col_idx'].astype(int)\n",
    "        df['data'] = df['data'].astype(str)\n",
    "        \n",
    "        num_tables = len(df.groupby(\"table_id\"))\n",
    "        valid_index = int(num_tables * 0.8)\n",
    "        num_train = int(train_ratio * num_tables * 0.8)        \n",
    "        \n",
    "        # df.drop(df[(df['data'] == '') & (df['class_id'] == -1)].index, inplace=True)\n",
    "        total_num_cols = 0\n",
    "        for i, (index, group_df) in enumerate(df.groupby(\"table_id\")):\n",
    "            if (split == \"train\") and ((i >= num_train) or (i >= valid_index)):\n",
    "                break\n",
    "            if split == \"valid\" and i < valid_index:\n",
    "                continue\n",
    "            #     break\n",
    "            labeled_columns = group_df[group_df['class_id'] > -1]\n",
    "            unlabeled_columns = group_df[group_df['class_id'] == -1]\n",
    "            num_unlabeled = min(max(max_unlabeled-len(labeled_columns), 0), len(unlabeled_columns)) if max_unlabeled > 0 else len(unlabeled_columns)\n",
    "            unlabeled_columns = unlabeled_columns.sample(num_unlabeled) if random_sample else unlabeled_columns[0:num_unlabeled]\n",
    "            # group_df = pd.concat([group_df[group_df['class_id'] > -1], unlabeled_columns.sample(min(10-len(labeled_columns), len(unlabeled_columns)))])\n",
    "            # group_df = pd.concat([group_df[group_df['class_id'] > -1], unlabeled_columns[0:min(max(10-len(labeled_columns), 0), len(unlabeled_columns))]])\n",
    "            group_df = pd.concat([group_df[group_df['class_id'] > -1], unlabeled_columns]) # TODO\n",
    "            group_df.sort_values(by=['col_idx'], inplace=True)\n",
    "\n",
    "            if max_length <= 128:\n",
    "                cur_maxlen = min(max_length, 512 // len(list(group_df[\"class_id\"].values)) - 1)\n",
    "            else:\n",
    "                cur_maxlen = max(1, max_length // len(list(group_df[\"class_id\"].values)) - 1)\n",
    "                \n",
    "            token_ids_list = group_df[\"data\"].apply(lambda x: tokenizer.encode(\n",
    "                tokenizer.cls_token + \" \" + x, add_special_tokens=False, max_length=cur_maxlen, truncation=True)).tolist(\n",
    "                )\n",
    "            token_ids = torch.LongTensor(reduce(operator.add,\n",
    "                                                token_ids_list)).to(device)\n",
    "            for col_i in range(len(token_ids_list)):\n",
    "                if group_df[\"class_id\"].values[col_i] == -1:\n",
    "                    continue\n",
    "                target_col_mask = []\n",
    "                cls_index_value = 0\n",
    "                context_id = 1\n",
    "                for col_j in range(len(token_ids_list)):\n",
    "                    if col_j == col_i:\n",
    "                        target_col_mask += [0] * len(token_ids_list[col_j])\n",
    "                    else:\n",
    "                        target_col_mask += [context_id] * len(token_ids_list[col_j])\n",
    "                        context_id += 1\n",
    "                    if col_j < col_i:\n",
    "                        cls_index_value += len(token_ids_list[col_j])\n",
    "                cls_index_list = [cls_index_value] \n",
    "                for cls_index in cls_index_list:\n",
    "                    assert token_ids[\n",
    "                        cls_index] == tokenizer.cls_token_id, \"cls_indexes validation\"\n",
    "                cls_indexes = torch.LongTensor(cls_index_list).to(device)\n",
    "                class_ids = torch.LongTensor(\n",
    "                    [group_df[\"class_id\"].values[col_i]]).to(device)\n",
    "                target_col_mask = torch.LongTensor(target_col_mask).to(device)\n",
    "                data_list.append(\n",
    "                    [index,\n",
    "                    len(group_df), token_ids, class_ids, cls_indexes, target_col_mask])                \n",
    "        print(split, len(data_list))\n",
    "        self.table_df = pd.DataFrame(data_list,\n",
    "                                     columns=[\n",
    "                                         \"table_id\", \"num_col\", \"data_tensor\",\n",
    "                                         \"label_tensor\", \"cls_indexes\", \"target_col_mask\"\n",
    "                                     ])\n",
    "        \"\"\"\n",
    "        # NOTE: msato contains a small portion of single-col tables. keep it to be consistent.  \n",
    "        if multicol_only:\n",
    "            # Check\n",
    "            num_all_tables = len(self.table_df)\n",
    "            self.table_df = self.table_df[self.table_df[\"num_col\"] > 1]\n",
    "            assert len(self.table_df) == num_all_tables\n",
    "        \"\"\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.table_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"data\": self.table_df.iloc[idx][\"data_tensor\"],\n",
    "            \"label\": self.table_df.iloc[idx][\"label_tensor\"],\n",
    "            \"cls_indexes\": self.table_df.iloc[idx][\"cls_indexes\"],\n",
    "            \"target_col_mask\": self.table_df.iloc[idx][\"target_col_mask\"],\n",
    "        }\n",
    "        #\"idx\": torch.LongTensor([idx])}\n",
    "        #\"cls_indexes\": self.table_df.iloc[idx][\"cls_indexes\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(pad_token_id, data_only=True):\n",
    "    '''padder for input batch'''\n",
    "\n",
    "    def padder(samples):    \n",
    "        data = torch.nn.utils.rnn.pad_sequence(\n",
    "            [sample[\"data\"] for sample in samples], padding_value=pad_token_id)\n",
    "        if not data_only:\n",
    "            label = torch.nn.utils.rnn.pad_sequence(\n",
    "                [sample[\"label\"] for sample in samples], padding_value=-1)\n",
    "        else:\n",
    "            label = torch.cat([sample[\"label\"] for sample in samples])\n",
    "        batch = {\"data\": data, \"label\": label}\n",
    "        if \"idx\" in samples[0]:\n",
    "            batch[\"idx\"] = [sample[\"idx\"] for sample in samples]\n",
    "        if \"cls_indexes\" in samples[0]:\n",
    "            cls_indexes = torch.nn.utils.rnn.pad_sequence(\n",
    "                [sample[\"cls_indexes\"] for sample in samples], padding_value=0)\n",
    "            batch[\"cls_indexes\"] = cls_indexes\n",
    "        if \"target_col_mask\" in samples[0]:\n",
    "            target_col_mask = torch.nn.utils.rnn.pad_sequence(\n",
    "                [sample[\"target_col_mask\"] for sample in samples], padding_value=-1)\n",
    "            batch[\"target_col_mask\"] = target_col_mask\n",
    "        if \"table_embedding\" in samples[0]:\n",
    "            table_embeddings = [sample[\"table_embedding\"] for sample in samples]\n",
    "            batch[\"table_embedding\"] = torch.stack(table_embeddings, dim=0)\n",
    "        return batch\n",
    "        \n",
    "    return padder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "test 1085\n"
     ]
    }
   ],
   "source": [
    "src = None\n",
    "test_dataset_iter = GittablesTablewiseIterateDataset(cv=cv,\n",
    "                            split=\"test\", src=src,\n",
    "                            tokenizer=tokenizer,\n",
    "                            max_length=max_length,\n",
    "                            gt_only='all' not in task,\n",
    "                            device=device,\n",
    "                            base_dirpath=os.path.join(args.data_path, \"GitTables/semtab_gittables/2022\"),\n",
    "                            small_tag=\"semi1\")\n",
    "padder = collate_fn(tokenizer.pad_token_id)\n",
    "test_dataloader_iter = DataLoader(test_dataset_iter,\n",
    "                                batch_size=1,\n",
    "                            #   collate_fn=collate_fn)\n",
    "                            collate_fn=padder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 1\n",
      "train 2\n",
      "train 3\n",
      "train 4\n",
      "train 3463\n"
     ]
    }
   ],
   "source": [
    "train_dataset_iter = GittablesTablewiseIterateDataset(cv=cv,\n",
    "                            split=\"train\", src=src,\n",
    "                            tokenizer=tokenizer,\n",
    "                            max_length=max_length,\n",
    "                            gt_only='all' not in task,\n",
    "                            device=device,\n",
    "                            base_dirpath=os.path.join(args.data_path, \"GitTables/semtab_gittables/2022\"),\n",
    "                            small_tag=\"semi1\")\n",
    "padder = collate_fn(tokenizer.pad_token_id)\n",
    "train_dataloader_iter = DataLoader(train_dataset_iter,\n",
    "                                batch_size=1,\n",
    "                            #   collate_fn=collate_fn)\n",
    "                            collate_fn=padder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ts_micro_f1=0.5355, ts_macro_f1=0.2745\n",
      "ts_micro_f1=0.5351, ts_macro_f1=0.2745\n",
      "ts_micro_f1=1.0000, ts_macro_f1=1.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "ft_embs_test = []\n",
    "labels_test = []\n",
    "logits_test = []\n",
    "num_cols = []\n",
    "for batch_idx, batch in enumerate(test_dataloader_iter):\n",
    "    cls_indexes = torch.LongTensor([[0, batch[\"cls_indexes\"].cpu().item()]]).to(device)\n",
    "    target_col_mask = batch[\"target_col_mask\"].T\n",
    "    logits, embs = model(batch[\"data\"].T, cls_indexes=cls_indexes, get_enc=True)\n",
    "    labels_test.append(batch[\"label\"].cpu())\n",
    "    logits_test.append(logits.detach().cpu())\n",
    "    num_cols.append(batch[\"target_col_mask\"].max().item())\n",
    "    ft_embs_test.append(embs.detach().cpu().reshape(1, -1))\n",
    "labels_test = torch.cat(labels_test, dim=0)\n",
    "logits_test = torch.stack(logits_test, dim=0)\n",
    "preds_test = torch.argmax(logits_test, dim=1)\n",
    "num_cols = torch.tensor(num_cols)\n",
    "ft_embs_test = torch.cat(ft_embs_test, dim=0)\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "mask = num_cols > 0\n",
    "ts_pred_list = logits_test.argmax(\n",
    "                            1).cpu().detach().numpy().tolist()\n",
    "ts_micro_f1 = f1_score(labels_test.reshape(-1).numpy().tolist(),\n",
    "                    ts_pred_list,\n",
    "                    average=\"micro\")\n",
    "ts_macro_f1 = f1_score(labels_test.reshape(-1).numpy().tolist(),\n",
    "                    ts_pred_list,\n",
    "                    average=\"macro\")\n",
    "print(\"ts_micro_f1={:.4f}, ts_macro_f1={:.4f}\".format(ts_micro_f1, ts_macro_f1))\n",
    "ts_pred_list = logits_test.argmax(\n",
    "                            1).cpu().detach()[mask].numpy().tolist()\n",
    "ts_micro_f1 = f1_score(labels_test.reshape(-1)[mask].numpy().tolist(),\n",
    "                    ts_pred_list,\n",
    "                    average=\"micro\")\n",
    "ts_macro_f1 = f1_score(labels_test.reshape(-1)[mask].numpy().tolist(),\n",
    "                    ts_pred_list,\n",
    "                    average=\"macro\")\n",
    "print(\"ts_micro_f1={:.4f}, ts_macro_f1={:.4f}\".format(ts_micro_f1, ts_macro_f1))\n",
    "ts_pred_list = logits_test.argmax(\n",
    "                            1).cpu().detach()[~mask].numpy().tolist()\n",
    "ts_micro_f1 = f1_score(labels_test.reshape(-1)[~mask].numpy().tolist(),\n",
    "                    ts_pred_list,\n",
    "                    average=\"micro\")\n",
    "ts_macro_f1 = f1_score(labels_test.reshape(-1)[~mask].numpy().tolist(),\n",
    "                    ts_pred_list,\n",
    "                    average=\"macro\")\n",
    "print(\"ts_micro_f1={:.4f}, ts_macro_f1={:.4f}\".format(ts_micro_f1, ts_macro_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************** max_unlabeled=1 **************************\n",
      "test\n",
      "test 1085\n",
      "ts_micro_f1=0.4876, ts_macro_f1=0.2104\n",
      "ts_micro_f1=0.4818, ts_macro_f1=0.2082\n",
      "ts_micro_f1=0.4941, ts_macro_f1=0.1724\n",
      "************************** max_unlabeled=2 **************************\n",
      "test\n",
      "test 1085\n",
      "ts_micro_f1=0.5014, ts_macro_f1=0.2123\n",
      "ts_micro_f1=0.5009, ts_macro_f1=0.2123\n",
      "ts_micro_f1=1.0000, ts_macro_f1=1.0000\n",
      "************************** max_unlabeled=4 **************************\n",
      "test\n",
      "test 1085\n",
      "ts_micro_f1=0.5235, ts_macro_f1=0.2501\n",
      "ts_micro_f1=0.5231, ts_macro_f1=0.2501\n",
      "ts_micro_f1=1.0000, ts_macro_f1=1.0000\n"
     ]
    }
   ],
   "source": [
    "for max_unlabeled in [1, 2, 4]:\n",
    "    print(f\"************************** max_unlabeled={max_unlabeled} **************************\")\n",
    "    src = None\n",
    "    test_dataset_iter = GittablesTablewiseIterateDataset(cv=cv,\n",
    "                                split=\"test\", src=src,\n",
    "                                tokenizer=tokenizer,\n",
    "                                max_length=max_length,\n",
    "                                gt_only='all' not in task,\n",
    "                                device=device,\n",
    "                                max_unlabeled = max_unlabeled,\n",
    "                                base_dirpath=os.path.join(args.data_path, \"GitTables/semtab_gittables/2022\"),\n",
    "                                small_tag=\"semi1\")\n",
    "    padder = collate_fn(tokenizer.pad_token_id)\n",
    "    test_dataloader_iter = DataLoader(test_dataset_iter,\n",
    "                                    batch_size=1,\n",
    "                                #   collate_fn=collate_fn)\n",
    "                                collate_fn=padder)\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    ft_embs_test = []\n",
    "    labels_test = []\n",
    "    logits_test = []\n",
    "    num_cols = []\n",
    "    for batch_idx, batch in enumerate(test_dataloader_iter):\n",
    "        cls_indexes = torch.LongTensor([[0, batch[\"cls_indexes\"].cpu().item()]]).to(device)\n",
    "        target_col_mask = batch[\"target_col_mask\"].T\n",
    "        logits, embs = model(batch[\"data\"].T, cls_indexes=cls_indexes, get_enc=True)\n",
    "        labels_test.append(batch[\"label\"].cpu())\n",
    "        logits_test.append(logits.detach().cpu())\n",
    "        num_cols.append(batch[\"target_col_mask\"].max().item())\n",
    "        ft_embs_test.append(embs.detach().cpu().reshape(1, -1))\n",
    "    labels_test = torch.cat(labels_test, dim=0)\n",
    "    logits_test = torch.stack(logits_test, dim=0)\n",
    "    preds_test = torch.argmax(logits_test, dim=1)\n",
    "    num_cols = torch.tensor(num_cols)\n",
    "    ft_embs_test = torch.cat(ft_embs_test, dim=0)\n",
    "    from sklearn.metrics import confusion_matrix, f1_score\n",
    "    mask = num_cols > 0\n",
    "    ts_pred_list = logits_test.argmax(\n",
    "                                1).cpu().detach().numpy().tolist()\n",
    "    ts_micro_f1 = f1_score(labels_test.reshape(-1).numpy().tolist(),\n",
    "                        ts_pred_list,\n",
    "                        average=\"micro\")\n",
    "    ts_macro_f1 = f1_score(labels_test.reshape(-1).numpy().tolist(),\n",
    "                        ts_pred_list,\n",
    "                        average=\"macro\")\n",
    "    print(\"ts_micro_f1={:.4f}, ts_macro_f1={:.4f}\".format(ts_micro_f1, ts_macro_f1))\n",
    "    ts_pred_list = logits_test.argmax(\n",
    "                                1).cpu().detach()[mask].numpy().tolist()\n",
    "    ts_micro_f1 = f1_score(labels_test.reshape(-1)[mask].numpy().tolist(),\n",
    "                        ts_pred_list,\n",
    "                        average=\"micro\")\n",
    "    ts_macro_f1 = f1_score(labels_test.reshape(-1)[mask].numpy().tolist(),\n",
    "                        ts_pred_list,\n",
    "                        average=\"macro\")\n",
    "    print(\"ts_micro_f1={:.4f}, ts_macro_f1={:.4f}\".format(ts_micro_f1, ts_macro_f1))\n",
    "    ts_pred_list = logits_test.argmax(\n",
    "                                1).cpu().detach()[~mask].numpy().tolist()\n",
    "    ts_micro_f1 = f1_score(labels_test.reshape(-1)[~mask].numpy().tolist(),\n",
    "                        ts_pred_list,\n",
    "                        average=\"micro\")\n",
    "    ts_macro_f1 = f1_score(labels_test.reshape(-1)[~mask].numpy().tolist(),\n",
    "                        ts_pred_list,\n",
    "                        average=\"macro\")\n",
    "    print(\"ts_micro_f1={:.4f}, ts_macro_f1={:.4f}\".format(ts_micro_f1, ts_macro_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'label', 'cls_indexes'])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 3, 7,  ..., 7, 7, 2])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: ylabel='Count'>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.histplot(num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3695852534562212"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(num_cols<7).sum().item()/len(num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_freq = torch.zeros(args.num_classes)\n",
    "for batch_idx, batch in enumerate(train_dataloader_iter):\n",
    "    class_freq[batch[\"label\"].item()] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def reweight_logits(logits, class_weights):\n",
    "#     # Reweight the logits by multiplying with class weights\n",
    "#     reweighted_logits = logits * torch.sqrt(class_weights)\n",
    "    \n",
    "#     # Apply softmax to the reweighted logits\n",
    "#     reweighted_probs = F.softmax(reweighted_logits, dim=-1)\n",
    "    \n",
    "#     return reweighted_probs\n",
    "def reweight_logits(logits, class_weights):\n",
    "    # Step 1: Apply exp(logits)\n",
    "    exp_logits = torch.exp(logits)\n",
    "    \n",
    "    # Step 2: Multiply by class weights\n",
    "    # reweighted_exp = exp_logits * torch.sqrt(class_weights)\n",
    "    reweighted_exp = exp_logits * class_weights\n",
    "    # Step 3: Normalize to get valid probabilities (like softmax)\n",
    "    reweighted_probs = reweighted_exp / reweighted_exp.sum(dim=-1, keepdim=True)\n",
    "    \n",
    "    return reweighted_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reweight_logits_after_softmax(logits, class_weights):\n",
    "    # Step 1: Apply exp(logits)\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    \n",
    "    # Step 2: Multiply by class weights\n",
    "    # reweighted_exp = exp_logits * torch.sqrt(class_weights)\n",
    "    reweighted_probs = probs * class_weights\n",
    "    # Step 3: Normalize to get valid probabilities (like softmax)\n",
    "    reweighted_probs = reweighted_probs / reweighted_probs.sum(dim=-1, keepdim=True)\n",
    "    \n",
    "    return reweighted_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_test = []\n",
    "num_cols = []\n",
    "for batch_idx, batch in enumerate(test_dataloader_iter):\n",
    "    labels_test.append(batch[\"label\"].cpu().item())\n",
    "    num_cols.append(batch[\"target_col_mask\"].max().item())\n",
    "labels_test = torch.Tensor(labels_test).long()\n",
    "num_cols = torch.tensor(num_cols)\n",
    "labels_test = labels_test[num_cols>0]\n",
    "num_cols = num_cols[num_cols>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(num_cols==0).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************Threshold: 0.9****************************\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# brute force perumate, find uncertain init prediction, weighted msp\n",
    "import itertools\n",
    "\n",
    "def is_sublist(A, B):\n",
    "    it = iter(B)\n",
    "    return all(x in it for x in A)\n",
    "def get_permutation(x):\n",
    "    new = []\n",
    "    x = x.tolist()\n",
    "    if len(x) == 1:\n",
    "        x = x[0]\n",
    "    for k in x:\n",
    "        if k not in new:\n",
    "            new.append(k)\n",
    "    return new\n",
    "    \n",
    "model.load_state_dict(best_state_dict, strict=False)\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "change_log = []\n",
    "score_init = []\n",
    "score_best = [] \n",
    "idx_list = []\n",
    "uncertain_init_mask = []\n",
    "uncertain_target_mask = []\n",
    "correct_init_mask = []\n",
    "correct_target_mask = []\n",
    "correct_permutation_mask = []\n",
    "correct_permutation_confident_mask = []\n",
    "correct_permutation_confident_scores = []\n",
    "correct_msp_mask = []\n",
    "init_target_mask = []\n",
    "ood_score_init_list = []\n",
    "ood_score_target_list = []\n",
    "ood_score_final_list = []\n",
    "round_msp_predict = defaultdict(list)\n",
    "ood_score_round_msp = defaultdict(list)\n",
    "final_msp_round = []\n",
    "correct_permutation_round = []\n",
    "\n",
    "msp_predicts = []\n",
    "max_col_length = 3\n",
    "\n",
    "alpha = 0.25\n",
    "class_weights = (1.0 / class_freq) ** alpha\n",
    "debias_threshold = 1.0\n",
    "# Normalize the weights\n",
    "class_weights /= class_weights.sum()\n",
    "for threshold in [0.9]:\n",
    "    print(f\"*********************Threshold: {threshold}****************************\")\n",
    "    ft_embs_test = []\n",
    "    logits_test = []\n",
    "    log = defaultdict(list)\n",
    "    corrected = 0\n",
    "    total_mistakes = 0\n",
    "\n",
    "    for batch_idx, batch in enumerate(test_dataloader_iter):\n",
    "        target_col_mask = batch[\"target_col_mask\"].T\n",
    "        if 1 in target_col_mask:\n",
    "            idx_list.append(batch_idx)\n",
    "\n",
    "            cls_indexes = torch.LongTensor([[0, batch[\"cls_indexes\"].cpu().item()]]).to(device)\n",
    "            logits = model(batch[\"data\"].T, cls_indexes=cls_indexes,)\n",
    "            logits = reweight_logits(logits.detach().cpu(), class_weights)\n",
    "            num_cols.append(batch[\"target_col_mask\"].max().item())         \n",
    "            ood_score_init = logits.max().item()\n",
    "            ood_score_init_list.append(ood_score_init)\n",
    "            uncertain_init_mask.append(ood_score_init < threshold)\n",
    "            predict_init = logits.argmax().item()\n",
    "            correct_init_mask.append(predict_init == batch[\"label\"].item())\n",
    "            init_permutation_i = get_permutation(target_col_mask)\n",
    "            \n",
    "            correct_permutation = False\n",
    "            correct_permutation_ood_score = 0.0\n",
    "            correct_permutation_confident = False\n",
    "            correct_permutation_round_i = 0\n",
    "            ood_score_max = max_msp = 0\n",
    "            max_msp_debiased = 0\n",
    "            debias_class = []\n",
    "            col_idx_set = target_col_mask.unique().tolist()\n",
    "            assert -1 not in col_idx_set\n",
    "            \n",
    "            for r in range(len(init_permutation_i)-1, 1, -1):\n",
    "                round_msp = 0\n",
    "                for x in itertools.combinations(init_permutation_i, r):\n",
    "                    if 0 not in x:\n",
    "                        continue\n",
    "                    if not is_sublist(x, init_permutation_i):\n",
    "                        continue\n",
    "                    new_batch_data = []\n",
    "                    for col_i in x:\n",
    "                        if col_i == 0:\n",
    "                            if len(new_batch_data) == 0:\n",
    "                                cls_indexes_value = 0\n",
    "                            else:\n",
    "                                cls_indexes_value = sum([len(new_batch_data[i]) for i in range(len(new_batch_data))])\n",
    "                        new_batch_data.append(batch[\"data\"].T[target_col_mask==col_i])\n",
    "                    if 0 not in x:\n",
    "                        cls_indexes_value = 0\n",
    "                    new_batch_data = torch.cat(new_batch_data, dim=-1).reshape(1, -1)\n",
    "                    cls_indexes = torch.tensor([0, cls_indexes_value]).reshape(1, -1).to(device)\n",
    "                    logits_temp = model(new_batch_data, cls_indexes=cls_indexes,)\n",
    "                    logits_temp = reweight_logits(logits_temp.detach().cpu(), class_weights)\n",
    "                    msp_temp = logits_temp.max().item()\n",
    "                    predict_temp = logits_temp.argmax().item()\n",
    "                    # if 0 not in x and msp_temp > debias_threshold and (predict_temp!=predict_target or predict_temp!=predict_init):\n",
    "                    #     debias_class.append(predict_temp)\n",
    "                    #     continue\n",
    "                    # print(x, msp_temp, predict_temp)\n",
    "                    # if msp_temp > max_msp and 0 in x and predict_temp not in debias_class:\n",
    "                    if msp_temp > max_msp and 0 in x:\n",
    "                        max_msp = msp_temp\n",
    "                        msp_predict = predict_temp\n",
    "                    if msp_temp > round_msp:\n",
    "                        round_msp = msp_temp\n",
    "                        round_msp_predict_i= predict_temp\n",
    "                    if predict_temp == batch[\"label\"].item():\n",
    "                        if not correct_permutation:\n",
    "                            correct_permutation_round_i = len(init_permutation_i)-r\n",
    "                        correct_permutation = True\n",
    "                        if msp_temp > correct_permutation_ood_score:\n",
    "                            correct_permutation_ood_score = msp_temp\n",
    "                round_msp_predict[len(init_permutation_i)-r].append(round_msp_predict_i)\n",
    "                ood_score_round_msp[len(init_permutation_i)-r].append(round_msp)\n",
    "            correct_permutation_round.append(correct_permutation_round_i)\n",
    "            final_msp_round.append(len(init_permutation_i)-r)\n",
    "            msp_predicts.append(msp_predict)\n",
    "            ood_score_final_list.append(max_msp)\n",
    "            correct_permutation_mask.append(correct_permutation)\n",
    "            correct_permutation_confident_scores.append(correct_permutation_ood_score)\n",
    "            correct_msp_mask.append(msp_predict == batch[\"label\"].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round_msp_predict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7232472324723247\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: ylabel='Count'>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAD4CAYAAAAD6PrjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAAsTAAALEwEAmpwYAAAQz0lEQVR4nO3df6yeZX3H8fdHKurwR0HOmqYtK4uNzrgo5MhQjFEaDaCzbHHo4qQhuJoMjcZFh+4PY7I/NFn8tRhmQ9WyocAQQnUEZQV1xoGeKoJSjB2R9FSgxx/grziDfvfHuXp5KIf2UM79PG3P+5U8ea77uq/7fr73P/30vu4fJ1WFJEkATxh3AZKkw4ehIEnqDAVJUmcoSJI6Q0GS1C0bdwGPx4knnlhr164ddxmSdETZsWPHj6pqYr51R3QorF27lqmpqXGXIUlHlCT3PNo6p48kSZ2hIEnqDAVJUmcoSJI6Q0GS1BkKkqTOUJAkdYaCJKkzFCRJ3ZINhVVrTiLJyD6r1pw07kOWpIM6ol9z8Xj8cHo3r/v410b2e1e++cUj+y1JOlRL9kxBkvRIhoIkqTMUJEmdoSBJ6gwFSVJnKEiSOkNBktQZCpKkzlCQJHWGgiSpMxQkSZ2hIEnqDAVJUmcoSJI6Q0GS1A0aCkmWJ7k6yV1JdiZ5UZITktyY5Pvt+/g2Nkk+mmRXktuTnDpkbZKkRxr6TOEjwA1V9Rzg+cBO4GJge1WtA7a3ZYCzgXXtswm4ZODaJEn7GSwUkjwDeCmwBaCqflNVDwAbgK1t2Fbg3NbeAFxWs24BlidZOVR9kqRHGvJM4WRgBvhkkm8luTTJccCKqrq3jbkPWNHaq4Ddc7afbn0Pk2RTkqkkUzMzMwOWL0lLz5ChsAw4Fbikqk4Bfsnvp4oAqKoC6rHstKo2V9VkVU1OTEwsWrGSpGFDYRqYrqpb2/LVzIbE/fumhdr33rZ+D7BmzvarW58kaUQGC4Wqug/YneTZrWs9cCewDdjY+jYC17X2NuD8dhfS6cCDc6aZJEkjsGzg/b8VuDzJscDdwAXMBtFVSS4E7gHOa2OvB84BdgG/amMlSSM0aChU1W3A5Dyr1s8ztoCLhqxHknRgPtEsSeoMBUlSZyhIkjpDQZLUGQqSpM5QkCR1hoIkqTMUJEmdoSBJ6gwFSVJnKEiSOkNBktQZCpKkzlCQJHWGgiSpMxQkSZ2hIEnqDAVJUmcoSJI6Q0GS1BkKkqTOUJAkdYOGQpIfJLkjyW1JplrfCUluTPL99n1860+SjybZleT2JKcOWZsk6ZFGcabw8qp6QVVNtuWLge1VtQ7Y3pYBzgbWtc8m4JIR1CZJmmMc00cbgK2tvRU4d07/ZTXrFmB5kpVjqE+SlqyhQ6GALybZkWRT61tRVfe29n3AitZeBeyes+1063uYJJuSTCWZmpmZGapuSVqSlg28/5dU1Z4kfwjcmOSuuSurqpLUY9lhVW0GNgNMTk4+pm0lSQc26JlCVe1p33uBa4HTgPv3TQu1771t+B5gzZzNV7c+SdKIDBYKSY5L8rR9beCVwHeAbcDGNmwjcF1rbwPOb3chnQ48OGeaSZI0AkNOH60Ark2y73c+XVU3JPkGcFWSC4F7gPPa+OuBc4BdwK+ACwasTZI0j8FCoaruBp4/T/+PgfXz9Bdw0VD1SJIOzieaJUmdoSBJ6gwFSVJnKEiSOkNBktQZCpKkzlCQJHWGgiSpMxQkSZ2hIEnqDAVJUmcoSJI6Q0GS1BkKkqTOUJAkdYaCJKkzFCRJnaEgSeoMBUlSZyhIkjpDQZLUGQqSpG7wUEhyTJJvJfl8Wz45ya1JdiW5Msmxrf9JbXlXW7926NokSQ83ijOFtwE75yx/APhQVT0L+ClwYeu/EPhp6/9QGydJGqFBQyHJauBVwKVtOcCZwNVtyFbg3Nbe0JZp69e38ZKkERn6TOHDwLuA37XlZwIPVNVDbXkaWNXaq4DdAG39g238wyTZlGQqydTMzMyApUvS0jNYKCR5NbC3qnYs5n6ranNVTVbV5MTExGLuWpKWvGUD7vsM4DVJzgGeDDwd+AiwPMmydjawGtjTxu8B1gDTSZYBzwB+PGB9kqT9DHamUFXvrqrVVbUWeD1wU1W9AbgZeG0bthG4rrW3tWXa+puqqoaqT5L0SAsKhSRnLKRvgf4BeEeSXcxeM9jS+rcAz2z97wAuPsT9S5IO0UKnj/4FOHUBffOqqi8BX2rtu4HT5hnza+CvFliPJGkABwyFJC8CXgxMJHnHnFVPB44ZsjBJ0ugd7EzhWOCpbdzT5vT/jN9fF5AkHSUOGApV9WXgy0k+VVX3jKgmSdKYLPSawpOSbAbWzt2mqs4coihJ0ngsNBT+A/hXZl9X8dvhypEkjdNCQ+Ghqrpk0EokSWO30IfXPpfk75KsTHLCvs+glUmSRm6hZwr7njR+55y+Av54ccuRJI3TgkKhqk4euhBJ0vgtKBSSnD9ff1VdtrjlSJLGaaHTRy+c034ysB74JmAoSNJRZKHTR2+du5xkOXDFEAVJksbnUF+d/UvA6wySdJRZ6DWFzzF7txHMvgjvT4CrhipKkjQeC72m8M9z2g8B91TV9AD1SJLGaEHTR+3FeHcx+6bU44HfDFmUJGk8FvqX184Dvs7sH8E5D7g1ia/OlqSjzEKnj/4ReGFV7QVIMgH8F3D1UIVJkkZvoXcfPWFfIDQ/fgzbSpKOEAs9U7ghyReAz7Tl1wHXD1OSJGlcDvY3mp8FrKiqdyb5S+AlbdX/AJcPXZwkabQOdqbwYeDdAFV1DXANQJI/bev+fMDaJEkjdrDrAiuq6o79O1vf2gNtmOTJSb6e5NtJvpvkfa3/5CS3JtmV5Mokx7b+J7XlXW39AfcvSVp8BwuF5QdY95SDbPt/wJlV9XzgBcBZSU4HPgB8qKqeBfwUuLCNvxD4aev/UBsnSRqhg4XCVJK/3b8zyZuAHQfasGb9oi0+sX0KOJPf38q6FTi3tTe0Zdr69UlysAOQJC2eg11TeDtwbZI38PsQmASOBf7iYDtPckzb7lnAx4D/BR6oqofakGlgVWuvAnYDVNVDSR4Engn8aKEHI0l6fA4YClV1P/DiJC8Hnte6/7OqblrIzqvqt8AL2qu2rwWe8zhqBSDJJmATwEknnfR4dydJmmOhf0/hZuDmQ/2Rqnogyc3Ai4DlSZa1s4XVwJ42bA+wBphOsgx4BrMPye2/r83AZoDJycnaf70k6dAN9lRykol2hkCSpwCvAHYyGy773pu0Ebiutbe1Zdr6m6rKf/QlaYQW+kTzoVgJbG3XFZ4AXFVVn09yJ3BFkn8CvgVsaeO3AP+WZBfwE+D1A9YmSZrHYKFQVbcDp8zTfzdw2jz9v2b2LaySpDHxpXaSpM5QkCR1hoIkqTMUJEmdoSBJ6gwFSVJnKEiSOkNBktQZCpKkzlCQJHWGgiSpMxQkSZ2hIEnqDAVJUmcoSJI6Q0GS1BkKkqTOUJAkdYaCJKkzFCRJnaEgSeoMBUlSN1goJFmT5OYkdyb5bpK3tf4TktyY5Pvt+/jWnyQfTbIrye1JTh2qNknS/IY8U3gI+Puqei5wOnBRkucCFwPbq2odsL0tA5wNrGufTcAlA9YmSZrHYKFQVfdW1Tdb++fATmAVsAHY2oZtBc5t7Q3AZTXrFmB5kpVD1SdJeqSRXFNIshY4BbgVWFFV97ZV9wErWnsVsHvOZtOtT5I0IoOHQpKnAp8F3l5VP5u7rqoKqMe4v01JppJMzczMLGKlkqRBQyHJE5kNhMur6prWff++aaH2vbf17wHWzNl8det7mKraXFWTVTU5MTExXPGStAQNefdRgC3Azqr64JxV24CNrb0RuG5O//ntLqTTgQfnTDNJkkZg2YD7PgN4I3BHktta33uA9wNXJbkQuAc4r627HjgH2AX8CrhgwNokSfMYLBSq6qtAHmX1+nnGF3DRUPVIkg7OJ5olSZ2hIEnqDAVJUmcoSJI6Q0GS1BkKkqTOUJAkdYaCJKkzFCRJnaEgSeoMBUlSZyhIkjpDQZLUGQqSpM5QkCR1hoIkqTMUJEmdoSBJ6gwFSVJnKEiSOkNBktQZCpKkzlCQJHWDhUKSTyTZm+Q7c/pOSHJjku+37+Nbf5J8NMmuJLcnOXWouiRJj27IM4VPAWft13cxsL2q1gHb2zLA2cC69tkEXDJgXZKkRzFYKFTVV4Cf7Ne9Adja2luBc+f0X1azbgGWJ1k5VG2SpPmN+prCiqq6t7XvA1a09ipg95xx063vEZJsSjKVZGpmZma4SiVpCRrbheaqKqAOYbvNVTVZVZMTExMDVCZJS9eoQ+H+fdNC7Xtv698DrJkzbnXrkySN0KhDYRuwsbU3AtfN6T+/3YV0OvDgnGkmSdKILBtqx0k+A7wMODHJNPBe4P3AVUkuBO4BzmvDrwfOAXYBvwIuGKouSdKjGywUquqvH2XV+nnGFnDRULVIkhbGJ5olSZ2hIEnqDAVJUmcoSJI6Q0GS1BkKkqTOUJAkdYaCJKkzFCRJnaEgSeoMhaPUqjUnkWQkn1VrThr34UpaJIO9+0jj9cPp3bzu418byW9d+eYXj+R3JA3PMwVJUmco6Igzyqkxp8e01Dh9pCPOKKfGwOkxLS2eKUiSOkNBktQZCpKkzlCQDiNeRNe4eaFZOox4EV3j5pmCJKkzFCRJ3WEVCknOSvK9JLuSXDzueiQtLt/Jdfg7bK4pJDkG+BjwCmAa+EaSbVV153grk7RYfCfX4e9wOlM4DdhVVXdX1W+AK4ANY65JkhbkaLlzLFU1yI4fqySvBc6qqje15TcCf1ZVb9lv3CZgU1t8NvC9Q/zJE4EfHeK2RyqPeWnwmJeGx3PMf1RVE/OtOGymjxaqqjYDmx/vfpJMVdXkIpR0xPCYlwaPeWkY6pgPp+mjPcCaOcurW58kaUQOp1D4BrAuyclJjgVeD2wbc02StKQcNtNHVfVQkrcAXwCOAT5RVd8d8Ccf9xTUEchjXho85qVhkGM+bC40S5LG73CaPpIkjZmhIEnqllwoJPlEkr1JvjPuWkYlyZokNye5M8l3k7xt3DUNLcmTk3w9ybfbMb9v3DWNQpJjknwryefHXcsoJPlBkjuS3JZkatz1jEKS5UmuTnJXkp1JXrSo+19q1xSSvBT4BXBZVT1v3PWMQpKVwMqq+maSpwE7gHOP5leIJAlwXFX9IskTga8Cb6uqW8Zc2qCSvAOYBJ5eVa8edz1DS/IDYLKqlsyDa0m2Av9dVZe2OzX/oKoeWKz9L7kzhar6CvCTcdcxSlV1b1V9s7V/DuwEVo23qmHVrF+0xSe2z1H9P6Akq4FXAZeOuxYNI8kzgJcCWwCq6jeLGQiwBENhqUuyFjgFuHXMpQyuTaXcBuwFbqyqo/2YPwy8C/jdmOsYpQK+mGRHewXO0e5kYAb4ZJsmvDTJcYv5A4bCEpLkqcBngbdX1c/GXc/Qquq3VfUCZp+OPy3JUTtdmOTVwN6q2jHuWkbsJVV1KnA2cFGbHj6aLQNOBS6pqlOAXwKL+mcGDIUlos2rfxa4vKquGXc9o9ROr28GzhpzKUM6A3hNm2O/Ajgzyb+Pt6ThVdWe9r0XuJbZty0fzaaB6TlnvVczGxKLxlBYAtpF1y3Azqr64LjrGYUkE0mWt/ZTmP07HXeNtagBVdW7q2p1Va1l9hUxN1XV34y5rEElOa7dOEGbQnklcFTfVVhV9wG7kzy7da0HFvWGkcPmNRejkuQzwMuAE5NMA++tqi3jrWpwZwBvBO5oc+wA76mq68dX0uBWAlvbH296AnBVVS2J2zSXkBXAtbP/52EZ8OmqumG8JY3EW4HL251HdwMXLObOl9wtqZKkR+f0kSSpMxQkSZ2hIEnqDAVJUmcoSJI6Q0GS1BkKkqTu/wGWbGXgQomClAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(len(correct_permutation_round[correct_permutation_mask])/len(correct_permutation_mask))\n",
    "sns.histplot(correct_permutation_round[correct_permutation_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: ylabel='Count'>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(final_msp_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: ylabel='Count'>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(final_msp_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([1, 2, 3])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round_msp_predict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_cols[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5313653136531366\n",
      "1084 0.518450184501845\n",
      "round=1\n",
      "tensor(1084) tensor(0.5332)\n",
      "round=2\n",
      "tensor(935) tensor(0.5251)\n",
      "round=3\n",
      "tensor(759) tensor(0.5046)\n"
     ]
    }
   ],
   "source": [
    "# TODO: maybe shorter columns are easier to predict \n",
    "print(correct_init_mask.sum().item()/len(correct_init_mask))\n",
    "print(len(correct_msp_mask), correct_msp_mask.sum().item()/len(correct_msp_mask))\n",
    "for round in round_msp_predict.keys():\n",
    "    print(f\"round={round}\")\n",
    "    round_mask_temp = [(value- (value+1)//2)>(round-1) for value in num_cols]\n",
    "    print(sum(round_mask_temp),sum(torch.Tensor(round_msp_predict[round]).reshape(-1)==labels_test[round_mask_temp])/len(round_msp_predict[round])\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1084 1085\n"
     ]
    }
   ],
   "source": [
    "print(len(correct_init_mask), len(num_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2) 66 tensor(0.6364)\n",
      "tensor(3) 83 tensor(0.5060)\n",
      "tensor(4) 88 tensor(0.5795)\n",
      "tensor(5) 88 tensor(0.5455)\n",
      "tensor(6) 75 tensor(0.5733)\n",
      "tensor(7) 684 tensor(0.5117)\n"
     ]
    }
   ],
   "source": [
    "num_cols = num_cols[num_cols>0]\n",
    "for i in num_cols.unique():\n",
    "    mask = num_cols == i\n",
    "    correct_init_mask_i = correct_init_mask[mask]\n",
    "    print(i, len(correct_init_mask_i), correct_init_mask_i.sum()/len(correct_init_mask_i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round_msp_predict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1084])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_cols.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1085])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5313653136531366\n",
      "1084 0.49723247232472323\n",
      "round=1\n",
      "tensor(1084) tensor(0.5332)\n",
      "round=2\n",
      "tensor(1018) tensor(0.5236)\n",
      "round=3\n",
      "tensor(935) tensor(0.5091)\n",
      "round=4\n",
      "tensor(847) tensor(0.4841)\n",
      "round=5\n",
      "tensor(759) tensor(0.4862)\n",
      "round=6\n",
      "tensor(684) tensor(0.4532)\n"
     ]
    }
   ],
   "source": [
    "# TODO: maybe shorter columns are easier to predict \n",
    "print(correct_init_mask.sum().item()/len(correct_init_mask))\n",
    "print(len(correct_msp_mask), correct_msp_mask.sum().item()/len(correct_msp_mask))\n",
    "for round in round_msp_predict.keys():\n",
    "    print(f\"round={round}\")\n",
    "    round_mask_temp = [(value- 1)>(round-1) for value in num_cols]\n",
    "    print(sum(round_mask_temp),sum(torch.Tensor(round_msp_predict[round]).reshape(-1)==labels_test[round_mask_temp])/len(round_msp_predict[round])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************num_col=2, num 66****************************\n",
      "Init: 0.6363636363636364\n",
      "MSP final 66 0.5606060606060606\n",
      "round=1\n",
      "tensor(66) tensor(0.5606)\n",
      "**********************num_col=3, num 83****************************\n",
      "Init: 0.5060240963855421\n",
      "MSP final 83 0.5301204819277109\n",
      "round=1\n",
      "tensor(83) tensor(0.5301)\n",
      "round=2\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "The shape of the mask [1084] at index 0 does not match the shape of the indexed tensor [1018] at index 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [164]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m round_mask_temp \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([(value\u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m>\u001b[39m(\u001b[38;5;28mround\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m num_cols])\n\u001b[1;32m     11\u001b[0m round_mask_for_predict \u001b[38;5;241m=\u001b[39m round_mask_temp[mask]\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28msum\u001b[39m(round_mask_temp\u001b[38;5;241m&\u001b[39mmask),\u001b[38;5;28msum\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mround_msp_predict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mround\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m]\u001b[49m[round_mask_for_predict]\u001b[38;5;241m==\u001b[39mlabels_test[round_mask_temp\u001b[38;5;241m&\u001b[39mmask])\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(torch\u001b[38;5;241m.\u001b[39mTensor(round_msp_predict[\u001b[38;5;28mround\u001b[39m])\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)[mask][round_mask_for_predict])\n\u001b[1;32m     13\u001b[0m )\n",
      "\u001b[0;31mIndexError\u001b[0m: The shape of the mask [1084] at index 0 does not match the shape of the indexed tensor [1018] at index 0"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "for num_col in num_cols.unique():\n",
    "    print(f\"**********************num_col={num_col}, num {(num_cols == num_col).sum().item()}****************************\")\n",
    "    mask = num_cols == num_col\n",
    "    print(\"Init:\", correct_init_mask[mask].sum().item()/len(correct_init_mask[mask]))\n",
    "    print(\"MSP final\", len(correct_msp_mask[mask]), correct_msp_mask[mask].sum().item()/len(correct_msp_mask[mask]))\n",
    "    for round in round_msp_predict.keys():\n",
    "        if round > num_col-1:\n",
    "            continue\n",
    "        print(f\"round={round}\")\n",
    "        round_mask_temp = torch.tensor([(value- 1)>(round-1) for value in num_cols])\n",
    "        round_mask_for_predict = round_mask_temp[mask]\n",
    "        print(sum(round_mask_temp&mask),sum(torch.Tensor(round_msp_predict[round]).reshape(-1)[mask][round_mask_for_predict]==labels_test[round_mask_temp&mask])/len(torch.Tensor(round_msp_predict[round]).reshape(-1)[mask][round_mask_for_predict])\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1084])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round_mask_temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([83])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round_mask_for_predict.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "The shape of the mask [83] at index 0 does not match the shape of the indexed tensor [1018] at index 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [168]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mlen\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mround_msp_predict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mround\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mround_mask_for_predict\u001b[49m\u001b[43m]\u001b[49m)\n",
      "\u001b[0;31mIndexError\u001b[0m: The shape of the mask [83] at index 0 does not match the shape of the indexed tensor [1018] at index 0"
     ]
    }
   ],
   "source": [
    "len(torch.Tensor(round_msp_predict[round]).reshape(-1)[round_mask_for_predict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (66) must match the size of tensor b (1084) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [161]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28msum\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mround_msp_predict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mround\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mround_mask_for_predict\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43mlabels_test\u001b[49m\u001b[43m[\u001b[49m\u001b[43mround_mask_temp\u001b[49m\u001b[43m]\u001b[49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (66) must match the size of tensor b (1084) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "sum(torch.Tensor(round_msp_predict[round]).reshape(-1)[mask][round_mask_for_predict]==labels_test[round_mask_temp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1084])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_test[round_mask_temp].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([66])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(round_msp_predict[round]).reshape(-1)[mask].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1084])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(round_msp_predict[round]).reshape(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1084])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round_mask_temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1084])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1084])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(round_msp_predict[round]).reshape(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "The shape of the mask [1084] at index 0 does not match the shape of the indexed tensor [1085] at index 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [135]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlabels_test\u001b[49m\u001b[43m[\u001b[49m\u001b[43mround_mask_temp\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mIndexError\u001b[0m: The shape of the mask [1084] at index 0 does not match the shape of the indexed tensor [1085] at index 0"
     ]
    }
   ],
   "source": [
    "labels_test[round_mask_temp].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1085])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1084 0 1084 0 1084\n"
     ]
    }
   ],
   "source": [
    "\n",
    "idx_list = torch.tensor(idx_list).reshape(-1)\n",
    "uncertain_init_mask = torch.tensor(uncertain_init_mask).reshape(-1)\n",
    "uncertain_target_mask = torch.tensor(uncertain_target_mask).reshape(-1)\n",
    "correct_init_mask = torch.tensor(correct_init_mask).reshape(-1)\n",
    "correct_target_mask = torch.tensor(correct_target_mask).reshape(-1)\n",
    "correct_permutation_mask = torch.tensor(correct_permutation_mask).reshape(-1)\n",
    "correct_permutation_confident_scores = torch.tensor(correct_permutation_confident_scores).reshape(-1)\n",
    "correct_msp_mask = torch.tensor(correct_msp_mask).reshape(-1)\n",
    "init_target_mask = torch.tensor(init_target_mask).reshape(-1)\n",
    "ood_score_init_list = torch.tensor(ood_score_init_list).reshape(-1)\n",
    "ood_score_target_list = torch.tensor(ood_score_target_list).reshape(-1)\n",
    "ood_score_final_list = torch.tensor(ood_score_final_list).reshape(-1)\n",
    "correct_permutation_round = torch.tensor(correct_permutation_round).reshape(-1)\n",
    "\n",
    "msp_predicts = torch.tensor(msp_predicts).reshape(-1)\n",
    "print(len(idx_list), len(init_target_mask), len(correct_init_mask), len(correct_target_mask), len(correct_permutation_mask))\n",
    "# torch.save({\"idx_list\": idx_list, \"uncertain_init_mask\": uncertain_init_mask, \"uncertain_target_mask\": uncertain_target_mask, \"correct_init_mask\": correct_init_mask, \"correct_target_mask\": correct_target_mask, \"correct_permutation_mask\": correct_permutation_mask, \"correct_msp_mask\": correct_msp_mask, \"init_target_mask\": init_target_mask, \"ood_score_init_list\": ood_score_init_list, \"ood_score_target_list\": ood_score_target_list, \"ood_score_final_list\": ood_score_final_list}, f\"./results/brute_force_permutation_alpha@{alpha}_drop.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************Threshold: 0.8****************************\n",
      "Init\n",
      "528 758 0.6965699208443272\n",
      "48 326 0.147239263803681\n",
      "MSP\n",
      "503 758 0.6635883905013192\n",
      "59 326 0.18098159509202455\n",
      "MSP & Init\n",
      "482\n",
      "549 758 0.7242744063324539\n",
      "85 326 0.2607361963190184\n",
      "Permutation\n",
      "609 758 0.8034300791556728\n",
      "141 326 0.4325153374233129\n",
      "Permutation Confident\n",
      "567 758 0.7480211081794196\n",
      "81 326 0.24846625766871167\n",
      "*********************Threshold: 0.82****************************\n",
      "Init\n",
      "524 742 0.706199460916442\n",
      "52 342 0.15204678362573099\n",
      "MSP\n",
      "497 742 0.6698113207547169\n",
      "65 342 0.19005847953216373\n",
      "MSP & Init\n",
      "479\n",
      "542 742 0.7304582210242587\n",
      "92 342 0.26900584795321636\n",
      "Permutation\n",
      "599 742 0.807277628032345\n",
      "151 342 0.4415204678362573\n",
      "Permutation Confident\n",
      "558 742 0.7520215633423181\n",
      "88 342 0.2573099415204678\n",
      "*********************Threshold: 0.83****************************\n",
      "Init\n",
      "523 736 0.7105978260869565\n",
      "53 348 0.15229885057471265\n",
      "MSP\n",
      "496 736 0.6739130434782609\n",
      "66 348 0.1896551724137931\n",
      "MSP & Init\n",
      "478\n",
      "541 736 0.7350543478260869\n",
      "93 348 0.2672413793103448\n",
      "Permutation\n",
      "595 736 0.8084239130434783\n",
      "155 348 0.4454022988505747\n",
      "Permutation Confident\n",
      "555 736 0.7540760869565217\n",
      "89 348 0.2557471264367816\n",
      "*********************Threshold: 0.84****************************\n",
      "Init\n",
      "520 731 0.7113543091655267\n",
      "56 353 0.15864022662889518\n",
      "MSP\n",
      "494 731 0.6757865937072504\n",
      "68 353 0.19263456090651557\n",
      "MSP & Init\n",
      "476\n",
      "538 731 0.7359781121751026\n",
      "96 353 0.2719546742209632\n",
      "Permutation\n",
      "590 731 0.8071135430916553\n",
      "160 353 0.45325779036827196\n",
      "Permutation Confident\n",
      "551 731 0.7537619699042407\n",
      "92 353 0.26062322946175637\n",
      "*********************Threshold: 0.85****************************\n",
      "Init\n",
      "519 725 0.7158620689655173\n",
      "57 359 0.15877437325905291\n",
      "MSP\n",
      "493 725 0.68\n",
      "69 359 0.19220055710306408\n",
      "MSP & Init\n",
      "475\n",
      "537 725 0.7406896551724138\n",
      "97 359 0.27019498607242337\n",
      "Permutation\n",
      "587 725 0.8096551724137931\n",
      "163 359 0.45403899721448465\n",
      "Permutation Confident\n",
      "549 725 0.7572413793103449\n",
      "90 359 0.25069637883008355\n",
      "*********************Threshold: 0.9****************************\n",
      "Init\n",
      "505 688 0.7340116279069767\n",
      "71 396 0.17929292929292928\n",
      "MSP\n",
      "482 688 0.7005813953488372\n",
      "80 396 0.20202020202020202\n",
      "MSP & Init\n",
      "466\n",
      "521 688 0.7572674418604651\n",
      "113 396 0.28535353535353536\n",
      "Permutation\n",
      "564 688 0.8197674418604651\n",
      "186 396 0.4696969696969697\n",
      "Permutation Confident\n",
      "531 688 0.7718023255813954\n",
      "100 396 0.25252525252525254\n"
     ]
    }
   ],
   "source": [
    "# weighted, only uncertain init\n",
    "\n",
    "for threshold in [0.80, 0.82, 0.83, 0.84, 0.85, 0.9]:\n",
    "    correct_permutation_confident_mask = correct_permutation_confident_scores>threshold\n",
    "    uncertain_init_mask = ood_score_init_list < threshold\n",
    "    uncertain_target_mask = ood_score_target_list < threshold\n",
    "    condition_mask = ~uncertain_init_mask\n",
    "    print(f\"*********************Threshold: {threshold}****************************\")\n",
    "    print(\"Init\")\n",
    "    # init prediction\n",
    "    print((condition_mask&correct_init_mask).sum().item(), \n",
    "        (condition_mask).sum().item(), \n",
    "        (condition_mask&correct_init_mask).sum().item()/(condition_mask).sum().item())\n",
    "    # init prediction\n",
    "    print((~condition_mask&correct_init_mask).sum().item(), \n",
    "        (~condition_mask).sum().item(), \n",
    "       (~condition_mask&correct_init_mask).sum().item()/(~condition_mask).sum().item())\n",
    "    print(\"MSP\")\n",
    "    # msp prediction\n",
    "    print((condition_mask&correct_msp_mask).sum().item(), \n",
    "        (condition_mask).sum().item(), \n",
    "        (condition_mask&correct_msp_mask).sum().item()/(condition_mask).sum().item())\n",
    "    # msp prediction\n",
    "    print((~condition_mask&correct_msp_mask).sum().item(), \n",
    "        (~condition_mask).sum().item(), \n",
    "        (~condition_mask&correct_msp_mask).sum().item()/(~condition_mask).sum().item())\n",
    "    print(\"MSP & Init\")\n",
    "    print(((condition_mask&correct_init_mask)&(condition_mask&correct_msp_mask)).sum().item())\n",
    "    # print(((~(init_target_mask&(~uncertain_init_mask & ~uncertain_target_mask))& correct_msp_mask)&(~(init_target_mask&(~uncertain_init_mask & ~uncertain_target_mask))& correct_init_mask)).sum().item())\n",
    "    print(((condition_mask&correct_init_mask)|(condition_mask&correct_msp_mask)).sum().item(),\n",
    "          (condition_mask).sum().item(), \n",
    "          ((condition_mask&correct_init_mask)|(condition_mask&correct_msp_mask)).sum().item()/(condition_mask).sum().item())\n",
    "    print(((~condition_mask&correct_init_mask)|(~condition_mask&correct_msp_mask)).sum().item(),\n",
    "          (~condition_mask).sum().item(), \n",
    "          ((~condition_mask&correct_init_mask)|(~condition_mask&correct_msp_mask)).sum().item()/(~condition_mask).sum().item())\n",
    "    print(\"Permutation\")\n",
    "    print((condition_mask&correct_permutation_mask).sum().item(), \n",
    "        (condition_mask).sum().item(), \n",
    "        (condition_mask&correct_permutation_mask).sum().item()/(condition_mask).sum().item())\n",
    "    print((~condition_mask&correct_permutation_mask).sum().item(), \n",
    "         (~condition_mask).sum().item(), \n",
    "        (~condition_mask&correct_permutation_mask).sum().item()/(~condition_mask).sum().item())\n",
    "    print(\"Permutation Confident\")\n",
    "    print((condition_mask&correct_permutation_confident_mask).sum().item(), \n",
    "        (condition_mask).sum().item(), \n",
    "        (condition_mask&correct_permutation_confident_mask).sum().item()/(condition_mask).sum().item())\n",
    "    print((~condition_mask&correct_permutation_confident_mask).sum().item(),\n",
    "            (~condition_mask).sum().item(), \n",
    "            (~condition_mask&correct_permutation_confident_mask).sum().item()/(~condition_mask).sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************Threshold: 0.82****************************\n",
      "Init\n",
      "524 742 0.706199460916442\n",
      "52 342 0.15204678362573099\n",
      "MSP\n",
      "497 742 0.6698113207547169\n",
      "65 342 0.19005847953216373\n",
      "MSP & Init\n",
      "479\n",
      "542 742 0.7304582210242587\n",
      "92 342 0.26900584795321636\n",
      "Permutation\n",
      "599 742 0.807277628032345\n",
      "151 342 0.4415204678362573\n",
      "Permutation Confident\n",
      "558 742 0.7520215633423181\n",
      "88 342 0.2573099415204678\n"
     ]
    }
   ],
   "source": [
    "# weighted, only uncertain init\n",
    "\n",
    "for threshold in [0.82]:\n",
    "    correct_permutation_confident_mask = correct_permutation_confident_scores>threshold\n",
    "    uncertain_init_mask = ood_score_init_list < threshold\n",
    "    uncertain_target_mask = ood_score_target_list < threshold\n",
    "    condition_mask = ~uncertain_init_mask\n",
    "    print(f\"*********************Threshold: {threshold}****************************\")\n",
    "    print(\"Init\")\n",
    "    # init prediction\n",
    "    print((condition_mask&correct_init_mask).sum().item(), \n",
    "        (condition_mask).sum().item(), \n",
    "        (condition_mask&correct_init_mask).sum().item()/(condition_mask).sum().item())\n",
    "    # init prediction\n",
    "    print((~condition_mask&correct_init_mask).sum().item(), \n",
    "        (~condition_mask).sum().item(), \n",
    "       (~condition_mask&correct_init_mask).sum().item()/(~condition_mask).sum().item())\n",
    "    print(\"MSP\")\n",
    "    # msp prediction\n",
    "    print((condition_mask&correct_msp_mask).sum().item(), \n",
    "        (condition_mask).sum().item(), \n",
    "        (condition_mask&correct_msp_mask).sum().item()/(condition_mask).sum().item())\n",
    "    # msp prediction\n",
    "    print((~condition_mask&correct_msp_mask).sum().item(), \n",
    "        (~condition_mask).sum().item(), \n",
    "        (~condition_mask&correct_msp_mask).sum().item()/(~condition_mask).sum().item())\n",
    "    print(\"MSP & Init\")\n",
    "    print(((condition_mask&correct_init_mask)&(condition_mask&correct_msp_mask)).sum().item())\n",
    "    # print(((~(init_target_mask&(~uncertain_init_mask & ~uncertain_target_mask))& correct_msp_mask)&(~(init_target_mask&(~uncertain_init_mask & ~uncertain_target_mask))& correct_init_mask)).sum().item())\n",
    "    print(((condition_mask&correct_init_mask)|(condition_mask&correct_msp_mask)).sum().item(),\n",
    "          (condition_mask).sum().item(), \n",
    "          ((condition_mask&correct_init_mask)|(condition_mask&correct_msp_mask)).sum().item()/(condition_mask).sum().item())\n",
    "    print(((~condition_mask&correct_init_mask)|(~condition_mask&correct_msp_mask)).sum().item(),\n",
    "          (~condition_mask).sum().item(), \n",
    "          ((~condition_mask&correct_init_mask)|(~condition_mask&correct_msp_mask)).sum().item()/(~condition_mask).sum().item())\n",
    "    print(\"Permutation\")\n",
    "    print((condition_mask&correct_permutation_mask).sum().item(), \n",
    "        (condition_mask).sum().item(), \n",
    "        (condition_mask&correct_permutation_mask).sum().item()/(condition_mask).sum().item())\n",
    "    print((~condition_mask&correct_permutation_mask).sum().item(), \n",
    "         (~condition_mask).sum().item(), \n",
    "        (~condition_mask&correct_permutation_mask).sum().item()/(~condition_mask).sum().item())\n",
    "    print(\"Permutation Confident\")\n",
    "    print((condition_mask&correct_permutation_confident_mask).sum().item(), \n",
    "        (condition_mask).sum().item(), \n",
    "        (condition_mask&correct_permutation_confident_mask).sum().item()/(condition_mask).sum().item())\n",
    "    print((~condition_mask&correct_permutation_confident_mask).sum().item(),\n",
    "            (~condition_mask).sum().item(), \n",
    "            (~condition_mask&correct_permutation_confident_mask).sum().item()/(~condition_mask).sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both correct tensor(25) tensor(0.6187) tensor(0.9433)\n",
      "MSP correct  tensor(40) tensor(0.5216) tensor(0.9643)\n",
      "Init correct tensor(27) tensor(0.5349) tensor(0.9423)\n",
      "Permutation correct tensor(13) tensor(0.5649) tensor(0.9761)\n"
     ]
    }
   ],
   "source": [
    "# MSP is wrong, but init is correct\n",
    "target_col_idx_msp_init = idx_list[~condition_mask&correct_msp_mask&correct_init_mask]\n",
    "target_col_idx_msp =  idx_list[~condition_mask&correct_msp_mask&~correct_init_mask]\n",
    "target_col_idx_init =  idx_list[~condition_mask&~correct_msp_mask&correct_init_mask]\n",
    "target_col_idx_permutation = idx_list[~condition_mask&correct_permutation_confident_mask&~correct_msp_mask&~correct_init_mask]\n",
    "target_col_idx_permutation_msp =  idx_list[~condition_mask&correct_permutation_confident_mask&~correct_msp_mask]\n",
    "target_col_idx_permutation_init =  idx_list[~condition_mask&correct_permutation_confident_mask&~correct_init_mask]\n",
    "\n",
    "print(\"Both correct\", (~condition_mask&correct_msp_mask&correct_init_mask).sum(), \n",
    "      ood_score_init_list[~condition_mask&correct_msp_mask&correct_init_mask].mean(), \n",
    "      # ood_score_target_list[~condition_mask&correct_msp_mask&correct_init_mask].mean(),\n",
    "      ood_score_final_list[~condition_mask&correct_msp_mask&correct_init_mask].mean())\n",
    "print(\"MSP correct \", (~condition_mask&correct_msp_mask&~correct_init_mask).sum(), \n",
    "      ood_score_init_list[~condition_mask&correct_msp_mask&~correct_init_mask].mean(),\n",
    "      #   ood_score_target_list[~condition_mask&correct_msp_mask&~correct_init_mask].mean(),\n",
    "        ood_score_final_list[~condition_mask&correct_msp_mask&~correct_init_mask].mean())\n",
    "print(\"Init correct\", (~condition_mask&~correct_msp_mask&correct_init_mask).sum(), \n",
    "      ood_score_init_list[~condition_mask&~correct_msp_mask&correct_init_mask].mean(),\n",
    "      #   ood_score_target_list[~condition_mask&~correct_msp_mask&correct_init_mask].mean(),\n",
    "        ood_score_final_list[~condition_mask&~correct_msp_mask&correct_init_mask].mean())\n",
    "print(\"Permutation correct\",(~condition_mask&correct_permutation_confident_mask&~correct_msp_mask&~correct_init_mask).sum(), \n",
    "      ood_score_init_list[~condition_mask&correct_permutation_confident_mask&~correct_msp_mask&~correct_init_mask].mean(),\n",
    "      #   ood_score_target_list[~condition_mask&correct_permutation_confident_mask&~correct_msp_mask&~correct_init_mask].mean(),\n",
    "        ood_score_final_list[~condition_mask&correct_permutation_confident_mask&~correct_msp_mask&~correct_init_mask].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  21,   52,   59,  118,  138,  154,  246,  265,  273,  275,  286,  287,\n",
       "         296,  325,  372,  419,  433,  568,  610,  643,  743,  808,  894,  929,\n",
       "         989, 1042, 1075])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_col_idx_permutation_msp\n",
    "# imbalance: 21, 52, 59, 154, 265, 275, 286\n",
    "# early stop: 21, 59, 246, 275\n",
    "# not imformative: 118, 138, 154 286\n",
    "# todo: 287"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "7 [1, 2, 3, 0, 4, 5, 6, 7]\n",
      "********************************************************\n",
      "(1, 2, 3, 0, 4, 5, 6) tensor(0.6667) tensor(28)\n",
      "(1, 2, 3, 0, 4, 5, 7) tensor(0.7467) tensor(28)\n",
      "(1, 2, 3, 0, 4, 6, 7) tensor(0.6455) tensor(28)\n",
      "(1, 2, 3, 0, 5, 6, 7) tensor(0.7037) tensor(28)\n",
      "(1, 2, 0, 4, 5, 6, 7) tensor(0.2869) tensor(62)\n",
      "(1, 3, 0, 4, 5, 6, 7) tensor(0.1574) tensor(90)\n",
      "(2, 3, 0, 4, 5, 6, 7) tensor(0.1626) tensor(62)\n",
      "(1, 2, 3, 0, 4, 5) tensor(0.6485) tensor(28)\n",
      "(1, 2, 3, 0, 4, 6) tensor(0.7471) tensor(28)\n",
      "(1, 2, 3, 0, 4, 7) tensor(0.2197) tensor(95)\n",
      "(1, 2, 3, 0, 5, 6) tensor(0.6111) tensor(28)\n",
      "(1, 2, 3, 0, 5, 7) tensor(0.2820) tensor(28)\n",
      "(1, 2, 3, 0, 6, 7) tensor(0.2656) tensor(95)\n",
      "(1, 2, 0, 4, 5, 6) tensor(0.3814) tensor(28)\n",
      "(1, 2, 0, 4, 5, 7) tensor(0.1848) tensor(62)\n",
      "(1, 2, 0, 4, 6, 7) tensor(0.2487) tensor(62)\n",
      "(1, 2, 0, 5, 6, 7) tensor(0.2260) tensor(28)\n",
      "(1, 3, 0, 4, 5, 6) tensor(0.6227) tensor(51)\n",
      "(1, 3, 0, 4, 5, 7) tensor(0.2212) tensor(95)\n",
      "(1, 3, 0, 4, 6, 7) tensor(0.2318) tensor(95)\n",
      "(1, 3, 0, 5, 6, 7) tensor(0.2748) tensor(95)\n",
      "(1, 0, 4, 5, 6, 7) tensor(0.2908) tensor(62)\n",
      "(2, 3, 0, 4, 5, 6) tensor(0.6289) tensor(51)\n",
      "(2, 3, 0, 4, 5, 7) tensor(0.2255) tensor(95)\n",
      "(2, 3, 0, 4, 6, 7) tensor(0.2247) tensor(95)\n",
      "(2, 3, 0, 5, 6, 7) tensor(0.2712) tensor(95)\n",
      "(2, 0, 4, 5, 6, 7) tensor(0.6842) tensor(1)\n",
      "(3, 0, 4, 5, 6, 7) tensor(0.2852) tensor(75)\n",
      "(1, 2, 3, 0, 4) tensor(0.7054) tensor(28)\n",
      "(1, 2, 3, 0, 5) tensor(0.5548) tensor(28)\n",
      "(1, 2, 3, 0, 6) tensor(0.6785) tensor(28)\n",
      "(1, 2, 3, 0, 7) tensor(0.3647) tensor(28)\n",
      "(1, 2, 0, 4, 5) tensor(0.3709) tensor(28)\n",
      "(1, 2, 0, 4, 6) tensor(0.5416) tensor(28)\n",
      "(1, 2, 0, 4, 7) tensor(0.2831) tensor(28)\n",
      "(1, 2, 0, 5, 6) tensor(0.3538) tensor(28)\n",
      "(1, 2, 0, 5, 7) tensor(0.4304) tensor(28)\n",
      "(1, 2, 0, 6, 7) tensor(0.3135) tensor(28)\n",
      "(1, 3, 0, 4, 5) tensor(0.6746) tensor(51)\n",
      "(1, 3, 0, 4, 6) tensor(0.4329) tensor(51)\n",
      "(1, 3, 0, 4, 7) tensor(0.2827) tensor(95)\n",
      "(1, 3, 0, 5, 6) tensor(0.6847) tensor(51)\n",
      "(1, 3, 0, 5, 7) tensor(0.2843) tensor(95)\n",
      "(1, 3, 0, 6, 7) tensor(0.2649) tensor(95)\n",
      "(1, 0, 4, 5, 6) tensor(0.5031) tensor(51)\n",
      "(1, 0, 4, 5, 7) tensor(0.2238) tensor(62)\n",
      "(1, 0, 4, 6, 7) tensor(0.2633) tensor(62)\n",
      "(1, 0, 5, 6, 7) tensor(0.2139) tensor(75)\n",
      "(2, 3, 0, 4, 5) tensor(0.6643) tensor(51)\n",
      "(2, 3, 0, 4, 6) tensor(0.3600) tensor(95)\n",
      "(2, 3, 0, 4, 7) tensor(0.3008) tensor(95)\n",
      "(2, 3, 0, 5, 6) tensor(0.6593) tensor(51)\n",
      "(2, 3, 0, 5, 7) tensor(0.3370) tensor(95)\n",
      "(2, 3, 0, 6, 7) tensor(0.2789) tensor(95)\n",
      "(2, 0, 4, 5, 6) tensor(0.4709) tensor(75)\n",
      "(2, 0, 4, 5, 7) tensor(0.2072) tensor(1)\n",
      "(2, 0, 4, 6, 7) tensor(0.2348) tensor(1)\n",
      "(2, 0, 5, 6, 7) tensor(0.2025) tensor(75)\n",
      "(3, 0, 4, 5, 6) tensor(0.6337) tensor(75)\n",
      "(3, 0, 4, 5, 7) tensor(0.3696) tensor(75)\n",
      "(3, 0, 4, 6, 7) tensor(0.1550) tensor(72)\n",
      "(3, 0, 5, 6, 7) tensor(0.5863) tensor(75)\n",
      "(0, 4, 5, 6, 7) tensor(0.2229) tensor(30)\n",
      "********************************************************\n",
      "(1, 2, 3, 0, 4, 6) tensor(0.7471) tensor(28)\n"
     ]
    }
   ],
   "source": [
    "# use target as head, the MSP context\n",
    "import itertools\n",
    "\n",
    "def is_sublist(A, B):\n",
    "    it = iter(B)\n",
    "    return all(x in it for x in A)\n",
    "    \n",
    "model.load_state_dict(best_state_dict, strict=False)\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "ft_embs_test = []\n",
    "labels_test = []\n",
    "logits_test = []\n",
    "log = defaultdict(list)\n",
    "num_cols = []\n",
    "threshold = 0.8\n",
    "corrected = 0\n",
    "total_mistakes = 0\n",
    "init_permutation = defaultdict(list)\n",
    "corrected_permutation = defaultdict(list)\n",
    "init_logits = defaultdict(list)\n",
    "corrected_logits = defaultdict(list)\n",
    "max_col_length = 3\n",
    "msp_threshold = 0.9\n",
    "\n",
    "\n",
    "alpha = 1.0\n",
    "\n",
    "class_weights = (1.0 / class_freq) ** alpha\n",
    "\n",
    "# Normalize the weights\n",
    "class_weights /= class_weights.sum()\n",
    "\n",
    "for batch_idx, batch in enumerate(test_dataset_iter):\n",
    "    if batch_idx == 21:\n",
    "        break\n",
    "cls_indexes = torch.LongTensor([[0, batch[\"cls_indexes\"].cpu().item()]]).to(device)\n",
    "target_col_mask = batch[\"target_col_mask\"].T\n",
    "col_idx_set = target_col_mask.unique().tolist()\n",
    "init_permutation_i = get_permutation(target_col_mask)\n",
    "successs = False\n",
    "# logits = model(batch[\"data\"].T, cls_indexes=cls_indexes,).detach().cpu()\n",
    "# logits = reweight_logits(logits, class_weights)\n",
    "# logits_init = logits.clone()\n",
    "# num_cols.append(batch[\"target_col_mask\"].max().item())\n",
    "\n",
    "\n",
    "# col_idx_set = target_col_mask.unique().tolist()\n",
    "# successs = False\n",
    "# init_permutation_i = get_permutation(target_col_mask)\n",
    "# init_permutation[batch_idx].append(get_permutation(target_col_mask))\n",
    "# init_logits[batch_idx].append(logits_init.detach().cpu())     \n",
    "# init_msp = logits_init.max().item()\n",
    "# print(batch[\"label\"].item())\n",
    "# print(get_permutation(target_col_mask), init_msp, init_logits[batch_idx][0].argmax().item()) \n",
    "print(batch[\"label\"].item())  \n",
    "print(batch[\"target_col_mask\"].max().item(), get_permutation(target_col_mask))\n",
    "print(\"********************************************************\")\n",
    "assert -1 not in col_idx_set\n",
    "max_msp = 0 \n",
    "# for r in range(1, min(len(col_idx_set), max_col_length) + 1):\n",
    "enough = False\n",
    "# for r in range(len(col_idx_set)-1, 0, -1):\n",
    "for r in range(len(col_idx_set)-1, len(col_idx_set)//2, -1):\n",
    "    for subset in itertools.combinations(col_idx_set, r):\n",
    "        if 0 not in subset and r != 1:\n",
    "            continue\n",
    "        for x in itertools.permutations(subset):\n",
    "            if not is_sublist(x, init_permutation_i):\n",
    "                continue\n",
    "            new_batch_data = []\n",
    "            for col_i in x:\n",
    "                if col_i == 0:\n",
    "                    if len(new_batch_data) == 0:\n",
    "                        cls_indexes_value = 0\n",
    "                    else:\n",
    "                        cls_indexes_value = sum([len(new_batch_data[i]) for i in range(len(new_batch_data))])\n",
    "                new_batch_data.append(batch[\"data\"].T[target_col_mask==col_i])\n",
    "            if 0 not in x:\n",
    "                cls_indexes_value = 0\n",
    "            new_batch_data = torch.cat(new_batch_data, dim=-1).reshape(1, -1)\n",
    "            cls_indexes = torch.tensor([0, cls_indexes_value]).reshape(1, -1).to(device)\n",
    "            logits_temp = model(new_batch_data, cls_indexes=cls_indexes,).detach().cpu()\n",
    "            logits_temp = reweight_logits(logits_temp, class_weights)\n",
    "            msp_temp = logits_temp.max()\n",
    "            predict_temp = logits_temp.argmax()\n",
    "            # msp_temp = F.softmax(logits_temp).max().item()\n",
    "            # predict_temp = F.softmax(logits_temp).argmax().item()\n",
    "\n",
    "            print(x, msp_temp,predict_temp)\n",
    "            if msp_temp > max_msp and 0 in x:\n",
    "                max_msp = msp_temp\n",
    "                best_msp_perm = x\n",
    "                msp_predict = predict_temp\n",
    "        \n",
    "print(\"********************************************************\")\n",
    "print(best_msp_perm, max_msp, msp_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "7 [1, 2, 3, 4, 5, 6, 7, 0]\n",
      "********************************************************\n",
      "(1, 2, 3, 4, 5, 6, 0) tensor(0.9756) tensor(2)\n",
      "(1, 2, 3, 4, 5, 7, 0) tensor(0.9756) tensor(2)\n",
      "(1, 2, 3, 4, 6, 7, 0) tensor(0.9892) tensor(2)\n",
      "(1, 2, 3, 5, 6, 7, 0) tensor(0.9861) tensor(2)\n",
      "(1, 2, 4, 5, 6, 7, 0) tensor(0.8058) tensor(2)\n",
      "(1, 3, 4, 5, 6, 7, 0) tensor(0.9832) tensor(2)\n",
      "(2, 3, 4, 5, 6, 7, 0) tensor(0.9849) tensor(2)\n",
      "(1, 2, 3, 4, 5, 0) tensor(0.9788) tensor(17)\n",
      "(1, 2, 3, 4, 6, 0) tensor(0.6931) tensor(2)\n",
      "(1, 2, 3, 4, 7, 0) tensor(0.8591) tensor(17)\n",
      "(1, 2, 3, 5, 6, 0) tensor(0.4576) tensor(17)\n",
      "(1, 2, 3, 5, 7, 0) tensor(0.9128) tensor(17)\n",
      "(1, 2, 3, 6, 7, 0) tensor(0.9318) tensor(17)\n",
      "(1, 2, 4, 5, 6, 0) tensor(0.9621) tensor(17)\n",
      "(1, 2, 4, 5, 7, 0) tensor(0.8874) tensor(17)\n",
      "(1, 2, 4, 6, 7, 0) tensor(0.9143) tensor(17)\n",
      "(1, 2, 5, 6, 7, 0) tensor(0.9435) tensor(17)\n",
      "(1, 3, 4, 5, 6, 0) tensor(0.6641) tensor(2)\n",
      "(1, 3, 4, 5, 7, 0) tensor(0.9132) tensor(17)\n",
      "(1, 3, 4, 6, 7, 0) tensor(0.9250) tensor(17)\n",
      "(1, 3, 5, 6, 7, 0) tensor(0.9355) tensor(17)\n",
      "(1, 4, 5, 6, 7, 0) tensor(0.9499) tensor(17)\n",
      "(2, 3, 4, 5, 6, 0) tensor(0.5688) tensor(2)\n",
      "(2, 3, 4, 5, 7, 0) tensor(0.8687) tensor(17)\n",
      "(2, 3, 4, 6, 7, 0) tensor(0.9038) tensor(17)\n",
      "(2, 3, 5, 6, 7, 0) tensor(0.9312) tensor(17)\n",
      "(2, 4, 5, 6, 7, 0) tensor(0.9316) tensor(17)\n",
      "(3, 4, 5, 6, 7, 0) tensor(0.9629) tensor(17)\n",
      "(1, 2, 3, 4, 0) tensor(0.7773) tensor(2)\n",
      "(1, 2, 3, 5, 0) tensor(0.7912) tensor(2)\n",
      "(1, 2, 3, 6, 0) tensor(0.9775) tensor(2)\n",
      "(1, 2, 3, 7, 0) tensor(0.9461) tensor(2)\n",
      "(1, 2, 4, 5, 0) tensor(0.5401) tensor(2)\n",
      "(1, 2, 4, 6, 0) tensor(0.9524) tensor(2)\n",
      "(1, 2, 4, 7, 0) tensor(0.8936) tensor(2)\n",
      "(1, 2, 5, 6, 0) tensor(0.8892) tensor(2)\n",
      "(1, 2, 5, 7, 0) tensor(0.6477) tensor(2)\n",
      "(1, 2, 6, 7, 0) tensor(0.9313) tensor(2)\n",
      "(1, 3, 4, 5, 0) tensor(0.7349) tensor(2)\n",
      "(1, 3, 4, 6, 0) tensor(0.9743) tensor(2)\n",
      "(1, 3, 4, 7, 0) tensor(0.9496) tensor(2)\n",
      "(1, 3, 5, 6, 0) tensor(0.9717) tensor(2)\n",
      "(1, 3, 5, 7, 0) tensor(0.9367) tensor(2)\n",
      "(1, 3, 6, 7, 0) tensor(0.9720) tensor(2)\n",
      "(1, 4, 5, 6, 0) tensor(0.8835) tensor(2)\n",
      "(1, 4, 5, 7, 0) tensor(0.6229) tensor(2)\n",
      "(1, 4, 6, 7, 0) tensor(0.9298) tensor(2)\n",
      "(1, 5, 6, 7, 0) tensor(0.8745) tensor(2)\n",
      "(2, 3, 4, 5, 0) tensor(0.7891) tensor(2)\n",
      "(2, 3, 4, 6, 0) tensor(0.9679) tensor(2)\n",
      "(2, 3, 4, 7, 0) tensor(0.9539) tensor(2)\n",
      "(2, 3, 5, 6, 0) tensor(0.9569) tensor(2)\n",
      "(2, 3, 5, 7, 0) tensor(0.9183) tensor(2)\n",
      "(2, 3, 6, 7, 0) tensor(0.9639) tensor(2)\n",
      "(2, 4, 5, 6, 0) tensor(0.8854) tensor(2)\n",
      "(2, 4, 5, 7, 0) tensor(0.7749) tensor(2)\n",
      "(2, 4, 6, 7, 0) tensor(0.9456) tensor(2)\n",
      "(2, 5, 6, 7, 0) tensor(0.9016) tensor(2)\n",
      "(3, 4, 5, 6, 0) tensor(0.9491) tensor(2)\n",
      "(3, 4, 5, 7, 0) tensor(0.9097) tensor(2)\n",
      "(3, 4, 6, 7, 0) tensor(0.9600) tensor(2)\n",
      "(3, 5, 6, 7, 0) tensor(0.9491) tensor(2)\n",
      "(4, 5, 6, 7, 0) tensor(0.8982) tensor(2)\n",
      "********************************************************\n",
      "(1, 2, 3, 4, 6, 7, 0) tensor(0.9892) tensor(2)\n"
     ]
    }
   ],
   "source": [
    "# use target as head, the MSP context\n",
    "import itertools\n",
    "\n",
    "def is_sublist(A, B):\n",
    "    it = iter(B)\n",
    "    return all(x in it for x in A)\n",
    "    \n",
    "model.load_state_dict(best_state_dict, strict=False)\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "ft_embs_test = []\n",
    "labels_test = []\n",
    "logits_test = []\n",
    "log = defaultdict(list)\n",
    "num_cols = []\n",
    "threshold = 0.8\n",
    "corrected = 0\n",
    "total_mistakes = 0\n",
    "init_permutation = defaultdict(list)\n",
    "corrected_permutation = defaultdict(list)\n",
    "init_logits = defaultdict(list)\n",
    "corrected_logits = defaultdict(list)\n",
    "max_col_length = 3\n",
    "msp_threshold = 0.9\n",
    "\n",
    "\n",
    "alpha = 1.0\n",
    "\n",
    "class_weights = (1.0 / class_freq) ** alpha\n",
    "\n",
    "# Normalize the weights\n",
    "class_weights /= class_weights.sum()\n",
    "\n",
    "for batch_idx, batch in enumerate(test_dataset_iter):\n",
    "    if batch_idx == 287:\n",
    "        break\n",
    "cls_indexes = torch.LongTensor([[0, batch[\"cls_indexes\"].cpu().item()]]).to(device)\n",
    "target_col_mask = batch[\"target_col_mask\"].T\n",
    "col_idx_set = target_col_mask.unique().tolist()\n",
    "init_permutation_i = get_permutation(target_col_mask)\n",
    "successs = False\n",
    "# logits = model(batch[\"data\"].T, cls_indexes=cls_indexes,).detach().cpu()\n",
    "# logits = reweight_logits(logits, class_weights)\n",
    "# logits_init = logits.clone()\n",
    "# num_cols.append(batch[\"target_col_mask\"].max().item())\n",
    "\n",
    "\n",
    "# col_idx_set = target_col_mask.unique().tolist()\n",
    "# successs = False\n",
    "# init_permutation_i = get_permutation(target_col_mask)\n",
    "# init_permutation[batch_idx].append(get_permutation(target_col_mask))\n",
    "# init_logits[batch_idx].append(logits_init.detach().cpu())     \n",
    "# init_msp = logits_init.max().item()\n",
    "# print(batch[\"label\"].item())\n",
    "# print(get_permutation(target_col_mask), init_msp, init_logits[batch_idx][0].argmax().item()) \n",
    "print(batch[\"label\"].item())  \n",
    "print(batch[\"target_col_mask\"].max().item(), get_permutation(target_col_mask))\n",
    "print(\"********************************************************\")\n",
    "assert -1 not in col_idx_set\n",
    "max_msp = 0 \n",
    "# for r in range(1, min(len(col_idx_set), max_col_length) + 1):\n",
    "enough = False\n",
    "# for r in range(len(col_idx_set)-1, 0, -1):\n",
    "for r in range(len(col_idx_set)-1, len(col_idx_set)//2, -1):\n",
    "    for subset in itertools.combinations(col_idx_set, r):\n",
    "        if 0 not in subset and r != 1:\n",
    "            continue\n",
    "        for x in itertools.permutations(subset):\n",
    "            if not is_sublist(x, init_permutation_i):\n",
    "                continue\n",
    "            new_batch_data = []\n",
    "            for col_i in x:\n",
    "                if col_i == 0:\n",
    "                    if len(new_batch_data) == 0:\n",
    "                        cls_indexes_value = 0\n",
    "                    else:\n",
    "                        cls_indexes_value = sum([len(new_batch_data[i]) for i in range(len(new_batch_data))])\n",
    "                new_batch_data.append(batch[\"data\"].T[target_col_mask==col_i])\n",
    "            if 0 not in x:\n",
    "                cls_indexes_value = 0\n",
    "            new_batch_data = torch.cat(new_batch_data, dim=-1).reshape(1, -1)\n",
    "            cls_indexes = torch.tensor([0, cls_indexes_value]).reshape(1, -1).to(device)\n",
    "            logits_temp = model(new_batch_data, cls_indexes=cls_indexes,).detach().cpu()\n",
    "            logits_temp = reweight_logits_after_softmax(logits_temp, class_weights)\n",
    "            msp_temp = logits_temp.max()\n",
    "            predict_temp = logits_temp.argmax()\n",
    "            # msp_temp = F.softmax(logits_temp).max().item()\n",
    "            # predict_temp = F.softmax(logits_temp).argmax().item()\n",
    "\n",
    "            print(x, msp_temp,predict_temp)\n",
    "            if msp_temp > max_msp and 0 in x:\n",
    "                max_msp = msp_temp\n",
    "                best_msp_perm = x\n",
    "                msp_predict = predict_temp\n",
    "        \n",
    "print(\"********************************************************\")\n",
    "print(best_msp_perm, max_msp, msp_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************alpha: 0.25; uncertain: 0.8; debias: 1.0; msp: 0.8****************************\n",
      "ts_micro_f1=0.5429, ts_macro_f1=0.2641\n",
      "ts_micro_f1=0.5424, ts_macro_f1=0.2641\n",
      "ts_micro_f1=1.0000, ts_macro_f1=1.0000\n",
      "*********************alpha: 0.25; uncertain: 0.8; debias: 1.0; msp: 0.9****************************\n",
      "ts_micro_f1=0.5419, ts_macro_f1=0.2632\n",
      "ts_micro_f1=0.5415, ts_macro_f1=0.2632\n",
      "ts_micro_f1=1.0000, ts_macro_f1=1.0000\n",
      "*********************alpha: 0.25; uncertain: 0.8; debias: 1.0; msp: 0.99****************************\n",
      "ts_micro_f1=0.5382, ts_macro_f1=0.2665\n",
      "ts_micro_f1=0.5378, ts_macro_f1=0.2665\n",
      "ts_micro_f1=1.0000, ts_macro_f1=1.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# brute force perumate\n",
    "import itertools\n",
    "\n",
    "\n",
    "    \n",
    "model.load_state_dict(best_state_dict, strict=False)\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "change_log = []\n",
    "score_init = []\n",
    "score_best = [] \n",
    "\n",
    "max_col_length = 3\n",
    "for alpha in [0.25]:\n",
    "    class_weights = (1.0 / class_freq) ** alpha\n",
    "    # Normalize the weights\n",
    "    class_weights /= class_weights.sum()\n",
    "    for uncertainty_threshold in [ 0.8]:\n",
    "        for msp_threshold in [ 0.8, 0.9, 0.99]:\n",
    "            for debias_threshold in [1.0]:\n",
    "                print(f\"*********************alpha: {alpha}; uncertain: {uncertainty_threshold}; debias: {debias_threshold}; msp: {msp_threshold}****************************\")\n",
    "                ft_embs_test = []\n",
    "                labels_test = []\n",
    "                logits_test = []\n",
    "                log = defaultdict(list)\n",
    "                num_cols = []\n",
    "                corrected = 0\n",
    "                total_mistakes = 0\n",
    "\n",
    "                for batch_idx, batch in enumerate(test_dataloader_iter):\n",
    "                    cls_indexes = torch.LongTensor([[0, batch[\"cls_indexes\"].cpu().item()]]).to(device)\n",
    "                    target_col_mask = batch[\"target_col_mask\"].T\n",
    "                    logits = model(batch[\"data\"].T, cls_indexes=cls_indexes,)\n",
    "                    logits = reweight_logits(logits.detach().cpu(), class_weights) # Here !!!!!!!!!!\n",
    "                    msp_init = logits.max().item()\n",
    "                    num_cols.append(batch[\"target_col_mask\"].max().item())\n",
    "                    predict_init = logits.argmax().item()\n",
    "                    max_msp = 0\n",
    "                    \n",
    "                    if 1 in target_col_mask and msp_init < uncertainty_threshold:\n",
    "                        init_permutation_i = get_permutation(target_col_mask)\n",
    "                        col_idx_set = target_col_mask.unique().tolist()\n",
    "                        debias_classes = []\n",
    "                        assert -1 not in col_idx_set\n",
    "                        for r in range(len(col_idx_set)-1, len(col_idx_set)//2, -1):\n",
    "                            for subset in itertools.combinations(col_idx_set, r):\n",
    "                                if 0 not in subset and r != 1:\n",
    "                                    continue\n",
    "                                for x in itertools.permutations(subset):\n",
    "                                    if not is_sublist(x, init_permutation_i):\n",
    "                                        continue\n",
    "                                    new_batch_data = []\n",
    "                                    for col_i in x:\n",
    "                                        if col_i == 0:\n",
    "                                            if len(new_batch_data) == 0:\n",
    "                                                cls_indexes_value = 0\n",
    "                                            else:\n",
    "                                                cls_indexes_value = sum([len(new_batch_data[i]) for i in range(len(new_batch_data))])\n",
    "                                        new_batch_data.append(batch[\"data\"].T[target_col_mask==col_i])\n",
    "                                    if 0 not in x:\n",
    "                                        cls_indexes_value = 0\n",
    "                                    new_batch_data = torch.cat(new_batch_data, dim=-1).reshape(1, -1)\n",
    "                                    cls_indexes = torch.tensor([0, cls_indexes_value]).reshape(1, -1).to(device)\n",
    "                                    logits_temp = model(new_batch_data, cls_indexes=cls_indexes,)\n",
    "                                    logits_temp = reweight_logits(logits_temp.detach().cpu(), class_weights)\n",
    "                                    msp_temp = logits_temp.max().item()\n",
    "                                    predict_temp = logits_temp.argmax().item()\n",
    "                                    # if len(x) == 1 and 0 in x:\n",
    "                                    #     predict_target = predict_temp\n",
    "                                    #     msp_target = msp_temp\n",
    "                                    # # print(x, msp_temp, predict_temp)\n",
    "                                    # if 0 not in x and msp_temp > debias_threshold and (predict_temp != predict_target):\n",
    "                                    #     debias_classes.append(predict_temp)\n",
    "                                    #     continue\n",
    "                                    if msp_temp > max_msp and 0 in x and predict_temp not in debias_classes:\n",
    "                                        max_msp = msp_temp\n",
    "                                        best_msp_perm = x\n",
    "                                        msp_predict = predict_temp\n",
    "                                        logits_msp = logits_temp.clone()\n",
    "                    if max_msp > msp_threshold:\n",
    "                        logits = logits_msp.clone()\n",
    "                    labels_test.append(batch[\"label\"].cpu())\n",
    "                    logits_test.append(logits.detach().cpu())\n",
    "                labels_test = torch.cat(labels_test, dim=0)\n",
    "                logits_test = torch.stack(logits_test, dim=0)\n",
    "                preds_test = torch.argmax(logits_test, dim=1)\n",
    "                num_cols = torch.tensor(num_cols)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                from sklearn.metrics import confusion_matrix, f1_score\n",
    "                mask = num_cols > 0\n",
    "                ts_pred_list = logits_test.argmax(\n",
    "                                            1).cpu().detach().numpy().tolist()\n",
    "                ts_micro_f1 = f1_score(labels_test.reshape(-1).numpy().tolist(),\n",
    "                                    ts_pred_list,\n",
    "                                    average=\"micro\")\n",
    "                ts_macro_f1 = f1_score(labels_test.reshape(-1).numpy().tolist(),\n",
    "                                    ts_pred_list,\n",
    "                                    average=\"macro\")\n",
    "                print(\"ts_micro_f1={:.4f}, ts_macro_f1={:.4f}\".format(ts_micro_f1, ts_macro_f1))\n",
    "                ts_pred_list = logits_test.argmax(\n",
    "                                            1).cpu().detach()[mask].numpy().tolist()\n",
    "                ts_micro_f1 = f1_score(labels_test.reshape(-1)[mask].numpy().tolist(),\n",
    "                                    ts_pred_list,\n",
    "                                    average=\"micro\")\n",
    "                ts_macro_f1 = f1_score(labels_test.reshape(-1)[mask].numpy().tolist(),\n",
    "                                    ts_pred_list,\n",
    "                                    average=\"macro\")\n",
    "                print(\"ts_micro_f1={:.4f}, ts_macro_f1={:.4f}\".format(ts_micro_f1, ts_macro_f1))\n",
    "                ts_pred_list = logits_test.argmax(\n",
    "                                            1).cpu().detach()[~mask].numpy().tolist()\n",
    "                ts_micro_f1 = f1_score(labels_test.reshape(-1)[~mask].numpy().tolist(),\n",
    "                                    ts_pred_list,\n",
    "                                    average=\"micro\")\n",
    "                ts_macro_f1 = f1_score(labels_test.reshape(-1)[~mask].numpy().tolist(),\n",
    "                                    ts_pred_list,\n",
    "                                    average=\"macro\")\n",
    "                print(\"ts_micro_f1={:.4f}, ts_macro_f1={:.4f}\".format(ts_micro_f1, ts_macro_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************alpha: 0.25; uncertain: 0.8; debias: 1.0; msp: 0.0****************************\n",
      "ts_micro_f1=0.5419, ts_macro_f1=0.2611\n",
      "ts_micro_f1=0.5415, ts_macro_f1=0.2610\n",
      "ts_micro_f1=1.0000, ts_macro_f1=1.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# brute force perumate\n",
    "import itertools\n",
    "\n",
    "\n",
    "    \n",
    "model.load_state_dict(best_state_dict, strict=False)\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "change_log = []\n",
    "score_init = []\n",
    "score_best = [] \n",
    "\n",
    "max_col_length = 3\n",
    "for alpha in [0.25]:\n",
    "    class_weights = (1.0 / class_freq) ** alpha\n",
    "    # Normalize the weights\n",
    "    class_weights /= class_weights.sum()\n",
    "    for uncertainty_threshold in [ 0.8]:\n",
    "        for msp_threshold in [ 0.0]:\n",
    "            for debias_threshold in [1.0]:\n",
    "                print(f\"*********************alpha: {alpha}; uncertain: {uncertainty_threshold}; debias: {debias_threshold}; msp: {msp_threshold}****************************\")\n",
    "                ft_embs_test = []\n",
    "                labels_test = []\n",
    "                logits_test = []\n",
    "                log = defaultdict(list)\n",
    "                num_cols = []\n",
    "                corrected = 0\n",
    "                total_mistakes = 0\n",
    "\n",
    "                for batch_idx, batch in enumerate(test_dataloader_iter):\n",
    "                    cls_indexes = torch.LongTensor([[0, batch[\"cls_indexes\"].cpu().item()]]).to(device)\n",
    "                    target_col_mask = batch[\"target_col_mask\"].T\n",
    "                    logits = model(batch[\"data\"].T, cls_indexes=cls_indexes,)\n",
    "                    logits = reweight_logits(logits.detach().cpu(), class_weights) # Here !!!!!!!!!!\n",
    "                    msp_init = logits.max().item()\n",
    "                    num_cols.append(batch[\"target_col_mask\"].max().item())\n",
    "                    predict_init = logits.argmax().item()\n",
    "                    max_msp = 0\n",
    "                    \n",
    "                    if 1 in target_col_mask and msp_init < uncertainty_threshold:\n",
    "                        init_permutation_i = get_permutation(target_col_mask)\n",
    "                        col_idx_set = target_col_mask.unique().tolist()\n",
    "                        debias_classes = []\n",
    "                        assert -1 not in col_idx_set\n",
    "                        for r in range(len(col_idx_set)-1, len(col_idx_set)//2, -1):\n",
    "                            for subset in itertools.combinations(col_idx_set, r):\n",
    "                                if 0 not in subset and r != 1:\n",
    "                                    continue\n",
    "                                for x in itertools.permutations(subset):\n",
    "                                    if not is_sublist(x, init_permutation_i):\n",
    "                                        continue\n",
    "                                    new_batch_data = []\n",
    "                                    for col_i in x:\n",
    "                                        if col_i == 0:\n",
    "                                            if len(new_batch_data) == 0:\n",
    "                                                cls_indexes_value = 0\n",
    "                                            else:\n",
    "                                                cls_indexes_value = sum([len(new_batch_data[i]) for i in range(len(new_batch_data))])\n",
    "                                        new_batch_data.append(batch[\"data\"].T[target_col_mask==col_i])\n",
    "                                    if 0 not in x:\n",
    "                                        cls_indexes_value = 0\n",
    "                                    new_batch_data = torch.cat(new_batch_data, dim=-1).reshape(1, -1)\n",
    "                                    cls_indexes = torch.tensor([0, cls_indexes_value]).reshape(1, -1).to(device)\n",
    "                                    logits_temp = model(new_batch_data, cls_indexes=cls_indexes,)\n",
    "                                    logits_temp = reweight_logits(logits_temp.detach().cpu(), class_weights)\n",
    "                                    msp_temp = logits_temp.max().item()\n",
    "                                    predict_temp = logits_temp.argmax().item()\n",
    "                                    # if len(x) == 1 and 0 in x:\n",
    "                                    #     predict_target = predict_temp\n",
    "                                    #     msp_target = msp_temp\n",
    "                                    # # print(x, msp_temp, predict_temp)\n",
    "                                    # if 0 not in x and msp_temp > debias_threshold and (predict_temp != predict_target):\n",
    "                                    #     debias_classes.append(predict_temp)\n",
    "                                    #     continue\n",
    "                                    if msp_temp > max_msp and 0 in x and predict_temp not in debias_classes:\n",
    "                                        max_msp = msp_temp\n",
    "                                        best_msp_perm = x\n",
    "                                        msp_predict = predict_temp\n",
    "                                        logits_msp = logits_temp.clone()\n",
    "                    if max_msp > msp_threshold:\n",
    "                        logits = logits_msp.clone()\n",
    "                    labels_test.append(batch[\"label\"].cpu())\n",
    "                    logits_test.append(logits.detach().cpu())\n",
    "                labels_test = torch.cat(labels_test, dim=0)\n",
    "                logits_test = torch.stack(logits_test, dim=0)\n",
    "                preds_test = torch.argmax(logits_test, dim=1)\n",
    "                num_cols = torch.tensor(num_cols)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                from sklearn.metrics import confusion_matrix, f1_score\n",
    "                mask = num_cols > 0\n",
    "                ts_pred_list = logits_test.argmax(\n",
    "                                            1).cpu().detach().numpy().tolist()\n",
    "                ts_micro_f1 = f1_score(labels_test.reshape(-1).numpy().tolist(),\n",
    "                                    ts_pred_list,\n",
    "                                    average=\"micro\")\n",
    "                ts_macro_f1 = f1_score(labels_test.reshape(-1).numpy().tolist(),\n",
    "                                    ts_pred_list,\n",
    "                                    average=\"macro\")\n",
    "                print(\"ts_micro_f1={:.4f}, ts_macro_f1={:.4f}\".format(ts_micro_f1, ts_macro_f1))\n",
    "                ts_pred_list = logits_test.argmax(\n",
    "                                            1).cpu().detach()[mask].numpy().tolist()\n",
    "                ts_micro_f1 = f1_score(labels_test.reshape(-1)[mask].numpy().tolist(),\n",
    "                                    ts_pred_list,\n",
    "                                    average=\"micro\")\n",
    "                ts_macro_f1 = f1_score(labels_test.reshape(-1)[mask].numpy().tolist(),\n",
    "                                    ts_pred_list,\n",
    "                                    average=\"macro\")\n",
    "                print(\"ts_micro_f1={:.4f}, ts_macro_f1={:.4f}\".format(ts_micro_f1, ts_macro_f1))\n",
    "                ts_pred_list = logits_test.argmax(\n",
    "                                            1).cpu().detach()[~mask].numpy().tolist()\n",
    "                ts_micro_f1 = f1_score(labels_test.reshape(-1)[~mask].numpy().tolist(),\n",
    "                                    ts_pred_list,\n",
    "                                    average=\"micro\")\n",
    "                ts_macro_f1 = f1_score(labels_test.reshape(-1)[~mask].numpy().tolist(),\n",
    "                                    ts_pred_list,\n",
    "                                    average=\"macro\")\n",
    "                print(\"ts_micro_f1={:.4f}, ts_macro_f1={:.4f}\".format(ts_micro_f1, ts_macro_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************alpha: 0.25; uncertain: 0.85; debias: 1.0; msp: 0.0****************************\n",
      "ts_micro_f1=0.5429, ts_macro_f1=0.2634\n",
      "ts_micro_f1=0.5424, ts_macro_f1=0.2634\n",
      "ts_micro_f1=1.0000, ts_macro_f1=1.0000\n",
      "*********************alpha: 0.25; uncertain: 0.85; debias: 1.0; msp: 0.8****************************\n",
      "ts_micro_f1=0.5438, ts_macro_f1=0.2665\n",
      "ts_micro_f1=0.5434, ts_macro_f1=0.2665\n",
      "ts_micro_f1=1.0000, ts_macro_f1=1.0000\n",
      "*********************alpha: 0.25; uncertain: 0.85; debias: 1.0; msp: 0.85****************************\n",
      "ts_micro_f1=0.5419, ts_macro_f1=0.2636\n",
      "ts_micro_f1=0.5415, ts_macro_f1=0.2636\n",
      "ts_micro_f1=1.0000, ts_macro_f1=1.0000\n",
      "*********************alpha: 0.25; uncertain: 0.85; debias: 1.0; msp: 0.9****************************\n",
      "ts_micro_f1=0.5429, ts_macro_f1=0.2655\n",
      "ts_micro_f1=0.5424, ts_macro_f1=0.2655\n",
      "ts_micro_f1=1.0000, ts_macro_f1=1.0000\n",
      "*********************alpha: 0.25; uncertain: 0.85; debias: 1.0; msp: 0.99****************************\n",
      "ts_micro_f1=0.5392, ts_macro_f1=0.2678\n",
      "ts_micro_f1=0.5387, ts_macro_f1=0.2678\n",
      "ts_micro_f1=1.0000, ts_macro_f1=1.0000\n",
      "*********************alpha: 0.25; uncertain: 0.9; debias: 1.0; msp: 0.0****************************\n",
      "ts_micro_f1=0.5401, ts_macro_f1=0.2593\n",
      "ts_micro_f1=0.5397, ts_macro_f1=0.2593\n",
      "ts_micro_f1=1.0000, ts_macro_f1=1.0000\n",
      "*********************alpha: 0.25; uncertain: 0.9; debias: 1.0; msp: 0.8****************************\n",
      "ts_micro_f1=0.5410, ts_macro_f1=0.2624\n",
      "ts_micro_f1=0.5406, ts_macro_f1=0.2624\n",
      "ts_micro_f1=1.0000, ts_macro_f1=1.0000\n",
      "*********************alpha: 0.25; uncertain: 0.9; debias: 1.0; msp: 0.85****************************\n",
      "ts_micro_f1=0.5392, ts_macro_f1=0.2597\n",
      "ts_micro_f1=0.5387, ts_macro_f1=0.2597\n",
      "ts_micro_f1=1.0000, ts_macro_f1=1.0000\n",
      "*********************alpha: 0.25; uncertain: 0.9; debias: 1.0; msp: 0.9****************************\n",
      "ts_micro_f1=0.5401, ts_macro_f1=0.2618\n",
      "ts_micro_f1=0.5397, ts_macro_f1=0.2618\n",
      "ts_micro_f1=1.0000, ts_macro_f1=1.0000\n",
      "*********************alpha: 0.25; uncertain: 0.9; debias: 1.0; msp: 0.99****************************\n",
      "ts_micro_f1=0.5373, ts_macro_f1=0.2648\n",
      "ts_micro_f1=0.5369, ts_macro_f1=0.2648\n",
      "ts_micro_f1=1.0000, ts_macro_f1=1.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# brute force perumate\n",
    "import itertools\n",
    "\n",
    "\n",
    "    \n",
    "model.load_state_dict(best_state_dict, strict=False)\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "change_log = []\n",
    "score_init = []\n",
    "score_best = [] \n",
    "\n",
    "max_col_length = 3\n",
    "for alpha in [0.25]:\n",
    "    class_weights = (1.0 / class_freq) ** alpha\n",
    "    # Normalize the weights\n",
    "    class_weights /= class_weights.sum()\n",
    "    for uncertainty_threshold in [ 0.85, 0.9]:\n",
    "        for msp_threshold in [ 0.0, 0.8, 0.85, 0.9, 0.99]:\n",
    "            for debias_threshold in [1.0]:\n",
    "                print(f\"*********************alpha: {alpha}; uncertain: {uncertainty_threshold}; debias: {debias_threshold}; msp: {msp_threshold}****************************\")\n",
    "                ft_embs_test = []\n",
    "                labels_test = []\n",
    "                logits_test = []\n",
    "                log = defaultdict(list)\n",
    "                num_cols = []\n",
    "                corrected = 0\n",
    "                total_mistakes = 0\n",
    "\n",
    "                for batch_idx, batch in enumerate(test_dataloader_iter):\n",
    "                    cls_indexes = torch.LongTensor([[0, batch[\"cls_indexes\"].cpu().item()]]).to(device)\n",
    "                    target_col_mask = batch[\"target_col_mask\"].T\n",
    "                    logits = model(batch[\"data\"].T, cls_indexes=cls_indexes,)\n",
    "                    logits = reweight_logits(logits.detach().cpu(), class_weights) # Here !!!!!!!!!!\n",
    "                    msp_init = logits.max().item()\n",
    "                    num_cols.append(batch[\"target_col_mask\"].max().item())\n",
    "                    predict_init = logits.argmax().item()\n",
    "                    max_msp = 0\n",
    "                    \n",
    "                    if 1 in target_col_mask and msp_init < uncertainty_threshold:\n",
    "                        init_permutation_i = get_permutation(target_col_mask)\n",
    "                        col_idx_set = target_col_mask.unique().tolist()\n",
    "                        debias_classes = []\n",
    "                        assert -1 not in col_idx_set\n",
    "                        for r in range(len(col_idx_set)-1, len(col_idx_set)//2, -1):\n",
    "                            for subset in itertools.combinations(col_idx_set, r):\n",
    "                                if 0 not in subset and r != 1:\n",
    "                                    continue\n",
    "                                for x in itertools.permutations(subset):\n",
    "                                    if not is_sublist(x, init_permutation_i):\n",
    "                                        continue\n",
    "                                    new_batch_data = []\n",
    "                                    for col_i in x:\n",
    "                                        if col_i == 0:\n",
    "                                            if len(new_batch_data) == 0:\n",
    "                                                cls_indexes_value = 0\n",
    "                                            else:\n",
    "                                                cls_indexes_value = sum([len(new_batch_data[i]) for i in range(len(new_batch_data))])\n",
    "                                        new_batch_data.append(batch[\"data\"].T[target_col_mask==col_i])\n",
    "                                    if 0 not in x:\n",
    "                                        cls_indexes_value = 0\n",
    "                                    new_batch_data = torch.cat(new_batch_data, dim=-1).reshape(1, -1)\n",
    "                                    cls_indexes = torch.tensor([0, cls_indexes_value]).reshape(1, -1).to(device)\n",
    "                                    logits_temp = model(new_batch_data, cls_indexes=cls_indexes,)\n",
    "                                    logits_temp = reweight_logits(logits_temp.detach().cpu(), class_weights)\n",
    "                                    msp_temp = logits_temp.max().item()\n",
    "                                    predict_temp = logits_temp.argmax().item()\n",
    "                                    # if len(x) == 1 and 0 in x:\n",
    "                                    #     predict_target = predict_temp\n",
    "                                    #     msp_target = msp_temp\n",
    "                                    # # print(x, msp_temp, predict_temp)\n",
    "                                    # if 0 not in x and msp_temp > debias_threshold and (predict_temp != predict_target):\n",
    "                                    #     debias_classes.append(predict_temp)\n",
    "                                    #     continue\n",
    "                                    if msp_temp > max_msp and 0 in x and predict_temp not in debias_classes:\n",
    "                                        max_msp = msp_temp\n",
    "                                        best_msp_perm = x\n",
    "                                        msp_predict = predict_temp\n",
    "                                        logits_msp = logits_temp.clone()\n",
    "                    if max_msp > msp_threshold:\n",
    "                        logits = logits_msp.clone()\n",
    "                    labels_test.append(batch[\"label\"].cpu())\n",
    "                    logits_test.append(logits.detach().cpu())\n",
    "                labels_test = torch.cat(labels_test, dim=0)\n",
    "                logits_test = torch.stack(logits_test, dim=0)\n",
    "                preds_test = torch.argmax(logits_test, dim=1)\n",
    "                num_cols = torch.tensor(num_cols)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                from sklearn.metrics import confusion_matrix, f1_score\n",
    "                mask = num_cols > 0\n",
    "                ts_pred_list = logits_test.argmax(\n",
    "                                            1).cpu().detach().numpy().tolist()\n",
    "                ts_micro_f1 = f1_score(labels_test.reshape(-1).numpy().tolist(),\n",
    "                                    ts_pred_list,\n",
    "                                    average=\"micro\")\n",
    "                ts_macro_f1 = f1_score(labels_test.reshape(-1).numpy().tolist(),\n",
    "                                    ts_pred_list,\n",
    "                                    average=\"macro\")\n",
    "                print(\"ts_micro_f1={:.4f}, ts_macro_f1={:.4f}\".format(ts_micro_f1, ts_macro_f1))\n",
    "                ts_pred_list = logits_test.argmax(\n",
    "                                            1).cpu().detach()[mask].numpy().tolist()\n",
    "                ts_micro_f1 = f1_score(labels_test.reshape(-1)[mask].numpy().tolist(),\n",
    "                                    ts_pred_list,\n",
    "                                    average=\"micro\")\n",
    "                ts_macro_f1 = f1_score(labels_test.reshape(-1)[mask].numpy().tolist(),\n",
    "                                    ts_pred_list,\n",
    "                                    average=\"macro\")\n",
    "                print(\"ts_micro_f1={:.4f}, ts_macro_f1={:.4f}\".format(ts_micro_f1, ts_macro_f1))\n",
    "                ts_pred_list = logits_test.argmax(\n",
    "                                            1).cpu().detach()[~mask].numpy().tolist()\n",
    "                ts_micro_f1 = f1_score(labels_test.reshape(-1)[~mask].numpy().tolist(),\n",
    "                                    ts_pred_list,\n",
    "                                    average=\"micro\")\n",
    "                ts_macro_f1 = f1_score(labels_test.reshape(-1)[~mask].numpy().tolist(),\n",
    "                                    ts_pred_list,\n",
    "                                    average=\"macro\")\n",
    "                print(\"ts_micro_f1={:.4f}, ts_macro_f1={:.4f}\".format(ts_micro_f1, ts_macro_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
