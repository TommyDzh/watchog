{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from time import time\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from collections import defaultdict\n",
    "\n",
    "# import pytrec_eval\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss, BCEWithLogitsLoss\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig, AutoTokenizer\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from accelerate import Accelerator\n",
    "\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "from watchog.dataset import (\n",
    "    # collate_fn,\n",
    "    TURLColTypeTablewiseDataset,\n",
    "    TURLRelExtTablewiseDataset,\n",
    "    SatoCVTablewiseDataset,\n",
    "    ColPoplTablewiseDataset\n",
    ")\n",
    "\n",
    "from watchog.dataset import TableDataset, SupCLTableDataset, SemtableCVTablewiseDataset, GittablesTablewiseDataset, GittablesColwiseDataset\n",
    "from watchog.model import BertMultiPairPooler, BertForMultiOutputClassification, BertForMultiOutputClassificationColPopl\n",
    "from watchog.model import SupCLforTable, UnsupCLforTable, lm_mp\n",
    "from watchog.utils import load_checkpoint, f1_score_multilabel, collate_fn, get_col_pred, ColPoplEvaluator\n",
    "from watchog.utils import task_num_class_dict\n",
    "from accelerate import DistributedDataParallelKwargs\n",
    "import wandb\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--wandb\", type=bool, default=False)\n",
    "    parser.add_argument(\"--model\", type=str, default=\"Watchog\")\n",
    "    parser.add_argument(\"--unlabeled_train_only\", type=bool, default=False)\n",
    "    parser.add_argument(\"--context_encoding_type\", type=str, default=\"v0\")\n",
    "    parser.add_argument(\"--pool_version\", type=str, default=\"v0.2\")\n",
    "    parser.add_argument(\"--random_sample\", type=bool, default=False)\n",
    "    parser.add_argument(\"--comment\", type=str, default=\"debug\", help=\"to distinguish the runs\")\n",
    "    parser.add_argument(\n",
    "        \"--shortcut_name\",\n",
    "        default=\"bert-base-uncased\",\n",
    "        type=str,\n",
    "        help=\"Huggingface model shortcut name \",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_length\",\n",
    "        default=64,\n",
    "        type=int,\n",
    "        help=\n",
    "        \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "        \"than this will be truncated, sequences shorter will be padded.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_num_col\",\n",
    "        default=1,\n",
    "        type=int,\n",
    "    )   \n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--batch_size\",\n",
    "        default=16,\n",
    "        type=int,\n",
    "        help=\"Batch size\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--epoch\",\n",
    "        default=1,\n",
    "        type=int,\n",
    "        help=\"Number of epochs for training\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--random_seed\",\n",
    "        default=4649,\n",
    "        type=int,\n",
    "        help=\"Random seed\",\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--train_n_seed_cols\",\n",
    "        default=-1,\n",
    "        type=int,\n",
    "        help=\"number of seeding columns in training\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--num_classes\",\n",
    "        default=78,\n",
    "        type=int,\n",
    "        help=\"Number of classes\",\n",
    "    )\n",
    "    parser.add_argument(\"--multi_gpu\",\n",
    "                        action=\"store_true\",\n",
    "                        default=False,\n",
    "                        help=\"Use multiple GPU\")\n",
    "    parser.add_argument(\"--fp16\",\n",
    "                        action=\"store_true\",\n",
    "                        default=False,\n",
    "                        help=\"Use FP16\")\n",
    "    parser.add_argument(\"--warmup\",\n",
    "                        type=float,\n",
    "                        default=0.,\n",
    "                        help=\"Warmup ratio\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=5e-5, help=\"Learning rate\")\n",
    "    parser.add_argument(\"--task\",\n",
    "                        type=str,\n",
    "                        default='gt-semtab22-dbpedia-all0',\n",
    "                        choices=[\n",
    "                            \"sato0\", \"sato1\", \"sato2\", \"sato3\", \"sato4\",\n",
    "                            \"msato0\", \"msato1\", \"msato2\", \"msato3\", \"msato4\",\n",
    "                            \"gt-dbpedia0\", \"gt-dbpedia1\", \"gt-dbpedia2\", \"gt-dbpedia3\", \"gt-dbpedia4\",\n",
    "                            \"gt-dbpedia-all0\", \"gt-dbpedia-all1\", \"gt-dbpedia-all2\", \"gt-dbpedia-all3\", \"gt-dbpedia-all4\",\n",
    "                            \"gt-schema-all0\", \"gt-schema-all1\", \"gt-schema-all2\", \"gt-schema-all3\", \"gt-schema-all4\",\n",
    "                            \"gt-semtab22-dbpedia\", \"gt-semtab22-dbpedia0\", \"gt-semtab22-dbpedia1\", \"gt-semtab22-dbpedia2\", \"gt-semtab22-dbpedia3\", \"gt-semtab22-dbpedia4\",\n",
    "                            \"gt-semtab22-dbpedia-all\", \"gt-semtab22-dbpedia-all0\", \"gt-semtab22-dbpedia-all1\", \"gt-semtab22-dbpedia-all2\", \"gt-semtab22-dbpedia-all3\", \"gt-semtab22-dbpedia-all4\",\n",
    "                            \"gt-semtab22-schema-class-all\", \"gt-semtab22-schema-property-all\",\n",
    "                            \"turl\", \"turl-re\", \"col-popl-1\", \"col-popl-2\", \"col-popl-3\", \"row-popl\",\n",
    "                            \"col-popl-turl-0\", \"col-popl-turl-1\", \"col-popl-turl-2\",\n",
    "                            \"col-popl-turl-mdonly-0\", \"col-popl-turl-mdonly-1\", \"col-popl-turl-mdonly-2\"\n",
    "                        ],\n",
    "                        help=\"Task names}\")\n",
    "    parser.add_argument(\"--colpair\",\n",
    "                        action=\"store_true\",\n",
    "                        help=\"Use column pair embedding\")\n",
    "    parser.add_argument(\"--metadata\",\n",
    "                        action=\"store_true\",\n",
    "                        help=\"Use column header metadata\")\n",
    "    parser.add_argument(\"--from_scratch\",\n",
    "                        action=\"store_true\",\n",
    "                        help=\"Training from scratch\")\n",
    "    parser.add_argument(\"--cl_tag\",\n",
    "                        type=str,\n",
    "                        default=\"wikitables/simclr/bert_100000_10_32_256_5e-05_sample_row4,sample_row4_tfidf_entity_column_0.05_0_last.pt\",\n",
    "                        help=\"path to the pre-trained file\")\n",
    "    parser.add_argument(\"--dropout_prob\",\n",
    "                        type=float,\n",
    "                        default=0.5)\n",
    "    parser.add_argument(\"--eval_test\",\n",
    "                        action=\"store_true\",\n",
    "                        help=\"evaluate on testset and do not save the model file\")\n",
    "    parser.add_argument(\"--small_tag\",\n",
    "                        type=str,\n",
    "                        default=\"semi1\",\n",
    "                        help=\"e.g., by_table_t5_v1\")\n",
    "    parser.add_argument(\"--data_path\",\n",
    "                        type=str,\n",
    "                        default=\"/data/zhihao/TU/\")\n",
    "    parser.add_argument(\"--pretrained_ckpt_path\",\n",
    "                        type=str,\n",
    "                        default=\"/data/zhihao/TU/Watchog/model/\")    \n",
    "\n",
    "\n",
    "    args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args={\"wandb\": false, \"model\": \"Watchog\", \"unlabeled_train_only\": false, \"context_encoding_type\": \"v0\", \"pool_version\": \"v0.2\", \"random_sample\": false, \"comment\": \"debug\", \"shortcut_name\": \"bert-base-uncased\", \"max_length\": 64, \"max_num_col\": 1, \"batch_size\": 16, \"epoch\": 1, \"random_seed\": 4649, \"train_n_seed_cols\": -1, \"num_classes\": 101, \"multi_gpu\": false, \"fp16\": false, \"warmup\": 0.0, \"lr\": 5e-05, \"task\": \"gt-semtab22-dbpedia-all0\", \"colpair\": false, \"metadata\": false, \"from_scratch\": false, \"cl_tag\": \"wikitables/simclr/bert_100000_10_32_256_5e-05_sample_row4,sample_row4_tfidf_entity_column_0.05_0_last.pt\", \"dropout_prob\": 0.5, \"eval_test\": true, \"small_tag\": \"semi1\", \"data_path\": \"/data/zhihao/TU/\", \"pretrained_ckpt_path\": \"/data/zhihao/TU/Watchog/model/\"}\n"
     ]
    }
   ],
   "source": [
    "    task = args.task\n",
    "    if args.small_tag != \"\":\n",
    "        args.eval_test = True\n",
    "    \n",
    "    args.num_classes = task_num_class_dict[task]\n",
    "    if args.colpair:\n",
    "        assert \"turl-re\" == task, \"colpair can be only used for Relation Extraction\"\n",
    "    if args.metadata:\n",
    "        assert \"turl-re\" == task or \"turl\" == task, \"metadata can be only used for TURL datasets\"\n",
    "    if \"col-popl\":\n",
    "        # metrics = {\n",
    "        #     \"accuracy\": CategoricalAccuracy(tie_break=True),\n",
    "        # }\n",
    "        if args.train_n_seed_cols != -1:\n",
    "            if \"col-popl\" in task:\n",
    "                assert args.train_n_seed_cols == int(task[-1]),  \"# of seed columns must match\"\n",
    "\n",
    "    print(\"args={}\".format(json.dumps(vars(args))))\n",
    "\n",
    "    max_length = args.max_length\n",
    "    batch_size = args.batch_size\n",
    "    num_train_epochs = args.epoch\n",
    "\n",
    "    shortcut_name = args.shortcut_name\n",
    "\n",
    "    if args.colpair and args.metadata:\n",
    "        taskname = \"{}-colpair-metadata\".format(task)\n",
    "    elif args.colpair:\n",
    "        taskname = \"{}-colpair\".format(task)\n",
    "    elif args.metadata:\n",
    "        taskname = \"{}-metadata\".format(task)\n",
    "    elif args.train_n_seed_cols == -1 and 'popl' in task:\n",
    "        taskname = \"{}-mix\".format(task)\n",
    "    else:\n",
    "        taskname = \"\".join(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gt-semtab22-dbpedia-all0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "MAX_LEN = 512 \n",
    "checkpoint = \"meta-llama/Llama-2-7b-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, add_prefix_space=True, cache_dir=\"/data/zhihao/hf\")\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.cls_token = tokenizer.bos_token\n",
    "tokenizer.cls_token_id = tokenizer.bos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 1\n",
      "train 2\n",
      "train 3\n",
      "train 4\n",
      "train 3463\n",
      "valid 1\n",
      "valid 2\n",
      "valid 3\n",
      "valid 4\n",
      "valid 885\n",
      "test\n",
      "test 1085\n"
     ]
    }
   ],
   "source": [
    "src = 'dbpedia'\n",
    "dataset_cls = GittablesColwiseDataset\n",
    "cv = 0\n",
    "max_length = 64\n",
    "# train_dataset = dataset_cls(cv=cv,\n",
    "#                             split=\"train\",\n",
    "#                             src=src,\n",
    "#                             tokenizer=tokenizer,\n",
    "#                             max_length=max_length,\n",
    "#                             gt_only='all' not in task,\n",
    "#                             device=device,\n",
    "#                             base_dirpath=os.path.join(args.data_path, \"GitTables/semtab_gittables/2022\"),\n",
    "#                             small_tag=args.small_tag,\n",
    "#                             max_num_col=1,\n",
    "#                             random_sample=args.random_sample)\n",
    "train_dataset = dataset_cls(cv=cv,\n",
    "                            split=\"train\",\n",
    "                            src=src,\n",
    "                            tokenizer=tokenizer,\n",
    "                            max_length=max_length,\n",
    "                            gt_only='all' not in task,\n",
    "                            device=device,\n",
    "                            base_dirpath=os.path.join(args.data_path, \"GitTables/semtab_gittables/2022\"),\n",
    "                            small_tag=args.small_tag,\n",
    "                            max_num_col=args.max_num_col,\n",
    "                            random_sample=args.random_sample,\n",
    "                            context_encoding_type=args.context_encoding_type)\n",
    "valid_dataset = dataset_cls(cv=cv,\n",
    "                            split=\"valid\", src=src,\n",
    "                            tokenizer=tokenizer,\n",
    "                            max_length=max_length,\n",
    "                            gt_only='all' not in task or args.unlabeled_train_only,\n",
    "                            device=device,\n",
    "                            base_dirpath=os.path.join(args.data_path, \"GitTables/semtab_gittables/2022\"),\n",
    "                            small_tag=args.small_tag,\n",
    "                            max_num_col=args.max_num_col,\n",
    "                            context_encoding_type=args.context_encoding_type\n",
    "                            )\n",
    "test_dataset = dataset_cls(cv=cv,\n",
    "                            split=\"test\", src=src,\n",
    "                            tokenizer=tokenizer,\n",
    "                            max_length=max_length,\n",
    "                            gt_only='all' not in task or args.unlabeled_train_only,\n",
    "                            device=device,\n",
    "                            base_dirpath=os.path.join(args.data_path, \"GitTables/semtab_gittables/2022\"),\n",
    "                            small_tag=args.small_tag,\n",
    "                            max_num_col=args.max_num_col,\n",
    "                            context_encoding_type=args.context_encoding_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>table_id</th>\n",
       "      <th>num_col</th>\n",
       "      <th>data_tensor</th>\n",
       "      <th>label_tensor</th>\n",
       "      <th>cls_indexes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GitTables_100210</td>\n",
       "      <td>0</td>\n",
       "      <td>[tensor(1, device='cuda:0'), tensor(29871, dev...</td>\n",
       "      <td>[tensor(0, device='cuda:0')]</td>\n",
       "      <td>[tensor(0, device='cuda:0')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GitTables_10028</td>\n",
       "      <td>0</td>\n",
       "      <td>[tensor(1, device='cuda:0'), tensor(319, devic...</td>\n",
       "      <td>[tensor(1, device='cuda:0')]</td>\n",
       "      <td>[tensor(0, device='cuda:0')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GitTables_100427</td>\n",
       "      <td>0</td>\n",
       "      <td>[tensor(1, device='cuda:0'), tensor(3086, devi...</td>\n",
       "      <td>[tensor(2, device='cuda:0')]</td>\n",
       "      <td>[tensor(0, device='cuda:0')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GitTables_100434</td>\n",
       "      <td>0</td>\n",
       "      <td>[tensor(1, device='cuda:0'), tensor(402, devic...</td>\n",
       "      <td>[tensor(37, device='cuda:0')]</td>\n",
       "      <td>[tensor(0, device='cuda:0')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GitTables_10050</td>\n",
       "      <td>0</td>\n",
       "      <td>[tensor(1, device='cuda:0'), tensor(29871, dev...</td>\n",
       "      <td>[tensor(0, device='cuda:0')]</td>\n",
       "      <td>[tensor(0, device='cuda:0')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3458</th>\n",
       "      <td>GitTables_63829</td>\n",
       "      <td>0</td>\n",
       "      <td>[tensor(1, device='cuda:0'), tensor(3942, devi...</td>\n",
       "      <td>[tensor(1, device='cuda:0')]</td>\n",
       "      <td>[tensor(0, device='cuda:0')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3459</th>\n",
       "      <td>GitTables_63913</td>\n",
       "      <td>0</td>\n",
       "      <td>[tensor(1, device='cuda:0'), tensor(21521, dev...</td>\n",
       "      <td>[tensor(18, device='cuda:0')]</td>\n",
       "      <td>[tensor(0, device='cuda:0')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3460</th>\n",
       "      <td>GitTables_63913</td>\n",
       "      <td>0</td>\n",
       "      <td>[tensor(1, device='cuda:0'), tensor(3190, devi...</td>\n",
       "      <td>[tensor(3, device='cuda:0')]</td>\n",
       "      <td>[tensor(0, device='cuda:0')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3461</th>\n",
       "      <td>GitTables_64012</td>\n",
       "      <td>0</td>\n",
       "      <td>[tensor(1, device='cuda:0'), tensor(29871, dev...</td>\n",
       "      <td>[tensor(20, device='cuda:0')]</td>\n",
       "      <td>[tensor(0, device='cuda:0')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3462</th>\n",
       "      <td>GitTables_64012</td>\n",
       "      <td>0</td>\n",
       "      <td>[tensor(1, device='cuda:0'), tensor(27208, dev...</td>\n",
       "      <td>[tensor(25, device='cuda:0')]</td>\n",
       "      <td>[tensor(0, device='cuda:0')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3463 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              table_id  num_col  \\\n",
       "0     GitTables_100210        0   \n",
       "1      GitTables_10028        0   \n",
       "2     GitTables_100427        0   \n",
       "3     GitTables_100434        0   \n",
       "4      GitTables_10050        0   \n",
       "...                ...      ...   \n",
       "3458   GitTables_63829        0   \n",
       "3459   GitTables_63913        0   \n",
       "3460   GitTables_63913        0   \n",
       "3461   GitTables_64012        0   \n",
       "3462   GitTables_64012        0   \n",
       "\n",
       "                                            data_tensor  \\\n",
       "0     [tensor(1, device='cuda:0'), tensor(29871, dev...   \n",
       "1     [tensor(1, device='cuda:0'), tensor(319, devic...   \n",
       "2     [tensor(1, device='cuda:0'), tensor(3086, devi...   \n",
       "3     [tensor(1, device='cuda:0'), tensor(402, devic...   \n",
       "4     [tensor(1, device='cuda:0'), tensor(29871, dev...   \n",
       "...                                                 ...   \n",
       "3458  [tensor(1, device='cuda:0'), tensor(3942, devi...   \n",
       "3459  [tensor(1, device='cuda:0'), tensor(21521, dev...   \n",
       "3460  [tensor(1, device='cuda:0'), tensor(3190, devi...   \n",
       "3461  [tensor(1, device='cuda:0'), tensor(29871, dev...   \n",
       "3462  [tensor(1, device='cuda:0'), tensor(27208, dev...   \n",
       "\n",
       "                       label_tensor                   cls_indexes  \n",
       "0      [tensor(0, device='cuda:0')]  [tensor(0, device='cuda:0')]  \n",
       "1      [tensor(1, device='cuda:0')]  [tensor(0, device='cuda:0')]  \n",
       "2      [tensor(2, device='cuda:0')]  [tensor(0, device='cuda:0')]  \n",
       "3     [tensor(37, device='cuda:0')]  [tensor(0, device='cuda:0')]  \n",
       "4      [tensor(0, device='cuda:0')]  [tensor(0, device='cuda:0')]  \n",
       "...                             ...                           ...  \n",
       "3458   [tensor(1, device='cuda:0')]  [tensor(0, device='cuda:0')]  \n",
       "3459  [tensor(18, device='cuda:0')]  [tensor(0, device='cuda:0')]  \n",
       "3460   [tensor(3, device='cuda:0')]  [tensor(0, device='cuda:0')]  \n",
       "3461  [tensor(20, device='cuda:0')]  [tensor(0, device='cuda:0')]  \n",
       "3462  [tensor(25, device='cuda:0')]  [tensor(0, device='cuda:0')]  \n",
       "\n",
       "[3463 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.table_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "# dataset_train = Dataset.from_pandas(train_dataset.table_df[[\"data_tensor\", \"label_tensor\"]])\n",
    "dataset_train = Dataset.from_dict({'input_ids': train_dataset.table_df[\"data_tensor\"].tolist(), \"label\": train_dataset.table_df[\"label_tensor\"].tolist()})\n",
    "dataset_train.set_format(\"torch\")\n",
    "datset_valid = Dataset.from_dict({'input_ids': valid_dataset.table_df[\"data_tensor\"].tolist(), \"label\": valid_dataset.table_df[\"label_tensor\"].tolist()})\n",
    "datset_valid.set_format(\"torch\")\n",
    "dataset_test = Dataset.from_dict({'input_ids': test_dataset.table_df[\"data_tensor\"].tolist(), \"label\": test_dataset.table_df[\"label_tensor\"].tolist()})\n",
    "dataset_test.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset.table_df[\"data_tensor\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([    1, 29871, 29896, 29953, 29941, 29929, 29896, 29945, 29947, 29946,\n",
       "         29953, 29900, 29953, 29941, 29947, 29953, 29945, 29936, 29896, 29945,\n",
       "         29945, 29955, 29896, 29896, 29906, 29900, 29906, 29953, 29941, 29900,\n",
       "         29955, 29955, 29945, 29936, 29906, 29900, 29945, 29941, 29941, 29946,\n",
       "         29953, 29941, 29955, 29900, 29953, 29906, 29906, 29896, 29941, 29936,\n",
       "         29896, 29945, 29945, 29955, 29896, 29896, 29906, 29900, 29906, 29953,\n",
       "         29941, 29900, 29955, 29953]),\n",
       " 'label': tensor([0])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_train[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any, Callable, Dict, List, NewType, Optional, Tuple, Union\n",
    "@dataclass\n",
    "class DoduoCollatorWithPadding:\n",
    "    pad_token_id: int\n",
    "\n",
    "    def __call__(self, samples: List[Dict[str, Any]]) -> Dict[str, Any]: \n",
    "        data = torch.nn.utils.rnn.pad_sequence(\n",
    "            [sample[\"input_ids\"] for sample in samples], padding_value=self.pad_token_id)\n",
    "        label = torch.cat([sample[\"label\"] for sample in samples])\n",
    "        batch = {\"input_ids\": data.T, \"labels\": label}\n",
    "        if \"idx\" in samples[0]:\n",
    "            batch[\"idx\"] = [sample[\"idx\"] for sample in samples]\n",
    "        if \"cls_indexes\" in samples[0]:\n",
    "            cls_indexes = torch.nn.utils.rnn.pad_sequence(\n",
    "                [sample[\"cls_indexes\"] for sample in samples], padding_value=0)\n",
    "            batch[\"cls_indexes\"] = cls_indexes\n",
    "        return batch\n",
    "        \n",
    "data_collator = DoduoCollatorWithPadding(pad_token_id=tokenizer.pad_token_id)\n",
    "# data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    GenerationConfig\n",
    ")\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#           checkpoint, quantization_config=bnb_config, device_map={\"\": 0}, cache_dir=\"/data/zhihao/hf\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.llama.modeling_llama import LlamaPreTrainedModel, LlamaForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import importlib\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "from collections import OrderedDict\n",
    "\n",
    "from transformers.configuration_utils import PretrainedConfig\n",
    "from transformers.dynamic_module_utils import get_class_from_dynamic_module, resolve_trust_remote_code\n",
    "from transformers.utils import (\n",
    "    CONFIG_NAME,\n",
    "    cached_file,\n",
    "    copy_func,\n",
    "    extract_commit_hash,\n",
    "    find_adapter_config_file,\n",
    "    is_peft_available,\n",
    "    logging,\n",
    "    requires_backends,\n",
    ")\n",
    "from transformers.models.auto.configuration_auto import AutoConfig, model_type_to_module_name, replace_list_option_in_docstrings\n",
    "from watchog.llm_model import LlamaForColTypeClassification\n",
    "def load_pretrained_llm(pretrained_model_name_or_path, *model_args, **kwargs):\n",
    "    config = kwargs.pop(\"config\", None)\n",
    "    trust_remote_code = kwargs.pop(\"trust_remote_code\", None)\n",
    "    kwargs[\"_from_auto\"] = True\n",
    "    hub_kwargs_names = [\n",
    "        \"cache_dir\",\n",
    "        \"force_download\",\n",
    "        \"local_files_only\",\n",
    "        \"proxies\",\n",
    "        \"resume_download\",\n",
    "        \"revision\",\n",
    "        \"subfolder\",\n",
    "        \"use_auth_token\",\n",
    "        \"token\",\n",
    "    ]\n",
    "    hub_kwargs = {name: kwargs.pop(name) for name in hub_kwargs_names if name in kwargs}\n",
    "    code_revision = kwargs.pop(\"code_revision\", None)\n",
    "    commit_hash = kwargs.pop(\"_commit_hash\", None)\n",
    "    adapter_kwargs = kwargs.pop(\"adapter_kwargs\", None)\n",
    "\n",
    "    token = hub_kwargs.pop(\"token\", None)\n",
    "    use_auth_token = hub_kwargs.pop(\"use_auth_token\", None)\n",
    "    if use_auth_token is not None:\n",
    "        warnings.warn(\n",
    "            \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\n",
    "            FutureWarning,\n",
    "        )\n",
    "        if token is not None:\n",
    "            raise ValueError(\n",
    "                \"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\"\n",
    "            )\n",
    "        token = use_auth_token\n",
    "\n",
    "    if token is not None:\n",
    "        hub_kwargs[\"token\"] = token\n",
    "\n",
    "    if commit_hash is None:\n",
    "        if not isinstance(config, PretrainedConfig):\n",
    "            # We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\n",
    "            resolved_config_file = cached_file(\n",
    "                pretrained_model_name_or_path,\n",
    "                CONFIG_NAME,\n",
    "                _raise_exceptions_for_gated_repo=False,\n",
    "                _raise_exceptions_for_missing_entries=False,\n",
    "                _raise_exceptions_for_connection_errors=False,\n",
    "                **hub_kwargs,\n",
    "            )\n",
    "            commit_hash = extract_commit_hash(resolved_config_file, commit_hash)\n",
    "        else:\n",
    "            commit_hash = getattr(config, \"_commit_hash\", None)\n",
    "\n",
    "    if is_peft_available():\n",
    "        if adapter_kwargs is None:\n",
    "            adapter_kwargs = {}\n",
    "            if token is not None:\n",
    "                adapter_kwargs[\"token\"] = token\n",
    "\n",
    "        maybe_adapter_path = find_adapter_config_file(\n",
    "            pretrained_model_name_or_path, _commit_hash=commit_hash, **adapter_kwargs\n",
    "        )\n",
    "\n",
    "        if maybe_adapter_path is not None:\n",
    "            with open(maybe_adapter_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                adapter_config = json.load(f)\n",
    "\n",
    "                adapter_kwargs[\"_adapter_model_path\"] = pretrained_model_name_or_path\n",
    "                pretrained_model_name_or_path = adapter_config[\"base_model_name_or_path\"]\n",
    "\n",
    "    if not isinstance(config, PretrainedConfig):\n",
    "        kwargs_orig = copy.deepcopy(kwargs)\n",
    "        # ensure not to pollute the config object with torch_dtype=\"auto\" - since it's\n",
    "        # meaningless in the context of the config object - torch.dtype values are acceptable\n",
    "        if kwargs.get(\"torch_dtype\", None) == \"auto\":\n",
    "            _ = kwargs.pop(\"torch_dtype\")\n",
    "        # to not overwrite the quantization_config if config has a quantization_config\n",
    "        if kwargs.get(\"quantization_config\", None) is not None:\n",
    "            _ = kwargs.pop(\"quantization_config\")\n",
    "\n",
    "        config, kwargs = AutoConfig.from_pretrained(\n",
    "            pretrained_model_name_or_path,\n",
    "            return_unused_kwargs=True,\n",
    "            trust_remote_code=trust_remote_code,\n",
    "            code_revision=code_revision,\n",
    "            _commit_hash=commit_hash,\n",
    "            **hub_kwargs,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        # if torch_dtype=auto was passed here, ensure to pass it on\n",
    "        if kwargs_orig.get(\"torch_dtype\", None) == \"auto\":\n",
    "            kwargs[\"torch_dtype\"] = \"auto\"\n",
    "        if kwargs_orig.get(\"quantization_config\", None) is not None:\n",
    "            kwargs[\"quantization_config\"] = kwargs_orig[\"quantization_config\"]\n",
    "\n",
    "\n",
    "    # Set the adapter kwargs\n",
    "    kwargs[\"adapter_kwargs\"] = adapter_kwargs\n",
    "\n",
    "\n",
    "\n",
    "    model_class = LlamaForColTypeClassification\n",
    "    return model_class.from_pretrained(\n",
    "        pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForSequenceClassification, BitsAndBytesConfig\n",
    "# from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\n",
    "# import torch\n",
    "# compute_dtype = getattr(torch, \"float16\")\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# # bnb_config = BitsAndBytesConfig(\n",
    "# #         load_in_4bit=True,\n",
    "# #         bnb_4bit_quant_type=\"nf4\",\n",
    "# #         bnb_4bit_compute_dtype=compute_dtype,\n",
    "# #         bnb_4bit_use_double_quant=True,\n",
    "# # )\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_8bit=True,  # Enable 8-bit quantization\n",
    "#     bnb_8bit_compute_dtype=compute_dtype,  # Use FP16 for computation\n",
    "#     bnb_8bit_use_double_quant=True\n",
    "# )\n",
    "# model = load_pretrained_llm(\n",
    "#   pretrained_model_name_or_path=checkpoint,\n",
    "#   num_labels=args.num_classes,\n",
    "#   device_map={'': device},\n",
    "#   quantization_config=bnb_config,\n",
    "#   cache_dir=\"/data/zhihao/hf\"\n",
    "# )\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "# #Configure the pad token in the model\n",
    "# model.config.pad_token_id = tokenizer.pad_token_id\n",
    "# model.config.use_cache = False # Gradient checkpointing is used by default but not compatible with caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['bnb_8bit_compute_dtype', 'bnb_8bit_use_double_quant']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8cb10d4399245019bcb7c3566b5eb2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, BitsAndBytesConfig\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\n",
    "import torch\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#         load_in_4bit=True,\n",
    "#         bnb_4bit_quant_type=\"nf4\",\n",
    "#         bnb_4bit_compute_dtype=compute_dtype,\n",
    "#         bnb_4bit_use_double_quant=True,\n",
    "# )\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,  # Enable 8-bit quantization\n",
    "    bnb_8bit_compute_dtype=compute_dtype,  # Use FP16 for computation\n",
    "    bnb_8bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "model =  AutoModelForSequenceClassification.from_pretrained(\n",
    "  pretrained_model_name_or_path=checkpoint,\n",
    "  num_labels=args.num_classes,\n",
    "  device_map={'': device},\n",
    "  quantization_config=bnb_config,\n",
    "  cache_dir=\"/data/zhihao/hf\"\n",
    ")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "#Configure the pad token in the model\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.use_cache = False # Gradient checkpointing is used by default but not compatible with caching\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "peft_config = LoraConfig(\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.1,\n",
    "        r=8,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules= [\"q_proj\",\"v_proj\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 8,802,304 || all params: 6,616,559,616 || trainable%: 0.1330\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS, r=16, lora_alpha=16, lora_dropout=0.05, bias=\"none\", \n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"v_proj\",  \n",
    "    ],\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhihao/jupyterprojects/jupyter/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    # All metrics are already predefined in the HF `evaluate` package\n",
    "    f1_metric= evaluate.load(\"f1\")\n",
    "\n",
    "\n",
    "    logits, labels = eval_pred # eval_pred is the tuple of predictions and labels returned by the model\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    references = labels\n",
    "    macro_f1 = f1_metric.compute(average='macro', predictions=predictions, references=references)\n",
    "    micro_f1 = f1_metric.compute(average='micro', predictions=predictions, references=references)\n",
    "    class_f1 = f1_metric.compute(average=None, predictions=predictions, references=references)\n",
    "    torch.save({\"predictions\": predictions, \"references\": references}, \"./results/llm_eval_pred.pt\")\n",
    "    return {\"macro_f1\": macro_f1, \"micro_f1\": micro_f1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhihao/jupyterprojects/jupyter/lib/python3.8/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/zhihao/jupyterprojects/jupyter/lib/python3.8/site-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "Using auto half precision backend\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import Trainer\n",
    "from torch import inf\n",
    "\n",
    "class WeightedCELossTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        # Get model's predictions\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # Compute custom loss\n",
    "        loss_fct = torch.nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "# model = model.cuda()\n",
    "\n",
    "lr = 1e-4\n",
    "batch_size = 10\n",
    "num_epochs = 2\n",
    "training_args = TrainingArguments(\n",
    "        output_dir=\"/data/zhihao/hf/checkpoints\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        do_eval=True,\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=1,\n",
    "        per_device_eval_batch_size=4,\n",
    "        log_level=\"debug\",\n",
    "        optim= \"adamw_torch\", # \"paged_adamw_32bit\",\n",
    "        # save_strategy=\"no\",  # No checkpoints will be saved\n",
    "        # save_steps=None,  # Explicitly set to None\n",
    "        save_total_limit=1, \n",
    "        load_best_model_at_end=True,  \n",
    "        metric_for_best_model=\"eval_loss\",  # Metric to monitor\n",
    "        greater_is_better=False,         \n",
    "        logging_steps=1, #change to 100\n",
    "        learning_rate=1e-4,\n",
    "        eval_steps=5, #change to 200\n",
    "        fp16=True,\n",
    "        max_grad_norm=0.3,\n",
    "        num_train_epochs=10, # remove \"#\"\n",
    "        # max_steps=10, #remove this\n",
    "        warmup_ratio=0.03,\n",
    "        lr_scheduler_type=\"constant\",\n",
    "        report_to=\"wandb\",\n",
    "        run_name=\"gittables\",\n",
    ")\n",
    "\n",
    "\n",
    "trainer = WeightedCELossTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_train,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset_train,\n",
    "                                batch_size=batch_size,\n",
    "                            #   collate_fn=collate_fn)\n",
    "                            collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 64])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Currently training with a batch size of: 12\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-08-01 03:28:02,632] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/usr/bin/ld: cannot find -laio\n",
      "collect2: error: ld returned 1 exit status\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/zhihao/jupyterprojects/jupyter/lib/python3.8/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(ctx, input, weight, bias=None):\n",
      "/home/zhihao/jupyterprojects/jupyter/lib/python3.8/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 3,463\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 4\n",
      "  Training with DataParallel so batch size has been adjusted to: 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2,890\n",
      "  Number of trainable parameters = 8,802,304\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtommyding\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/zhihao/jupyterprojects/TU/watchog/wandb/run-20240801_032806-zk9t5jbm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tommyding/huggingface/runs/zk9t5jbm' target=\"_blank\">gittables</a></strong> to <a href='https://wandb.ai/tommyding/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tommyding/huggingface' target=\"_blank\">https://wandb.ai/tommyding/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tommyding/huggingface/runs/zk9t5jbm' target=\"_blank\">https://wandb.ai/tommyding/huggingface/runs/zk9t5jbm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhihao/jupyterprojects/jupyter/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/zhihao/jupyterprojects/jupyter/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/zhihao/jupyterprojects/jupyter/lib/python3.8/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='142' max='2890' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 142/2890 02:53 < 56:49, 0.81 it/s, Epoch 0.49/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
