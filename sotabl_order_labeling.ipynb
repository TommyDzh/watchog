{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/zhihao/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/zhihao/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from time import time\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from collections import defaultdict\n",
    "\n",
    "# import pytrec_eval\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss, BCEWithLogitsLoss\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig, AutoTokenizer\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from accelerate import Accelerator\n",
    "from copy import deepcopy\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "from watchog.dataset import (\n",
    "    # collate_fn,\n",
    "    TURLColTypeTablewiseDataset,\n",
    "    TURLRelExtTablewiseDataset,\n",
    "    SatoCVTablewiseDataset,\n",
    "    ColPoplTablewiseDataset,\n",
    "    SotabCVTablewiseDataset\n",
    ")\n",
    "\n",
    "from watchog.dataset import TableDataset, SupCLTableDataset, SemtableCVTablewiseDataset, GittablesColwiseDataset, GittablesTablewiseDataset\n",
    "from watchog.model import BertMultiPairPooler, BertForMultiOutputClassification, BertForMultiOutputClassificationColPopl\n",
    "from watchog.model import SupCLforTable, UnsupCLforTable, lm_mp\n",
    "from watchog.utils import load_checkpoint, f1_score_multilabel, collate_fn, get_col_pred, ColPoplEvaluator\n",
    "from watchog.utils import task_num_class_dict\n",
    "from accelerate import DistributedDataParallelKwargs\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args={\"wandb\": false, \"model\": \"Watchog\", \"unlabeled_train_only\": false, \"context_encoding_type\": \"v0\", \"pool_version\": \"v0.2\", \"random_sample\": false, \"comment\": \"debug\", \"shortcut_name\": \"bert-base-uncased\", \"max_length\": 64, \"adaptive_max_length\": false, \"max_num_col\": 8, \"batch_size\": 16, \"epoch\": 1, \"random_seed\": 4649, \"train_n_seed_cols\": -1, \"num_classes\": 91, \"multi_gpu\": false, \"fp16\": false, \"warmup\": 0.0, \"lr\": 5e-05, \"task\": \"SOTAB\", \"colpair\": false, \"metadata\": false, \"from_scratch\": false, \"cl_tag\": \"wikitables/simclr/bert_100000_10_32_256_5e-05_sample_row4,sample_row4_tfidf_entity_column_0.05_0_last.pt\", \"dropout_prob\": 0.5, \"eval_test\": true, \"small_tag\": \"semi1\", \"data_path\": \"/data/yongkang/TU/\", \"pretrained_ckpt_path\": \"/data/zhihao/TU/Watchog/model/\"}\n",
      "SOTAB/wikitables-simclr-bert_100000_10_32_256_5e-05_sample_row4,sample_row4_tfidf_entity_column_0.05_0_last.pt_bert-base-uncased-semi1-bs16-ml64-ne1-do0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/tmp/ipykernel_2093037/3060214680.py:213: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(ckpt_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(augment_op='sample_row4,sample_row4', batch_size=32, data_path='/data/zhihao/TU/TURL/', fp16=True, gpus='0', lm='bert', logdir='/data/zhihao/TU/Watchog/model/', lr=5e-05, max_len=256, mode='simclr', model='Watchog', n_epochs=10, pretrain_data='wikitables', pretrained_model_path='', projector=768, run_id=0, sample_meth='tfidf_entity', save_model=10, single_column=False, size=100000, table_order='column', temperature=0.05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n"
     ]
    }
   ],
   "source": [
    "    device = torch.device(2)\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--wandb\", type=bool, default=False)\n",
    "    parser.add_argument(\"--model\", type=str, default=\"Watchog\")\n",
    "    parser.add_argument(\"--unlabeled_train_only\", type=bool, default=False)\n",
    "    parser.add_argument(\"--context_encoding_type\", type=str, default=\"v0\")\n",
    "    parser.add_argument(\"--pool_version\", type=str, default=\"v0.2\")\n",
    "    parser.add_argument(\"--random_sample\", type=bool, default=False)\n",
    "    parser.add_argument(\"--comment\", type=str, default=\"debug\", help=\"to distinguish the runs\")\n",
    "    parser.add_argument(\n",
    "        \"--shortcut_name\",\n",
    "        default=\"bert-base-uncased\",\n",
    "        type=str,\n",
    "        help=\"Huggingface model shortcut name \",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_length\",\n",
    "        default=64,\n",
    "        type=int,\n",
    "        help=\n",
    "        \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "        \"than this will be truncated, sequences shorter will be padded.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--adaptive_max_length\",\n",
    "        default=False,\n",
    "        type=bool,\n",
    "    )    \n",
    "    parser.add_argument(\n",
    "        \"--max_num_col\",\n",
    "        default=8,\n",
    "        type=int,\n",
    "    )   \n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--batch_size\",\n",
    "        default=16,\n",
    "        type=int,\n",
    "        help=\"Batch size\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--epoch\",\n",
    "        default=1,\n",
    "        type=int,\n",
    "        help=\"Number of epochs for training\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--random_seed\",\n",
    "        default=4649,\n",
    "        type=int,\n",
    "        help=\"Random seed\",\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--train_n_seed_cols\",\n",
    "        default=-1,\n",
    "        type=int,\n",
    "        help=\"number of seeding columns in training\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--num_classes\",\n",
    "        default=78,\n",
    "        type=int,\n",
    "        help=\"Number of classes\",\n",
    "    )\n",
    "    parser.add_argument(\"--multi_gpu\",\n",
    "                        action=\"store_true\",\n",
    "                        default=False,\n",
    "                        help=\"Use multiple GPU\")\n",
    "    parser.add_argument(\"--fp16\",\n",
    "                        action=\"store_true\",\n",
    "                        default=False,\n",
    "                        help=\"Use FP16\")\n",
    "    parser.add_argument(\"--warmup\",\n",
    "                        type=float,\n",
    "                        default=0.,\n",
    "                        help=\"Warmup ratio\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=5e-5, help=\"Learning rate\")\n",
    "    parser.add_argument(\"--task\",\n",
    "                        type=str,\n",
    "                        default='SOTAB',\n",
    "                        choices=[\n",
    "                            \"sato0\", \"sato1\", \"sato2\", \"sato3\", \"sato4\",\n",
    "                            \"msato0\", \"msato1\", \"msato2\", \"msato3\", \"msato4\",\n",
    "                            \"gt-dbpedia0\", \"gt-dbpedia1\", \"gt-dbpedia2\", \"gt-dbpedia3\", \"gt-dbpedia4\",\n",
    "                            \"gt-dbpedia-all0\", \"gt-dbpedia-all1\", \"gt-dbpedia-all2\", \"gt-dbpedia-all3\", \"gt-dbpedia-all4\",\n",
    "                            \"gt-schema-all0\", \"gt-schema-all1\", \"gt-schema-all2\", \"gt-schema-all3\", \"gt-schema-all4\",\n",
    "                            \"gt-semtab22-dbpedia\", \"gt-semtab22-dbpedia0\", \"gt-semtab22-dbpedia1\", \"gt-semtab22-dbpedia2\", \"gt-semtab22-dbpedia3\", \"gt-semtab22-dbpedia4\",\n",
    "                            \"gt-semtab22-dbpedia-all\", \"gt-semtab22-dbpedia-all0\", \"gt-semtab22-dbpedia-all1\", \"gt-semtab22-dbpedia-all2\", \"gt-semtab22-dbpedia-all3\", \"gt-semtab22-dbpedia-all4\",\n",
    "                            \"gt-semtab22-schema-class-all\", \"gt-semtab22-schema-property-all\",\n",
    "                            \"turl\", \"turl-re\", \"col-popl-1\", \"col-popl-2\", \"col-popl-3\", \"row-popl\",\n",
    "                            \"col-popl-turl-0\", \"col-popl-turl-1\", \"col-popl-turl-2\",\n",
    "                            \"col-popl-turl-mdonly-0\", \"col-popl-turl-mdonly-1\", \"col-popl-turl-mdonly-2\"\n",
    "                        ],\n",
    "                        help=\"Task names}\")\n",
    "    parser.add_argument(\"--colpair\",\n",
    "                        action=\"store_true\",\n",
    "                        help=\"Use column pair embedding\")\n",
    "    parser.add_argument(\"--metadata\",\n",
    "                        action=\"store_true\",\n",
    "                        help=\"Use column header metadata\")\n",
    "    parser.add_argument(\"--from_scratch\",\n",
    "                        action=\"store_true\",\n",
    "                        help=\"Training from scratch\")\n",
    "    parser.add_argument(\"--cl_tag\",\n",
    "                        type=str,\n",
    "                        default=\"wikitables/simclr/bert_100000_10_32_256_5e-05_sample_row4,sample_row4_tfidf_entity_column_0.05_0_last.pt\",\n",
    "                        help=\"path to the pre-trained file\")\n",
    "    parser.add_argument(\"--dropout_prob\",\n",
    "                        type=float,\n",
    "                        default=0.5)\n",
    "    parser.add_argument(\"--eval_test\",\n",
    "                        action=\"store_true\",\n",
    "                        help=\"evaluate on testset and do not save the model file\")\n",
    "    parser.add_argument(\"--small_tag\",\n",
    "                        type=str,\n",
    "                        default=\"semi1\",\n",
    "                        help=\"e.g., by_table_t5_v1\")\n",
    "    parser.add_argument(\"--data_path\",\n",
    "                        type=str,\n",
    "                        default=\"/data/yongkang/TU/\")\n",
    "    parser.add_argument(\"--pretrained_ckpt_path\",\n",
    "                        type=str,\n",
    "                        default=\"/data/zhihao/TU/Watchog/model/\")    \n",
    "\n",
    "    args = parser.parse_args([])\n",
    "    task = args.task\n",
    "    if args.small_tag != \"\":\n",
    "        args.eval_test = True\n",
    "    \n",
    "    args.num_classes = task_num_class_dict[task]\n",
    "    if args.colpair:\n",
    "        assert \"turl-re\" == task, \"colpair can be only used for Relation Extraction\"\n",
    "    if args.metadata:\n",
    "        assert \"turl-re\" == task or \"turl\" == task, \"metadata can be only used for TURL datasets\"\n",
    "    if \"col-popl\":\n",
    "        # metrics = {\n",
    "        #     \"accuracy\": CategoricalAccuracy(tie_break=True),\n",
    "        # }\n",
    "        if args.train_n_seed_cols != -1:\n",
    "            if \"col-popl\" in task:\n",
    "                assert args.train_n_seed_cols == int(task[-1]),  \"# of seed columns must match\"\n",
    "\n",
    "    print(\"args={}\".format(json.dumps(vars(args))))\n",
    "\n",
    "    max_length = args.max_length\n",
    "    batch_size = args.batch_size\n",
    "    num_train_epochs = args.epoch\n",
    "\n",
    "    shortcut_name = args.shortcut_name\n",
    "\n",
    "    if args.colpair and args.metadata:\n",
    "        taskname = \"{}-colpair-metadata\".format(task)\n",
    "    elif args.colpair:\n",
    "        taskname = \"{}-colpair\".format(task)\n",
    "    elif args.metadata:\n",
    "        taskname = \"{}-metadata\".format(task)\n",
    "    elif args.train_n_seed_cols == -1 and 'popl' in task:\n",
    "        taskname = \"{}-mix\".format(task)\n",
    "    else:\n",
    "        taskname = \"\".join(task)\n",
    "\n",
    "    if args.from_scratch:\n",
    "        if \"gt\" in task:\n",
    "            tag_name = \"{}/{}-{}-{}-pool{}-max_cols{}-rand{}-bs{}-ml{}-ne{}-do{}{}\".format(\n",
    "                taskname,  \"{}-fromscratch\".format(shortcut_name), args.small_tag, args.comment, args.pool_version, args.max_num_col, args.random_sample,\n",
    "                batch_size, max_length, num_train_epochs, args.dropout_prob, \n",
    "                '-rs{}'.format(args.random_seed) if args.random_seed != 4649 else '')\n",
    "        else:\n",
    "            tag_name = \"{}/{}-{}-{}-bs{}-ml{}-ne{}-do{}{}\".format(\n",
    "                taskname,  \"{}-fromscratch\".format(shortcut_name), args.small_tag, args.comment, \n",
    "                batch_size, max_length, num_train_epochs, args.dropout_prob, \n",
    "                '-rs{}'.format(args.random_seed) if args.random_seed != 4649 else '')\n",
    "        \n",
    "    else:\n",
    "        if \"gt\" in task:\n",
    "            tag_name = \"{}/{}_{}-pool{}-max_cols{}-rand{}-bs{}-ml{}-ne{}-do{}{}\".format(\n",
    "                taskname, args.cl_tag.replace('/', '-'),  shortcut_name, args.small_tag, args.pool_version, args.max_num_col, args.random_sample,\n",
    "                batch_size, max_length, num_train_epochs, args.dropout_prob,\n",
    "                '-rs{}'.format(args.random_seed) if args.random_seed != 4649 else '')\n",
    "        else:\n",
    "            tag_name = \"{}/{}_{}-{}-bs{}-ml{}-ne{}-do{}{}\".format(\n",
    "                taskname, args.cl_tag.replace('/', '-'),  shortcut_name, args.small_tag,\n",
    "                batch_size, max_length, num_train_epochs, args.dropout_prob,\n",
    "                '-rs{}'.format(args.random_seed) if args.random_seed != 4649 else '')\n",
    "\n",
    "    # if args.eval_test:\n",
    "    #     if args.small_tag != '':\n",
    "    #         tag_name = tag_name.replace('outputs', 'small_outputs')\n",
    "    #         tag_name += '-' + args.small_tag\n",
    "    print(tag_name)\n",
    "    file_path = os.path.join(args.data_path, \"Watchog\", \"outputs\", tag_name)\n",
    "\n",
    "    dirpath = os.path.dirname(file_path)\n",
    "    if not os.path.exists(dirpath):\n",
    "        print(\"{} not exists. Created\".format(dirpath))\n",
    "        os.makedirs(dirpath)\n",
    "    \n",
    "    if args.fp16:\n",
    "        torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = True\n",
    "        \n",
    "      \n",
    "        \n",
    "    # accelerator = Accelerator(mixed_precision=\"no\" if not args.fp16 else \"fp16\")   \n",
    "    ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)\n",
    "    accelerator = Accelerator(mixed_precision=\"no\" if not args.fp16 else \"fp16\", kwargs_handlers=[ddp_kwargs])\n",
    "\n",
    "    device = torch.device(2)\n",
    "    ckpt_path = os.path.join(args.pretrained_ckpt_path, args.cl_tag)\n",
    "    # ckpt_path = '/efs/checkpoints/{}.pt'.format(args.cl_tag)\n",
    "    ckpt = torch.load(ckpt_path, map_location=device)\n",
    "    ckpt_hp = ckpt['hp']\n",
    "    print(ckpt_hp)\n",
    " \n",
    "    setattr(ckpt_hp, 'batch_size', args.batch_size)\n",
    "    setattr(ckpt_hp, 'hidden_dropout_prob', args.dropout_prob)\n",
    "    setattr(ckpt_hp, 'shortcut_name', args.shortcut_name)\n",
    "    setattr(ckpt_hp, 'num_labels', args.num_classes)\n",
    "    \n",
    "    \n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained(shortcut_name)\n",
    "    padder = collate_fn(tokenizer.pad_token_id)\n",
    "    if task == \"turl-re\" and args.colpair:\n",
    "        model = BertForMultiOutputClassification(ckpt_hp, device=device, lm=ckpt['hp'].lm, col_pair='Pair')\n",
    "    elif \"col-popl\" in task:\n",
    "        model = BertForMultiOutputClassificationColPopl(ckpt_hp, device=device, lm=ckpt['hp'].lm, n_seed_cols=int(task[i][-1]), cls_for_md=\"md\" in task)\n",
    "    else:\n",
    "        model = BertForMultiOutputClassification(ckpt_hp, device=device, lm=ckpt['hp'].lm, version=\"v0\", use_attention_mask=True)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2093037/1786997412.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  best_state_dict = torch.load(\"/data/yongkang/TU/Watchog/outputs/SOTAB/bert-base-uncased-fromscratch-comma-bs16-ml128-ne50-do0.5_fully_deduplicated_best_f1_micro.pt\", map_location=device)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_state_dict = torch.load(\"/data/yongkang/TU/Watchog/outputs/SOTAB/bert-base-uncased-fromscratch-comma-bs16-ml128-ne50-do0.5_fully_deduplicated_best_f1_micro.pt\", map_location=device)\n",
    "model.load_state_dict(best_state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/data/yongkang/TU/SOTAB/comma_train_filter_sotab.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_group = df.groupby('table_id')\n",
    "num_cols = []\n",
    "num_cols_gt = []\n",
    "for table_id, group in df_group:\n",
    "    num_cols.append(len(group))\n",
    "    num_cols_gt.append(len(group[group['label']>-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "print(min(num_cols), max(num_cols), np.mean(num_cols), np.median(num_cols))\n",
    "sns.histplot(num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 29 2.8656768255622125 2.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: ylabel='Count'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "print(min(num_cols_gt), max(num_cols_gt), np.mean(num_cols_gt), np.median(num_cols_gt))\n",
    "sns.histplot(num_cols_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cls_tokens(hidden_states, cls_indexes, head=False):\n",
    "    cls_embeddings = []\n",
    "    for i, j in cls_indexes:\n",
    "        sub_sentence_cls_embeddings = hidden_states[i, 0, :] if head else hidden_states[i, j, :]\n",
    "        cls_embeddings.append(sub_sentence_cls_embeddings)\n",
    "    cls_embeddings = torch.stack(cls_embeddings)\n",
    "    return cls_embeddings\n",
    "#pooled_outputs = extract_cls_tokens(hidden_states, cls_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import re\n",
    "import transformers\n",
    "from torch.utils import data\n",
    "from torch.nn.utils import rnn\n",
    "from transformers import AutoTokenizer\n",
    "from typing import List\n",
    "from functools import reduce\n",
    "import operator\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "class SOTABTablewiseIterateDataset(data.Dataset):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            # cv: int,\n",
    "            split: str,  # train or test\n",
    "            tokenizer: AutoTokenizer,\n",
    "            max_length: int = 256,\n",
    "            # multicol_only: bool = False,\n",
    "            train_ratio: float = 1.0,\n",
    "            device: torch.device = None,\n",
    "            base_dirpath: str = \"/data/yongkang/TU/SOTAB\",\n",
    "            small_tag: str = \"\",\n",
    "            label_encoder: LabelEncoder = None,\n",
    "            max_unlabeled=8,\n",
    "            random_sample=False, # TODO\n",
    "            gt_only=True,\n",
    "            ):\n",
    "        # ?\n",
    "        if device is None:\n",
    "            device = torch.device('cpu')\n",
    "\n",
    "        assert split in [\"train\", \"valid\", \"test\"]\n",
    "        if split == \"train\":\n",
    "            gt_df = pd.read_csv(os.path.join(base_dirpath, \"CTA_training_small_gt.csv\"))\n",
    "            data_folder = \"Train\"\n",
    "        elif split == \"valid\":\n",
    "            gt_df = pd.read_csv(os.path.join(base_dirpath, \"CTA_validation_gt.csv\"))\n",
    "            data_folder = \"Validation\"\n",
    "        else:  # test\n",
    "            gt_df = pd.read_csv(os.path.join(base_dirpath, \"CTA_test_gt.csv\"))\n",
    "            data_folder = \"Test\"\n",
    "\n",
    "        # 初始化或加载 LabelEncoder\n",
    "        if label_encoder is None:\n",
    "            label_encoder = LabelEncoder()\n",
    "            label_encoder.fit(gt_df['label'])\n",
    "        self.label_encoder = label_encoder\n",
    "\n",
    "        # gt_set = set(zip(gt_df[\"table_name\"], gt_df[\"column_index\"]))\n",
    "\n",
    "        table_files = [f for f in os.listdir(os.path.join(base_dirpath, data_folder)) if f in gt_df['table_name'].values]\n",
    "\n",
    "        mapping_file_path = os.path.join(base_dirpath, \"label_mapping.txt\")\n",
    "        df_csv_path = os.path.join(base_dirpath, f\"comma_{split}_fully_deduplicated_sotab.csv\")\n",
    "\n",
    "        # 检查是否存在之前保存的标签映射关系\n",
    "        if os.path.exists(mapping_file_path):\n",
    "            # 如果存在，直接从文件中读取映射\n",
    "            with open(mapping_file_path, 'r') as f:\n",
    "                label_mapping = json.load(f)\n",
    "            next_label_id = max(label_mapping.values()) + 1\n",
    "            print(f\"标签映射关系从 {mapping_file_path} 读取成功\")\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"{mapping_file_path} 文件不存在，请确认文件路径\")\n",
    "\n",
    "        # 检查 CSV 文件是否存在\n",
    "        if os.path.exists(df_csv_path):\n",
    "            # 直接从 CSV 文件中读取数据\n",
    "            df = pd.read_csv(df_csv_path)\n",
    "            print(f\"数据已从 {df_csv_path} 加载\")\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"{df_csv_path} 文件不存在，请确认文件路径\")\n",
    "        \n",
    "        if gt_only:\n",
    "            df = df[df[\"label\"] > -1]\n",
    "\n",
    "        data_list = []\n",
    "\n",
    "        df['label'] = df['label'].astype(int)\n",
    "        df.drop(df[(df['data'].isna()) & (df['label'] == -1)].index, inplace=True)\n",
    "        df['column_index'] = df['column_index'].astype(int)\n",
    "        df['data'] = df['data'].astype(str)\n",
    "\n",
    "        print(\"start encoder\")\n",
    "        for i, (index, group_df) in enumerate(df.groupby(\"table_id\")):\n",
    "            # if split == \"train\" and ((i >= num_train) or (i >= valid_index)):\n",
    "            #     break\n",
    "            # if split == \"valid\" and i < valid_index:\n",
    "            #     continue\n",
    "\n",
    "            labeled_columns = group_df[group_df['label'] > -1]\n",
    "            unlabeled_columns = group_df[group_df['label'] == -1]\n",
    "            num_unlabeled = min(max(max_unlabeled-len(labeled_columns), 0), len(unlabeled_columns))\n",
    "            unlabeled_columns = unlabeled_columns.sample(num_unlabeled) if random_sample else unlabeled_columns[0:num_unlabeled]\n",
    "            group_df = pd.concat([group_df[group_df['label'] > -1], unlabeled_columns]) # TODO\n",
    "            group_df.sort_values(by=['column_index'], inplace=True)\n",
    "\n",
    "            num_labels = len(list(group_df[\"label\"].values))\n",
    "            if max_length <= 128:\n",
    "                cur_maxlen = min(max_length, 512 // num_labels - 1)\n",
    "            else:\n",
    "                cur_maxlen = max(1, max_length // num_labels - 1)\n",
    "\n",
    "            token_ids_list = group_df[\"data\"].apply(lambda x: tokenizer.encode(\n",
    "                tokenizer.cls_token + \" \" + x, add_special_tokens=False, max_length=cur_maxlen, truncation=True)).tolist(\n",
    "                )\n",
    "            token_ids = torch.LongTensor(reduce(operator.add,\n",
    "                                                token_ids_list)).to(device)\n",
    "            for col_i in range(len(token_ids_list)):\n",
    "                if group_df[\"label\"].values[col_i] == -1:\n",
    "                    continue\n",
    "                target_col_mask = []\n",
    "                cls_index_value = 0\n",
    "                context_id = 1\n",
    "                new_token_ids_list = []\n",
    "                for col_j in range(len(token_ids_list)):\n",
    "                    if len(set(target_col_mask)) == max_unlabeled-1 and 0 not in target_col_mask and col_j != col_i:\n",
    "                        # skip the rest of the columns until the target one\n",
    "                        continue\n",
    "                    \n",
    "                    if col_j == col_i:\n",
    "                        target_col_mask += [0] * len(token_ids_list[col_j])\n",
    "                    else:\n",
    "                        target_col_mask += [context_id] * len(token_ids_list[col_j])\n",
    "                        context_id += 1\n",
    "                    if col_j < col_i:\n",
    "                        cls_index_value += len(token_ids_list[col_j])\n",
    "                    new_token_ids_list.append(token_ids_list[col_j])\n",
    "                    if len(set(target_col_mask)) == max_unlabeled and 0 in target_col_mask:\n",
    "                        break\n",
    "                new_token_ids_list = torch.LongTensor(reduce(operator.add,\n",
    "                                                new_token_ids_list)).to(device)\n",
    "                cls_index_list = [cls_index_value] \n",
    "                for cls_index in cls_index_list:\n",
    "                    assert token_ids[\n",
    "                        cls_index] == tokenizer.cls_token_id, \"cls_indexes validation\"\n",
    "                cls_indexes = torch.LongTensor(cls_index_list).to(device)\n",
    "                class_ids = torch.LongTensor(\n",
    "                    [group_df[\"label\"].values[col_i]]).to(device)\n",
    "                target_col_mask = torch.LongTensor(target_col_mask).to(device)\n",
    "                data_list.append(\n",
    "                    [index,\n",
    "                    len(group_df), new_token_ids_list, class_ids, cls_indexes, target_col_mask])                \n",
    "        print(split, len(data_list))\n",
    "        self.table_df = pd.DataFrame(data_list,\n",
    "                                     columns=[\n",
    "                                         \"table_id\", \"num_col\", \"data_tensor\",\n",
    "                                         \"label_tensor\", \"cls_indexes\", \"target_col_mask\"\n",
    "                                     ])\n",
    "        \"\"\"\n",
    "        # NOTE: msato contains a small portion of single-col tables. keep it to be consistent.  \n",
    "        if multicol_only:\n",
    "            # Check\n",
    "            num_all_tables = len(self.table_df)\n",
    "            self.table_df = self.table_df[self.table_df[\"num_col\"] > 1]\n",
    "            assert len(self.table_df) == num_all_tables\n",
    "        \"\"\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.table_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"data\": self.table_df.iloc[idx][\"data_tensor\"],\n",
    "            \"label\": self.table_df.iloc[idx][\"label_tensor\"],\n",
    "            \"cls_indexes\": self.table_df.iloc[idx][\"cls_indexes\"],\n",
    "            \"target_col_mask\": self.table_df.iloc[idx][\"target_col_mask\"],\n",
    "        }\n",
    "        #\"idx\": torch.LongTensor([idx])}\n",
    "        #\"cls_indexes\": self.table_df.iloc[idx][\"cls_indexes\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(pad_token_id, data_only=True):\n",
    "    '''padder for input batch'''\n",
    "\n",
    "    def padder(samples):    \n",
    "        data = torch.nn.utils.rnn.pad_sequence(\n",
    "            [sample[\"data\"] for sample in samples], padding_value=pad_token_id)\n",
    "        if not data_only:\n",
    "            label = torch.nn.utils.rnn.pad_sequence(\n",
    "                [sample[\"label\"] for sample in samples], padding_value=-1)\n",
    "        else:\n",
    "            label = torch.cat([sample[\"label\"] for sample in samples])\n",
    "        batch = {\"data\": data, \"label\": label}\n",
    "        if \"idx\" in samples[0]:\n",
    "            batch[\"idx\"] = [sample[\"idx\"] for sample in samples]\n",
    "        if \"cls_indexes\" in samples[0]:\n",
    "            cls_indexes = torch.nn.utils.rnn.pad_sequence(\n",
    "                [sample[\"cls_indexes\"] for sample in samples], padding_value=0)\n",
    "            batch[\"cls_indexes\"] = cls_indexes\n",
    "        if \"target_col_mask\" in samples[0]:\n",
    "            target_col_mask = torch.nn.utils.rnn.pad_sequence(\n",
    "                [sample[\"target_col_mask\"] for sample in samples], padding_value=-1)\n",
    "            batch[\"target_col_mask\"] = target_col_mask\n",
    "        if \"table_embedding\" in samples[0]:\n",
    "            table_embeddings = [sample[\"table_embedding\"] for sample in samples]\n",
    "            batch[\"table_embedding\"] = torch.stack(table_embeddings, dim=0)\n",
    "        return batch\n",
    "        \n",
    "    return padder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "标签映射关系从 /data/yongkang/TU/SOTAB/label_mapping.txt 读取成功\n",
      "数据已从 /data/yongkang/TU/SOTAB/comma_test_fully_deduplicated_sotab.csv 加载\n",
      "start encoder\n",
      "test 15040\n"
     ]
    }
   ],
   "source": [
    "src = None\n",
    "max_unlabeled=8\n",
    "test_dataset_iter = SOTABTablewiseIterateDataset(# cv=cv,\n",
    "                                        split=\"test\",\n",
    "                                        tokenizer=tokenizer,\n",
    "                                        max_length=max_length,\n",
    "                                        # multicol_only=multicol_only,\n",
    "                                        device=device,\n",
    "                                        gt_only=False,\n",
    "                                        base_dirpath=os.path.join(args.data_path, \"SOTAB\"),\n",
    "                                        max_unlabeled=max_unlabeled\n",
    "                                        )\n",
    "padder = collate_fn(tokenizer.pad_token_id)\n",
    "test_dataloader_iter = DataLoader(test_dataset_iter,\n",
    "                                batch_size=1,\n",
    "                            #   collate_fn=collate_fn)\n",
    "                            collate_fn=padder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols_test = []\n",
    "for batch_idx, batch in enumerate(test_dataloader_iter):\n",
    "    target_col_mask = batch[\"target_col_mask\"].T\n",
    "    init_permutation_i = get_permutation(target_col_mask)\n",
    "    num_cols_test.append(len(init_permutation_i))\n",
    "sns.histplot(num_cols_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "标签映射关系从 /data/yongkang/TU/SOTAB/label_mapping.txt 读取成功\n",
      "数据已从 /data/yongkang/TU/SOTAB/comma_valid_fully_deduplicated_sotab.csv 加载\n",
      "start encoder\n",
      "valid 16840\n"
     ]
    }
   ],
   "source": [
    "src = None\n",
    "max_unlabeled=8\n",
    "valid_dataset_iter = SOTABTablewiseIterateDataset(# cv=cv,\n",
    "                                            split=\"valid\",\n",
    "                                            tokenizer=tokenizer,\n",
    "                                            max_length=max_length,\n",
    "                                            # multicol_only=multicol_only,\n",
    "                                            device=device,\n",
    "                                            gt_only=False,\n",
    "                                            base_dirpath=os.path.join(args.data_path, \"SOTAB\"),\n",
    "                                            max_unlabeled=max_unlabeled\n",
    "                                            )\n",
    "padder = collate_fn(tokenizer.pad_token_id)\n",
    "valid_dataloader_iter = DataLoader(valid_dataset_iter,\n",
    "                                batch_size=1,\n",
    "                            #   collate_fn=collate_fn)\n",
    "                            collate_fn=padder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "标签映射关系从 /data/yongkang/TU/SOTAB/label_mapping.txt 读取成功\n",
      "数据已从 /data/yongkang/TU/SOTAB/comma_train_fully_deduplicated_sotab.csv 加载\n",
      "start encoder\n",
      "train 33004\n"
     ]
    }
   ],
   "source": [
    "train_dataset_iter = SOTABTablewiseIterateDataset(\n",
    "                            split=\"train\", \n",
    "                            tokenizer=tokenizer,\n",
    "                            max_length=max_length,\n",
    "                            gt_only=False,\n",
    "                            device=device,\n",
    "                            base_dirpath=os.path.join(args.data_path, \"SOTAB\"))\n",
    "padder = collate_fn(tokenizer.pad_token_id)\n",
    "train_dataloader_iter = DataLoader(train_dataset_iter,\n",
    "                                batch_size=1,\n",
    "                            #   collate_fn=collate_fn)\n",
    "                            collate_fn=padder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ts_micro_f1=0.8701, ts_macro_f1=0.8594\n",
      "ts_micro_f1=0.8701, ts_macro_f1=0.8594\n",
      "ts_micro_f1=0.0000, ts_macro_f1=nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhihao/jupyterprojects/jupyter/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/home/zhihao/jupyterprojects/jupyter/lib/python3.8/site-packages/numpy/lib/function_base.py:495: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis)\n",
      "/home/zhihao/jupyterprojects/jupyter/lib/python3.8/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "# strict truncate max_num_cols\n",
    "model.load_state_dict(best_state_dict, strict=False)\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "ft_embs_test = []\n",
    "labels_test = []\n",
    "logits_test = []\n",
    "num_cols = []\n",
    "for batch_idx, batch in enumerate(test_dataloader_iter):\n",
    "    cls_indexes = torch.LongTensor([[0, batch[\"cls_indexes\"].cpu().item()]]).to(device)\n",
    "    target_col_mask = batch[\"target_col_mask\"].T\n",
    "    logits = model(batch[\"data\"].T, cls_indexes=cls_indexes,)\n",
    "    labels_test.append(batch[\"label\"].cpu())\n",
    "    logits_test.append(logits.detach().cpu())\n",
    "    num_cols.append(batch[\"target_col_mask\"].max().item())\n",
    "labels_test = torch.cat(labels_test, dim=0)\n",
    "logits_test = torch.stack(logits_test, dim=0)\n",
    "preds_test = torch.argmax(logits_test, dim=1)\n",
    "num_cols = torch.tensor(num_cols)\n",
    " \n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "mask = num_cols > 0\n",
    "ts_pred_list = logits_test.argmax(\n",
    "                            1).cpu().detach().numpy().tolist()\n",
    "ts_micro_f1 = f1_score(labels_test.reshape(-1).numpy().tolist(),\n",
    "                    ts_pred_list,\n",
    "                    average=\"micro\")\n",
    "ts_macro_f1 = f1_score(labels_test.reshape(-1).numpy().tolist(),\n",
    "                    ts_pred_list,\n",
    "                    average=\"macro\")\n",
    "print(\"ts_micro_f1={:.4f}, ts_macro_f1={:.4f}\".format(ts_micro_f1, ts_macro_f1))\n",
    "ts_pred_list = logits_test.argmax(\n",
    "                            1).cpu().detach()[mask].numpy().tolist()\n",
    "ts_micro_f1 = f1_score(labels_test.reshape(-1)[mask].numpy().tolist(),\n",
    "                    ts_pred_list,\n",
    "                    average=\"micro\")\n",
    "ts_macro_f1 = f1_score(labels_test.reshape(-1)[mask].numpy().tolist(),\n",
    "                    ts_pred_list,\n",
    "                    average=\"macro\")\n",
    "print(\"ts_micro_f1={:.4f}, ts_macro_f1={:.4f}\".format(ts_micro_f1, ts_macro_f1))\n",
    "ts_pred_list = logits_test.argmax(\n",
    "                            1).cpu().detach()[~mask].numpy().tolist()\n",
    "ts_micro_f1 = f1_score(labels_test.reshape(-1)[~mask].numpy().tolist(),\n",
    "                    ts_pred_list,\n",
    "                    average=\"micro\")\n",
    "ts_macro_f1 = f1_score(labels_test.reshape(-1)[~mask].numpy().tolist(),\n",
    "                    ts_pred_list,\n",
    "                    average=\"macro\")\n",
    "print(\"ts_micro_f1={:.4f}, ts_macro_f1={:.4f}\".format(ts_micro_f1, ts_macro_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================max_num_cols=4===============================\n",
      "标签映射关系从 /data/yongkang/TU/SOTAB/label_mapping.txt 读取成功\n",
      "数据已从 /data/yongkang/TU/SOTAB/comma_test_fully_deduplicated_sotab.csv 加载\n",
      "start encoder\n",
      "test 15040\n",
      "ts_micro_f1=0.7612, ts_macro_f1=0.7696\n",
      "ts_micro_f1=0.7612, ts_macro_f1=0.7696\n",
      "ts_micro_f1=0.0000, ts_macro_f1=nan\n",
      "=============================max_num_cols=16===============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhihao/jupyterprojects/jupyter/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/home/zhihao/jupyterprojects/jupyter/lib/python3.8/site-packages/numpy/lib/function_base.py:495: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis)\n",
      "/home/zhihao/jupyterprojects/jupyter/lib/python3.8/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "标签映射关系从 /data/yongkang/TU/SOTAB/label_mapping.txt 读取成功\n",
      "数据已从 /data/yongkang/TU/SOTAB/comma_test_fully_deduplicated_sotab.csv 加载\n",
      "start encoder\n",
      "test 15040\n",
      "ts_micro_f1=0.8735, ts_macro_f1=0.8627\n",
      "ts_micro_f1=0.8735, ts_macro_f1=0.8627\n",
      "ts_micro_f1=0.0000, ts_macro_f1=nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhihao/jupyterprojects/jupyter/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/home/zhihao/jupyterprojects/jupyter/lib/python3.8/site-packages/numpy/lib/function_base.py:495: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis)\n",
      "/home/zhihao/jupyterprojects/jupyter/lib/python3.8/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "# strict truncate max_num_cols\n",
    "for max_num_cols in [4, 16]:\n",
    "    print(\"=============================max_num_cols={}===============================\".format(max_num_cols))\n",
    "    src = None\n",
    "    test_dataset_iter = SOTABTablewiseIterateDataset(# cv=cv,\n",
    "                                            split=\"test\",\n",
    "                                            tokenizer=tokenizer,\n",
    "                                            max_length=max_length,\n",
    "                                            # multicol_only=multicol_only,\n",
    "                                            device=device,\n",
    "                                            gt_only=False,\n",
    "                                            base_dirpath=os.path.join(args.data_path, \"SOTAB\"),\n",
    "                                            max_unlabeled=max_num_cols\n",
    "                                            )\n",
    "    padder = collate_fn(tokenizer.pad_token_id)\n",
    "    test_dataloader_iter = DataLoader(test_dataset_iter,\n",
    "                                    batch_size=1,\n",
    "                                #   collate_fn=collate_fn)\n",
    "                                collate_fn=padder)\n",
    "    \n",
    "    \n",
    "    model.load_state_dict(best_state_dict, strict=False)\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    ft_embs_test = []\n",
    "    labels_test = []\n",
    "    logits_test = []\n",
    "    num_cols = []\n",
    "    for batch_idx, batch in enumerate(test_dataloader_iter):\n",
    "        cls_indexes = torch.LongTensor([[0, batch[\"cls_indexes\"].cpu().item()]]).to(device)\n",
    "        target_col_mask = batch[\"target_col_mask\"].T\n",
    "        logits = model(batch[\"data\"].T, cls_indexes=cls_indexes,)\n",
    "        labels_test.append(batch[\"label\"].cpu())\n",
    "        logits_test.append(logits.detach().cpu())\n",
    "        num_cols.append(batch[\"target_col_mask\"].max().item())\n",
    "    labels_test = torch.cat(labels_test, dim=0)\n",
    "    logits_test = torch.stack(logits_test, dim=0)\n",
    "    preds_test = torch.argmax(logits_test, dim=1)\n",
    "    num_cols = torch.tensor(num_cols)\n",
    "    \n",
    "    from sklearn.metrics import confusion_matrix, f1_score\n",
    "    mask = num_cols > 0\n",
    "    ts_pred_list = logits_test.argmax(\n",
    "                                1).cpu().detach().numpy().tolist()\n",
    "    ts_micro_f1 = f1_score(labels_test.reshape(-1).numpy().tolist(),\n",
    "                        ts_pred_list,\n",
    "                        average=\"micro\")\n",
    "    ts_macro_f1 = f1_score(labels_test.reshape(-1).numpy().tolist(),\n",
    "                        ts_pred_list,\n",
    "                        average=\"macro\")\n",
    "    print(\"ts_micro_f1={:.4f}, ts_macro_f1={:.4f}\".format(ts_micro_f1, ts_macro_f1))\n",
    "    ts_pred_list = logits_test.argmax(\n",
    "                                1).cpu().detach()[mask].numpy().tolist()\n",
    "    ts_micro_f1 = f1_score(labels_test.reshape(-1)[mask].numpy().tolist(),\n",
    "                        ts_pred_list,\n",
    "                        average=\"micro\")\n",
    "    ts_macro_f1 = f1_score(labels_test.reshape(-1)[mask].numpy().tolist(),\n",
    "                        ts_pred_list,\n",
    "                        average=\"macro\")\n",
    "    print(\"ts_micro_f1={:.4f}, ts_macro_f1={:.4f}\".format(ts_micro_f1, ts_macro_f1))\n",
    "    ts_pred_list = logits_test.argmax(\n",
    "                                1).cpu().detach()[~mask].numpy().tolist()\n",
    "    ts_micro_f1 = f1_score(labels_test.reshape(-1)[~mask].numpy().tolist(),\n",
    "                        ts_pred_list,\n",
    "                        average=\"micro\")\n",
    "    ts_macro_f1 = f1_score(labels_test.reshape(-1)[~mask].numpy().tolist(),\n",
    "                        ts_pred_list,\n",
    "                        average=\"macro\")\n",
    "    print(\"ts_micro_f1={:.4f}, ts_macro_f1={:.4f}\".format(ts_micro_f1, ts_macro_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "标签映射关系从 /data/yongkang/TU/SOTAB/label_mapping.txt 读取成功\n",
      "数据已从 /data/yongkang/TU/SOTAB/comma_test_fully_deduplicated_sotab.csv 加载\n",
      "start encoder\n",
      "test 15040\n"
     ]
    }
   ],
   "source": [
    "src = None\n",
    "test_dataset_iter = SOTABTablewiseIterateDataset(# cv=cv,\n",
    "                                        split=\"test\",\n",
    "                                        tokenizer=tokenizer,\n",
    "                                        max_length=max_length,\n",
    "                                        # multicol_only=multicol_only,\n",
    "                                        device=device,\n",
    "                                        gt_only=False,\n",
    "                                        base_dirpath=os.path.join(args.data_path, \"SOTAB\"),\n",
    "                                        max_unlabeled=8,\n",
    "                                        )\n",
    "padder = collate_fn(tokenizer.pad_token_id)\n",
    "test_dataloader_iter = DataLoader(test_dataset_iter,\n",
    "                                batch_size=1,\n",
    "                            #   collate_fn=collate_fn)\n",
    "                            collate_fn=padder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: ylabel='Count'>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "def is_sublist(A, B):\n",
    "    it = iter(B)\n",
    "    return all(x in it for x in A)\n",
    "def get_permutation(x):\n",
    "    new = []\n",
    "    x = x.tolist()\n",
    "    if len(x) == 1:\n",
    "        x = x[0]\n",
    "    for k in x:\n",
    "        if k not in new:\n",
    "            new.append(k)\n",
    "    return new\n",
    "\n",
    "import seaborn as sns\n",
    "num_cols_test_truncate = []\n",
    "for batch_idx, batch in enumerate(test_dataloader_iter):\n",
    "    target_col_mask = batch[\"target_col_mask\"].T\n",
    "    init_permutation_i = get_permutation(target_col_mask)\n",
    "    num_cols_test_truncate.append(len(init_permutation_i))\n",
    "sns.histplot(num_cols_test_truncate)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************Threshold: 1.5****************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1560166/2479533116.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  score_init[batch_idx] = F.softmax(logits.detach()).max().item()\n",
      "/tmp/ipykernel_1560166/2479533116.py:107: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  ood_score_temp = F.softmax(logits_temp.detach()).max().item()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 16840\n",
      "1000 / 16840\n",
      "2000 / 16840\n",
      "3000 / 16840\n",
      "4000 / 16840\n",
      "5000 / 16840\n",
      "6000 / 16840\n",
      "7000 / 16840\n",
      "8000 / 16840\n",
      "9000 / 16840\n",
      "10000 / 16840\n",
      "11000 / 16840\n",
      "12000 / 16840\n",
      "13000 / 16840\n",
      "14000 / 16840\n",
      "15000 / 16840\n"
     ]
    }
   ],
   "source": [
    "# brute force perumate in validation TODO: restrict mamimus length of permutation\n",
    "import itertools\n",
    "\n",
    "def is_sublist(A, B):\n",
    "    it = iter(B)\n",
    "    return all(x in it for x in A)\n",
    "def get_permutation(x):\n",
    "    new = []\n",
    "    x = x.tolist()\n",
    "    if len(x) == 1:\n",
    "        x = x[0]\n",
    "    for k in x:\n",
    "        if k not in new:\n",
    "            new.append(k)\n",
    "    return new\n",
    "\n",
    "import torch.nn.functional as F\n",
    "test_data = defaultdict(list)\n",
    "test_logits = defaultdict(list)\n",
    "test_cls_indexes = defaultdict(list)\n",
    "test_target_col_mask = defaultdict(list)\n",
    "test_embs = defaultdict(list)\n",
    "test_col_num = defaultdict(list)\n",
    "test_label = defaultdict(list)\n",
    "test_class = defaultdict(list)\n",
    "test_target_embs = defaultdict(list)\n",
    "test_drop_idx = defaultdict(list)\n",
    "    \n",
    "model.load_state_dict(best_state_dict, strict=False)\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "change_log = []\n",
    "score_init = []\n",
    "score_best = [] \n",
    "\n",
    "\n",
    "ft_embs_test = []\n",
    "labels_test = []\n",
    "logits_test = []\n",
    "log = defaultdict(list)\n",
    "num_cols = []\n",
    "corrected = 0\n",
    "total_mistakes = 0\n",
    "num_permutations = {}\n",
    "init_permutation = {}\n",
    "init_correctness = {}\n",
    "score_init = {}\n",
    "score_permutation = defaultdict(list)\n",
    "permutation_correctness = defaultdict(list)\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch in enumerate(test_dataloader_iter):\n",
    "        cls_indexes = torch.LongTensor([[0, batch[\"cls_indexes\"].cpu().item()]]).to(device)\n",
    "        target_col_mask = batch[\"target_col_mask\"].T\n",
    "        logits = model(batch[\"data\"].T, cls_indexes=cls_indexes,)\n",
    "        score_init[batch_idx] = F.softmax(logits.detach()).max().item()\n",
    "        label_i = batch[\"label\"].item()\n",
    "        predict_init = logits.argmax().item()\n",
    "        init_permutation_i = get_permutation(target_col_mask)\n",
    "        init_permutation[batch_idx] = init_permutation_i\n",
    "        if predict_init == label_i:\n",
    "            init_correctness[batch_idx] = True\n",
    "        else:\n",
    "            init_correctness[batch_idx] = False\n",
    "        num_permutations[batch_idx] = 0\n",
    "\n",
    "\n",
    "        x = [0]\n",
    "        new_batch_data = []\n",
    "        for col_i in x:\n",
    "            if col_i == 0:\n",
    "                if len(new_batch_data) == 0:\n",
    "                    cls_indexes_value = 0\n",
    "                else:\n",
    "                    cls_indexes_value = sum([len(new_batch_data[i]) for i in range(len(new_batch_data))])\n",
    "            new_batch_data.append(batch[\"data\"].T[target_col_mask==col_i])\n",
    "        new_batch_data = torch.cat(new_batch_data, dim=-1).reshape(1, -1)\n",
    "        cls_indexes = torch.tensor([0, cls_indexes_value]).reshape(1, -1).to(device)\n",
    "        logits_temp, embs_temp = model(new_batch_data, cls_indexes=cls_indexes, get_enc=True)\n",
    "        test_target_embs[batch_idx].append(embs_temp.cpu())\n",
    "\n",
    "\n",
    "        num_cols.append(len(init_permutation_i))\n",
    "        labels_test.append(batch[\"label\"].cpu())\n",
    "        col_idx_set = target_col_mask.unique().tolist()\n",
    "        assert -1 not in col_idx_set\n",
    "        for r in range(len(init_permutation_i), 1, -1): # not \n",
    "            for x in itertools.combinations(init_permutation_i, r):\n",
    "                if 0 not in x:\n",
    "                    continue\n",
    "\n",
    "                num_permutations[batch_idx] += 1\n",
    "                new_batch_data = []\n",
    "                if len(x) != len(init_permutation_i):\n",
    "                    drop_idx = (set(init_permutation_i)-set(x)).pop()\n",
    "                else:\n",
    "                    drop_idx = -1\n",
    "                for col_i in x:\n",
    "                    if col_i == 0:\n",
    "                        if len(new_batch_data) == 0:\n",
    "                            cls_indexes_value = 0\n",
    "                        else:\n",
    "                            cls_indexes_value = sum([len(new_batch_data[i]) for i in range(len(new_batch_data))])\n",
    "                    new_batch_data.append(batch[\"data\"].T[target_col_mask==col_i])\n",
    "                new_batch_data = torch.cat(new_batch_data, dim=-1).reshape(1, -1)\n",
    "                cls_indexes = torch.tensor([0, cls_indexes_value]).reshape(1, -1).to(device)\n",
    "                logits_temp, embs_temp = model(new_batch_data, cls_indexes=cls_indexes, get_enc=True)\n",
    "                ood_score_temp = F.softmax(logits_temp.detach()).max().item()\n",
    "                score_permutation[batch_idx].append(ood_score_temp)\n",
    "                predict_temp = logits_temp.argmax().item()\n",
    "                permutation_correctness[batch_idx].append(predict_temp == label_i)\n",
    "                \n",
    "                test_data[batch_idx].append(new_batch_data.cpu())\n",
    "                test_logits[batch_idx].append(logits_temp.detach().cpu())\n",
    "                test_cls_indexes[batch_idx].append(cls_indexes_value)\n",
    "                test_embs[batch_idx].append(embs_temp.cpu())\n",
    "                test_col_num[batch_idx].append(len(x))\n",
    "                test_label[batch_idx].append(torch.tensor(predict_temp == label_i).long()) # indicate whether the permutation is correct or not\n",
    "                test_class[batch_idx].append(label_i)\n",
    "                test_drop_idx[batch_idx].append(drop_idx)\n",
    "        if batch_idx % 1000 == 0:\n",
    "            print(batch_idx, '/', len(valid_dataloader_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"/data/zhihao/TU/Watchog/verification\", exist_ok=True)\n",
    "torch.save({\"data\": test_data, \"logits\": test_logits, \"cls_indexes\": test_cls_indexes, \n",
    "            \"embs\": test_embs, \"col_num\": test_col_num, \"label\": test_label, \"class\": test_class}, f\"/data/zhihao/TU/Watchog/verification/{args.task}_test_data.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ts_micro_f1=0.8781, ts_macro_f1=0.8657\n"
     ]
    }
   ],
   "source": [
    "# check test data\n",
    "labels_test = []\n",
    "logits_test = []\n",
    "for batch_idx, batch in enumerate(test_dataloader_iter):\n",
    "    embs = test_embs[batch_idx][0].reshape(1,-1)\n",
    "    logits = test_logits[batch_idx][0].reshape(-1)\n",
    "    labels_test.append(batch[\"label\"].cpu())\n",
    "    logits_test.append(logits.detach().cpu())\n",
    "labels_test = torch.cat(labels_test, dim=0)\n",
    "logits_test = torch.stack(logits_test, dim=0)\n",
    "preds_test = torch.argmax(logits_test, dim=1)\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "ts_pred_list = logits_test.argmax(\n",
    "                            1).cpu().detach().numpy().tolist()\n",
    "ts_micro_f1 = f1_score(labels_test.reshape(-1).numpy().tolist(),\n",
    "                    ts_pred_list,\n",
    "                    average=\"micro\")\n",
    "ts_macro_f1 = f1_score(labels_test.reshape(-1).numpy().tolist(),\n",
    "                    ts_pred_list,\n",
    "                    average=\"macro\")\n",
    "ts_f1_full = f1_score(labels_test.reshape(-1).numpy().tolist(),\n",
    "                    ts_pred_list,\n",
    "                    average=None)\n",
    "# full_f1_init\n",
    "print(\"ts_micro_f1={:.4f}, ts_macro_f1={:.4f}\".format(ts_micro_f1, ts_macro_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************Threshold: 1.5****************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1560166/1950327127.py:54: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  score_init[batch_idx] = F.softmax(logits.detach()).max().item()\n",
      "/tmp/ipykernel_1560166/1950327127.py:100: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  ood_score_temp = F.softmax(logits_temp.detach()).max().item()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 16840\n",
      "1000 / 16840\n",
      "2000 / 16840\n",
      "3000 / 16840\n",
      "4000 / 16840\n",
      "5000 / 16840\n",
      "6000 / 16840\n",
      "7000 / 16840\n",
      "8000 / 16840\n",
      "9000 / 16840\n",
      "10000 / 16840\n",
      "11000 / 16840\n",
      "12000 / 16840\n",
      "13000 / 16840\n",
      "14000 / 16840\n",
      "15000 / 16840\n",
      "16000 / 16840\n"
     ]
    }
   ],
   "source": [
    "# brute force perumate in validation TODO: restrict mamimus length of permutation\n",
    "import itertools\n",
    "\n",
    "def is_sublist(A, B):\n",
    "    it = iter(B)\n",
    "    return all(x in it for x in A)\n",
    "def get_permutation(x):\n",
    "    new = []\n",
    "    x = x.tolist()\n",
    "    if len(x) == 1:\n",
    "        x = x[0]\n",
    "    for k in x:\n",
    "        if k not in new:\n",
    "            new.append(k)\n",
    "    return new\n",
    "\n",
    "import torch.nn.functional as F\n",
    "veri_data = defaultdict(list)\n",
    "veri_logits = defaultdict(list)\n",
    "veri_cls_indexes = defaultdict(list)\n",
    "veri_target_col_mask = defaultdict(list)\n",
    "veri_embs = defaultdict(list)\n",
    "veri_col_num = defaultdict(list)\n",
    "veri_label = defaultdict(list)\n",
    "veri_class = defaultdict(list)\n",
    "veri_target_embs = defaultdict(list)\n",
    "    \n",
    "model.load_state_dict(best_state_dict, strict=False)\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "change_log = []\n",
    "score_init = []\n",
    "score_best = [] \n",
    "\n",
    "\n",
    "ft_embs_test = []\n",
    "labels_test = []\n",
    "logits_test = []\n",
    "log = defaultdict(list)\n",
    "num_cols = []\n",
    "corrected = 0\n",
    "total_mistakes = 0\n",
    "num_permutations = {}\n",
    "init_permutation = {}\n",
    "init_correctness = {}\n",
    "score_init = {}\n",
    "score_permutation = defaultdict(list)\n",
    "permutation_correctness = defaultdict(list)\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch in enumerate(valid_dataloader_iter):\n",
    "        cls_indexes = torch.LongTensor([[0, batch[\"cls_indexes\"].cpu().item()]]).to(device)\n",
    "        target_col_mask = batch[\"target_col_mask\"].T\n",
    "        logits = model(batch[\"data\"].T, cls_indexes=cls_indexes,)\n",
    "        score_init[batch_idx] = F.softmax(logits.detach()).max().item()\n",
    "        label_i = batch[\"label\"].item()\n",
    "        predict_init = logits.argmax().item()\n",
    "        init_permutation_i = get_permutation(target_col_mask)\n",
    "        init_permutation[batch_idx] = init_permutation_i\n",
    "        if predict_init == label_i:\n",
    "            init_correctness[batch_idx] = True\n",
    "        else:\n",
    "            init_correctness[batch_idx] = False\n",
    "        num_permutations[batch_idx] = 0\n",
    "\n",
    "        x = [0]\n",
    "        new_batch_data = []\n",
    "        for col_i in x:\n",
    "            if col_i == 0:\n",
    "                if len(new_batch_data) == 0:\n",
    "                    cls_indexes_value = 0\n",
    "                else:\n",
    "                    cls_indexes_value = sum([len(new_batch_data[i]) for i in range(len(new_batch_data))])\n",
    "            new_batch_data.append(batch[\"data\"].T[target_col_mask==col_i])\n",
    "        new_batch_data = torch.cat(new_batch_data, dim=-1).reshape(1, -1)\n",
    "        cls_indexes = torch.tensor([0, cls_indexes_value]).reshape(1, -1).to(device)\n",
    "        logits_temp, embs_temp = model(new_batch_data, cls_indexes=cls_indexes, get_enc=True)\n",
    "        veri_target_embs[batch_idx].append(embs_temp.cpu())\n",
    "\n",
    "        num_cols.append(len(init_permutation_i))\n",
    "        labels_test.append(batch[\"label\"].cpu().item())\n",
    "        col_idx_set = target_col_mask.unique().tolist()\n",
    "        assert -1 not in col_idx_set\n",
    "        for r in range(len(init_permutation_i), 0, -1): # not \n",
    "            for x in itertools.combinations(init_permutation_i, r):\n",
    "                if 0 not in x:\n",
    "                    continue\n",
    "\n",
    "                num_permutations[batch_idx] += 1\n",
    "                new_batch_data = []\n",
    "                for col_i in x:\n",
    "                    if col_i == 0:\n",
    "                        if len(new_batch_data) == 0:\n",
    "                            cls_indexes_value = 0\n",
    "                        else:\n",
    "                            cls_indexes_value = sum([len(new_batch_data[i]) for i in range(len(new_batch_data))])\n",
    "                    new_batch_data.append(batch[\"data\"].T[target_col_mask==col_i])\n",
    "                new_batch_data = torch.cat(new_batch_data, dim=-1).reshape(1, -1)\n",
    "                cls_indexes = torch.tensor([0, cls_indexes_value]).reshape(1, -1).to(device)\n",
    "                logits_temp, embs_temp = model(new_batch_data, cls_indexes=cls_indexes, get_enc=True)\n",
    "                ood_score_temp = F.softmax(logits_temp.detach()).max().item()\n",
    "                score_permutation[batch_idx].append(ood_score_temp)\n",
    "                predict_temp = logits_temp.argmax().item()\n",
    "                permutation_correctness[batch_idx].append(predict_temp == label_i)\n",
    "                \n",
    "                \n",
    "                if predict_temp == label_i and r < max(len(init_permutation_i)-2, len(init_permutation_i)//2):\n",
    "                    continue\n",
    "                veri_data[batch_idx].append(new_batch_data.cpu())\n",
    "                veri_logits[batch_idx].append(logits_temp.detach().cpu())\n",
    "                veri_cls_indexes[batch_idx].append(cls_indexes_value)\n",
    "                veri_embs[batch_idx].append(embs_temp.cpu())\n",
    "                veri_col_num[batch_idx].append(len(x))\n",
    "                veri_label[batch_idx].append(torch.tensor(predict_temp == label_i).long()) # indicate whether the permutation is correct or not\n",
    "                veri_class[batch_idx].append(label_i)\n",
    "\n",
    "        if batch_idx % 1000 == 0:\n",
    "            print(batch_idx, '/', len(valid_dataloader_iter))\n",
    "                    # if predict_temp == label_i and r < max(len(init_permutation_i)-2, len(init_permutation_i)//2):\n",
    "                    #     continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# SOTAB, col num from 8 to 1, all negs, half pos\n",
    "os.makedirs(\"/data/zhihao/TU/Watchog/verification\", exist_ok=True)\n",
    "torch.save({\"data\": veri_data, \"label\": veri_label, \"logits\": veri_logits, \"cls_indexes\": veri_cls_indexes, \"embs\": veri_embs, \"target_embs\":veri_target_embs , \"col_num\": veri_col_num}, f\"/data/zhihao/TU/Watchog/verification/{args.task}_veri_data.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
