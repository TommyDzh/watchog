{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/zhihao/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/zhihao/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from time import time\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from collections import defaultdict\n",
    "\n",
    "# import pytrec_eval\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss, BCEWithLogitsLoss\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig, AutoTokenizer\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from accelerate import Accelerator\n",
    "from copy import deepcopy\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "from watchog.dataset import (\n",
    "    # collate_fn,\n",
    "    TURLColTypeTablewiseDataset,\n",
    "    TURLRelExtTablewiseDataset,\n",
    "    SatoCVTablewiseDataset,\n",
    "    ColPoplTablewiseDataset\n",
    ")\n",
    "\n",
    "from watchog.dataset import TableDataset, SupCLTableDataset, SemtableCVTablewiseDataset, GittablesColwiseDataset, GittablesTablewiseDataset\n",
    "from watchog.model import BertMultiPairPooler, BertForMultiOutputClassification, BertForMultiOutputClassificationColPopl\n",
    "from watchog.model import SupCLforTable, UnsupCLforTable, lm_mp\n",
    "from watchog.utils import load_checkpoint, f1_score_multilabel, collate_fn, get_col_pred, ColPoplEvaluator\n",
    "from watchog.utils import task_num_class_dict\n",
    "from accelerate import DistributedDataParallelKwargs\n",
    "import wandb\n",
    "\n",
    "from argparse import Namespace\n",
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import re\n",
    "import transformers\n",
    "from torch.utils import data\n",
    "from torch.nn.utils import rnn\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from typing import List\n",
    "from functools import reduce\n",
    "import operator\n",
    "\n",
    "from itertools import chain\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args={\"wandb\": false, \"model\": \"Watchog\", \"unlabeled_train_only\": false, \"context_encoding_type\": \"v0\", \"pool_version\": \"v0.2\", \"random_sample\": false, \"comment\": \"debug\", \"shortcut_name\": \"bert-base-uncased\", \"max_length\": 64, \"adaptive_max_length\": false, \"max_num_col\": 8, \"batch_size\": 16, \"epoch\": 1, \"random_seed\": 4649, \"train_n_seed_cols\": -1, \"num_classes\": 101, \"multi_gpu\": false, \"fp16\": false, \"warmup\": 0.0, \"lr\": 5e-05, \"task\": \"gt-semtab22-dbpedia-all0\", \"colpair\": false, \"metadata\": false, \"from_scratch\": false, \"cl_tag\": \"wikitables/simclr/bert_100000_10_32_256_5e-05_sample_row4,sample_row4_tfidf_entity_column_0.05_0_last.pt\", \"dropout_prob\": 0.5, \"eval_test\": true, \"small_tag\": \"semi1\", \"data_path\": \"/data/zhihao/TU/\", \"pretrained_ckpt_path\": \"/data/zhihao/TU/Watchog/model/\"}\n",
      "gt-semtab22-dbpedia-all0/wikitables-simclr-bert_100000_10_32_256_5e-05_sample_row4,sample_row4_tfidf_entity_column_0.05_0_last.pt_bert-base-uncased-poolsemi1-max_colsv0.2-rand8-bsFalse-ml16-ne64-do10.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/tmp/ipykernel_3942545/3314320431.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(ckpt_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(augment_op='sample_row4,sample_row4', batch_size=32, data_path='/data/zhihao/TU/TURL/', fp16=True, gpus='0', lm='bert', logdir='/data/zhihao/TU/Watchog/model/', lr=5e-05, max_len=256, mode='simclr', model='Watchog', n_epochs=10, pretrain_data='wikitables', pretrained_model_path='', projector=768, run_id=0, sample_meth='tfidf_entity', save_model=10, single_column=False, size=100000, table_order='column', temperature=0.05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n"
     ]
    }
   ],
   "source": [
    "    device = torch.device(2)\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--wandb\", type=bool, default=False)\n",
    "    parser.add_argument(\"--model\", type=str, default=\"Watchog\")\n",
    "    parser.add_argument(\"--unlabeled_train_only\", type=bool, default=False)\n",
    "    parser.add_argument(\"--context_encoding_type\", type=str, default=\"v0\")\n",
    "    parser.add_argument(\"--pool_version\", type=str, default=\"v0.2\")\n",
    "    parser.add_argument(\"--random_sample\", type=bool, default=False)\n",
    "    parser.add_argument(\"--comment\", type=str, default=\"debug\", help=\"to distinguish the runs\")\n",
    "    parser.add_argument(\n",
    "        \"--shortcut_name\",\n",
    "        default=\"bert-base-uncased\",\n",
    "        type=str,\n",
    "        help=\"Huggingface model shortcut name \",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_length\",\n",
    "        default=64,\n",
    "        type=int,\n",
    "        help=\n",
    "        \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "        \"than this will be truncated, sequences shorter will be padded.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--adaptive_max_length\",\n",
    "        default=False,\n",
    "        type=bool,\n",
    "    )    \n",
    "    parser.add_argument(\n",
    "        \"--max_num_col\",\n",
    "        default=8,\n",
    "        type=int,\n",
    "    )   \n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--batch_size\",\n",
    "        default=16,\n",
    "        type=int,\n",
    "        help=\"Batch size\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--epoch\",\n",
    "        default=1,\n",
    "        type=int,\n",
    "        help=\"Number of epochs for training\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--random_seed\",\n",
    "        default=4649,\n",
    "        type=int,\n",
    "        help=\"Random seed\",\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--train_n_seed_cols\",\n",
    "        default=-1,\n",
    "        type=int,\n",
    "        help=\"number of seeding columns in training\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--num_classes\",\n",
    "        default=78,\n",
    "        type=int,\n",
    "        help=\"Number of classes\",\n",
    "    )\n",
    "    parser.add_argument(\"--multi_gpu\",\n",
    "                        action=\"store_true\",\n",
    "                        default=False,\n",
    "                        help=\"Use multiple GPU\")\n",
    "    parser.add_argument(\"--fp16\",\n",
    "                        action=\"store_true\",\n",
    "                        default=False,\n",
    "                        help=\"Use FP16\")\n",
    "    parser.add_argument(\"--warmup\",\n",
    "                        type=float,\n",
    "                        default=0.,\n",
    "                        help=\"Warmup ratio\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=5e-5, help=\"Learning rate\")\n",
    "    parser.add_argument(\"--task\",\n",
    "                        type=str,\n",
    "                        default='gt-semtab22-dbpedia-all0',\n",
    "                        choices=[\n",
    "                            \"sato0\", \"sato1\", \"sato2\", \"sato3\", \"sato4\",\n",
    "                            \"msato0\", \"msato1\", \"msato2\", \"msato3\", \"msato4\",\n",
    "                            \"gt-dbpedia0\", \"gt-dbpedia1\", \"gt-dbpedia2\", \"gt-dbpedia3\", \"gt-dbpedia4\",\n",
    "                            \"gt-dbpedia-all0\", \"gt-dbpedia-all1\", \"gt-dbpedia-all2\", \"gt-dbpedia-all3\", \"gt-dbpedia-all4\",\n",
    "                            \"gt-schema-all0\", \"gt-schema-all1\", \"gt-schema-all2\", \"gt-schema-all3\", \"gt-schema-all4\",\n",
    "                            \"gt-semtab22-dbpedia\", \"gt-semtab22-dbpedia0\", \"gt-semtab22-dbpedia1\", \"gt-semtab22-dbpedia2\", \"gt-semtab22-dbpedia3\", \"gt-semtab22-dbpedia4\",\n",
    "                            \"gt-semtab22-dbpedia-all\", \"gt-semtab22-dbpedia-all0\", \"gt-semtab22-dbpedia-all1\", \"gt-semtab22-dbpedia-all2\", \"gt-semtab22-dbpedia-all3\", \"gt-semtab22-dbpedia-all4\",\n",
    "                            \"gt-semtab22-schema-class-all\", \"gt-semtab22-schema-property-all\",\n",
    "                            \"turl\", \"turl-re\", \"col-popl-1\", \"col-popl-2\", \"col-popl-3\", \"row-popl\",\n",
    "                            \"col-popl-turl-0\", \"col-popl-turl-1\", \"col-popl-turl-2\",\n",
    "                            \"col-popl-turl-mdonly-0\", \"col-popl-turl-mdonly-1\", \"col-popl-turl-mdonly-2\"\n",
    "                        ],\n",
    "                        help=\"Task names}\")\n",
    "    parser.add_argument(\"--colpair\",\n",
    "                        action=\"store_true\",\n",
    "                        help=\"Use column pair embedding\")\n",
    "    parser.add_argument(\"--metadata\",\n",
    "                        action=\"store_true\",\n",
    "                        help=\"Use column header metadata\")\n",
    "    parser.add_argument(\"--from_scratch\",\n",
    "                        action=\"store_true\",\n",
    "                        help=\"Training from scratch\")\n",
    "    parser.add_argument(\"--cl_tag\",\n",
    "                        type=str,\n",
    "                        default=\"wikitables/simclr/bert_100000_10_32_256_5e-05_sample_row4,sample_row4_tfidf_entity_column_0.05_0_last.pt\",\n",
    "                        help=\"path to the pre-trained file\")\n",
    "    parser.add_argument(\"--dropout_prob\",\n",
    "                        type=float,\n",
    "                        default=0.5)\n",
    "    parser.add_argument(\"--eval_test\",\n",
    "                        action=\"store_true\",\n",
    "                        help=\"evaluate on testset and do not save the model file\")\n",
    "    parser.add_argument(\"--small_tag\",\n",
    "                        type=str,\n",
    "                        default=\"semi1\",\n",
    "                        help=\"e.g., by_table_t5_v1\")\n",
    "    parser.add_argument(\"--data_path\",\n",
    "                        type=str,\n",
    "                        default=\"/data/zhihao/TU/\")\n",
    "    parser.add_argument(\"--pretrained_ckpt_path\",\n",
    "                        type=str,\n",
    "                        default=\"/data/zhihao/TU/Watchog/model/\")    \n",
    "\n",
    "    args = parser.parse_args([])\n",
    "    task = args.task\n",
    "    if args.small_tag != \"\":\n",
    "        args.eval_test = True\n",
    "    \n",
    "    args.num_classes = task_num_class_dict[task]\n",
    "    if args.colpair:\n",
    "        assert \"turl-re\" == task, \"colpair can be only used for Relation Extraction\"\n",
    "    if args.metadata:\n",
    "        assert \"turl-re\" == task or \"turl\" == task, \"metadata can be only used for TURL datasets\"\n",
    "    if \"col-popl\":\n",
    "        # metrics = {\n",
    "        #     \"accuracy\": CategoricalAccuracy(tie_break=True),\n",
    "        # }\n",
    "        if args.train_n_seed_cols != -1:\n",
    "            if \"col-popl\" in task:\n",
    "                assert args.train_n_seed_cols == int(task[-1]),  \"# of seed columns must match\"\n",
    "\n",
    "    print(\"args={}\".format(json.dumps(vars(args))))\n",
    "\n",
    "    max_length = args.max_length\n",
    "    batch_size = args.batch_size\n",
    "    num_train_epochs = args.epoch\n",
    "\n",
    "    shortcut_name = args.shortcut_name\n",
    "\n",
    "    if args.colpair and args.metadata:\n",
    "        taskname = \"{}-colpair-metadata\".format(task)\n",
    "    elif args.colpair:\n",
    "        taskname = \"{}-colpair\".format(task)\n",
    "    elif args.metadata:\n",
    "        taskname = \"{}-metadata\".format(task)\n",
    "    elif args.train_n_seed_cols == -1 and 'popl' in task:\n",
    "        taskname = \"{}-mix\".format(task)\n",
    "    else:\n",
    "        taskname = \"\".join(task)\n",
    "    cv = int(task[-1])\n",
    "\n",
    "    if args.from_scratch:\n",
    "        if \"gt\" in task:\n",
    "            tag_name = \"{}/{}-{}-{}-pool{}-max_cols{}-rand{}-bs{}-ml{}-ne{}-do{}{}\".format(\n",
    "                taskname,  \"{}-fromscratch\".format(shortcut_name), args.small_tag, args.comment, args.pool_version, args.max_num_col, args.random_sample,\n",
    "                batch_size, max_length, num_train_epochs, args.dropout_prob, \n",
    "                '-rs{}'.format(args.random_seed) if args.random_seed != 4649 else '')\n",
    "        else:\n",
    "            tag_name = \"{}/{}-{}-{}-bs{}-ml{}-ne{}-do{}{}\".format(\n",
    "                taskname,  \"{}-fromscratch\".format(shortcut_name), args.small_tag, args.comment, \n",
    "                batch_size, max_length, num_train_epochs, args.dropout_prob, \n",
    "                '-rs{}'.format(args.random_seed) if args.random_seed != 4649 else '')\n",
    "        \n",
    "    else:\n",
    "        if \"gt\" in task:\n",
    "            tag_name = \"{}/{}_{}-pool{}-max_cols{}-rand{}-bs{}-ml{}-ne{}-do{}{}\".format(\n",
    "                taskname, args.cl_tag.replace('/', '-'),  shortcut_name, args.small_tag, args.pool_version, args.max_num_col, args.random_sample,\n",
    "                batch_size, max_length, num_train_epochs, args.dropout_prob,\n",
    "                '-rs{}'.format(args.random_seed) if args.random_seed != 4649 else '')\n",
    "        else:\n",
    "            tag_name = \"{}/{}_{}-{}-bs{}-ml{}-ne{}-do{}{}\".format(\n",
    "                taskname, args.cl_tag.replace('/', '-'),  shortcut_name, args.small_tag,\n",
    "                batch_size, max_length, num_train_epochs, args.dropout_prob,\n",
    "                '-rs{}'.format(args.random_seed) if args.random_seed != 4649 else '')\n",
    "\n",
    "    # if args.eval_test:\n",
    "    #     if args.small_tag != '':\n",
    "    #         tag_name = tag_name.replace('outputs', 'small_outputs')\n",
    "    #         tag_name += '-' + args.small_tag\n",
    "    print(tag_name)\n",
    "    file_path = os.path.join(args.data_path, \"Watchog\", \"outputs\", tag_name)\n",
    "\n",
    "    dirpath = os.path.dirname(file_path)\n",
    "    if not os.path.exists(dirpath):\n",
    "        print(\"{} not exists. Created\".format(dirpath))\n",
    "        os.makedirs(dirpath)\n",
    "    \n",
    "    if args.fp16:\n",
    "        torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = True\n",
    "        \n",
    "      \n",
    "        \n",
    "    # accelerator = Accelerator(mixed_precision=\"no\" if not args.fp16 else \"fp16\")   \n",
    "    ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)\n",
    "    accelerator = Accelerator(mixed_precision=\"no\" if not args.fp16 else \"fp16\", kwargs_handlers=[ddp_kwargs])\n",
    "\n",
    "    device = torch.device(2)\n",
    "    ckpt_path = os.path.join(args.pretrained_ckpt_path, args.cl_tag)\n",
    "    # ckpt_path = '/efs/checkpoints/{}.pt'.format(args.cl_tag)\n",
    "    ckpt = torch.load(ckpt_path, map_location=device)\n",
    "    ckpt_hp = ckpt['hp']\n",
    "    print(ckpt_hp)\n",
    " \n",
    "    setattr(ckpt_hp, 'batch_size', args.batch_size)\n",
    "    setattr(ckpt_hp, 'hidden_dropout_prob', args.dropout_prob)\n",
    "    setattr(ckpt_hp, 'shortcut_name', args.shortcut_name)\n",
    "    setattr(ckpt_hp, 'num_labels', args.num_classes)\n",
    "    \n",
    "    \n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained(shortcut_name)\n",
    "    padder = collate_fn(tokenizer.pad_token_id)\n",
    "    if task == \"turl-re\" and args.colpair:\n",
    "        model = BertForMultiOutputClassification(ckpt_hp, device=device, lm=ckpt['hp'].lm, col_pair='Pair')\n",
    "    elif \"col-popl\" in task:\n",
    "        model = BertForMultiOutputClassificationColPopl(ckpt_hp, device=device, lm=ckpt['hp'].lm, n_seed_cols=int(task[i][-1]), cls_for_md=\"md\" in task)\n",
    "    else:\n",
    "        model = BertForMultiOutputClassification(ckpt_hp, device=device, lm=ckpt['hp'].lm, version=\"v0\", use_attention_mask=True)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3942545/307592194.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  best_state_dict = torch.load(\"/data/zhihao/TU/Watchog/outputs/gt-semtab22-dbpedia-all0/bert-base-uncased-fromscratch-semi1-Repeat@5-AttnMask-UnlabelValid-max-unlabeled@8-poolv0-unlabeled8-randFalse-bs16-ml128-ne50-do0.1_best_f1_macro.pt\", map_location=device)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best_state_dict = torch.load(\"/data/zhihao/TU/Watchog/outputs/gt-semtab22-dbpedia-all0/bert-base-uncased-fromscratch-semi1-AttnMask-max-unlabeled@8-poolv0-unlabeled8-randFalse-bs16-ml128-ne50-do0.1_best_f1_micro.pt\", map_location=device)\n",
    "best_state_dict = torch.load(\"/data/zhihao/TU/Watchog/outputs/gt-semtab22-dbpedia-all0/bert-base-uncased-fromscratch-semi1-Repeat@5-AttnMask-UnlabelValid-max-unlabeled@8-poolv0-unlabeled8-randFalse-bs16-ml128-ne50-do0.1_best_f1_macro.pt\", map_location=device)\n",
    "model.load_state_dict(best_state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GittablesTablewiseIterateDataset(data.Dataset):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            cv: int,\n",
    "            split: str,\n",
    "            src: str,  # train or test\n",
    "            tokenizer: AutoTokenizer,\n",
    "            max_length: int = 256,\n",
    "            gt_only: bool = False,\n",
    "            device: torch.device = None,\n",
    "            base_dirpath: str = \"/data/zhihao/TU/GitTables/semtab_gittables/2022\",\n",
    "            base_tag: str = '', # blank, comma\n",
    "            small_tag: str = \"\",\n",
    "            train_ratio: float = 1.0,\n",
    "            max_unlabeled=8,\n",
    "            random_sample=False, # TODO\n",
    "            train_only=False): # TODO\n",
    "        if device is None:\n",
    "            device = torch.device('cpu')\n",
    "        basename = small_tag+ \"_cv_{}.csv\"\n",
    "    \n",
    "        if split in [\"train\", \"valid\"]:\n",
    "            df_list = []\n",
    "            for i in range(5):\n",
    "                if i == cv:\n",
    "                    continue\n",
    "                filepath = os.path.join(base_dirpath, basename.format(i))\n",
    "                df_list.append(pd.read_csv(filepath))\n",
    "                print(split, i)\n",
    "            df = pd.concat(df_list, axis=0)\n",
    "        else:\n",
    "            # test\n",
    "            filepath = os.path.join(base_dirpath, basename.format(cv))\n",
    "            df = pd.read_csv(filepath)\n",
    "            print(split)\n",
    "\n",
    "\n",
    "        if gt_only:\n",
    "            df = df[df[\"class_id\"] > -1]\n",
    "        if train_only and split != \"train\":\n",
    "            df = df[df[\"class_id\"] > -1]\n",
    "\n",
    "        \n",
    "        data_list = []\n",
    "        \n",
    "        df['class_id'] = df['class_id'].astype(int)\n",
    "        df.drop(df[(df['data'].isna()) & (df['class_id'] == -1)].index, inplace=True)\n",
    "        df['col_idx'] = df['col_idx'].astype(int)\n",
    "        df['data'] = df['data'].astype(str)\n",
    "        \n",
    "        num_tables = len(df.groupby(\"table_id\"))\n",
    "        valid_index = int(num_tables * 0.8)\n",
    "        num_train = int(train_ratio * num_tables * 0.8)        \n",
    "        \n",
    "        # df.drop(df[(df['data'] == '') & (df['class_id'] == -1)].index, inplace=True)\n",
    "        total_num_cols = 0\n",
    "        for i, (index, group_df) in enumerate(df.groupby(\"table_id\")):\n",
    "            if (split == \"train\") and ((i >= num_train) or (i >= valid_index)):\n",
    "                break\n",
    "            if split == \"valid\" and i < valid_index:\n",
    "                continue\n",
    "            #     break\n",
    "            labeled_columns = group_df[group_df['class_id'] > -1]\n",
    "            unlabeled_columns = group_df[group_df['class_id'] == -1]\n",
    "            num_unlabeled = min(max(max_unlabeled-len(labeled_columns), 0), len(unlabeled_columns)) if max_unlabeled > 0 else len(unlabeled_columns)\n",
    "            unlabeled_columns = unlabeled_columns.sample(num_unlabeled) if random_sample else unlabeled_columns[0:num_unlabeled]\n",
    "            # group_df = pd.concat([group_df[group_df['class_id'] > -1], unlabeled_columns.sample(min(10-len(labeled_columns), len(unlabeled_columns)))])\n",
    "            # group_df = pd.concat([group_df[group_df['class_id'] > -1], unlabeled_columns[0:min(max(10-len(labeled_columns), 0), len(unlabeled_columns))]])\n",
    "            group_df = pd.concat([group_df[group_df['class_id'] > -1], unlabeled_columns]) # TODO\n",
    "            group_df.sort_values(by=['col_idx'], inplace=True)\n",
    "\n",
    "            if max_length <= 128:\n",
    "                cur_maxlen = min(max_length, 512 // len(list(group_df[\"class_id\"].values)) - 1)\n",
    "            else:\n",
    "                cur_maxlen = max(1, max_length // len(list(group_df[\"class_id\"].values)) - 1)\n",
    "                \n",
    "            token_ids_list = group_df[\"data\"].apply(lambda x: tokenizer.encode(\n",
    "                tokenizer.cls_token + \" \" + x, add_special_tokens=False, max_length=cur_maxlen, truncation=True)).tolist(\n",
    "                )\n",
    "            token_ids = torch.LongTensor(reduce(operator.add,\n",
    "                                                token_ids_list)).to(device)\n",
    "            for col_i in range(len(token_ids_list)):\n",
    "                if group_df[\"class_id\"].values[col_i] == -1:\n",
    "                    continue\n",
    "                target_col_mask = []\n",
    "                cls_index_value = 0\n",
    "                context_id = 1\n",
    "                for col_j in range(len(token_ids_list)):\n",
    "                    if col_j == col_i:\n",
    "                        target_col_mask += [0] * len(token_ids_list[col_j])\n",
    "                    else:\n",
    "                        target_col_mask += [context_id] * len(token_ids_list[col_j])\n",
    "                        context_id += 1\n",
    "                    if col_j < col_i:\n",
    "                        cls_index_value += len(token_ids_list[col_j])\n",
    "                cls_index_list = [cls_index_value] \n",
    "                for cls_index in cls_index_list:\n",
    "                    assert token_ids[\n",
    "                        cls_index] == tokenizer.cls_token_id, \"cls_indexes validation\"\n",
    "                cls_indexes = torch.LongTensor(cls_index_list).to(device)\n",
    "                class_ids = torch.LongTensor(\n",
    "                    [group_df[\"class_id\"].values[col_i]]).to(device)\n",
    "                target_col_mask = torch.LongTensor(target_col_mask).to(device)\n",
    "                data_list.append(\n",
    "                    [index,\n",
    "                    len(group_df), token_ids, class_ids, cls_indexes, target_col_mask])                \n",
    "        print(split, len(data_list))\n",
    "        self.table_df = pd.DataFrame(data_list,\n",
    "                                     columns=[\n",
    "                                         \"table_id\", \"num_col\", \"data_tensor\",\n",
    "                                         \"label_tensor\", \"cls_indexes\", \"target_col_mask\"\n",
    "                                     ])\n",
    "        \"\"\"\n",
    "        # NOTE: msato contains a small portion of single-col tables. keep it to be consistent.  \n",
    "        if multicol_only:\n",
    "            # Check\n",
    "            num_all_tables = len(self.table_df)\n",
    "            self.table_df = self.table_df[self.table_df[\"num_col\"] > 1]\n",
    "            assert len(self.table_df) == num_all_tables\n",
    "        \"\"\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.table_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"data\": self.table_df.iloc[idx][\"data_tensor\"],\n",
    "            \"label\": self.table_df.iloc[idx][\"label_tensor\"],\n",
    "            \"cls_indexes\": self.table_df.iloc[idx][\"cls_indexes\"],\n",
    "            \"target_col_mask\": self.table_df.iloc[idx][\"target_col_mask\"],\n",
    "        }\n",
    "        #\"idx\": torch.LongTensor([idx])}\n",
    "        #\"cls_indexes\": self.table_df.iloc[idx][\"cls_indexes\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(pad_token_id, data_only=True):\n",
    "    '''padder for input batch'''\n",
    "\n",
    "    def padder(samples):    \n",
    "        data = torch.nn.utils.rnn.pad_sequence(\n",
    "            [sample[\"data\"] for sample in samples], padding_value=pad_token_id)\n",
    "        if not data_only:\n",
    "            label = torch.nn.utils.rnn.pad_sequence(\n",
    "                [sample[\"label\"] for sample in samples], padding_value=-1)\n",
    "        else:\n",
    "            label = torch.cat([sample[\"label\"] for sample in samples])\n",
    "        batch = {\"data\": data, \"label\": label}\n",
    "        if \"idx\" in samples[0]:\n",
    "            batch[\"idx\"] = [sample[\"idx\"] for sample in samples]\n",
    "        if \"cls_indexes\" in samples[0]:\n",
    "            cls_indexes = torch.nn.utils.rnn.pad_sequence(\n",
    "                [sample[\"cls_indexes\"] for sample in samples], padding_value=0)\n",
    "            batch[\"cls_indexes\"] = cls_indexes\n",
    "        if \"target_col_mask\" in samples[0]:\n",
    "            target_col_mask = torch.nn.utils.rnn.pad_sequence(\n",
    "                [sample[\"target_col_mask\"] for sample in samples], padding_value=-1)\n",
    "            batch[\"target_col_mask\"] = target_col_mask\n",
    "        if \"table_embedding\" in samples[0]:\n",
    "            table_embeddings = [sample[\"table_embedding\"] for sample in samples]\n",
    "            batch[\"table_embedding\"] = torch.stack(table_embeddings, dim=0)\n",
    "        return batch\n",
    "        \n",
    "    return padder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "test 1085\n"
     ]
    }
   ],
   "source": [
    "src = None\n",
    "test_dataset_iter = GittablesTablewiseIterateDataset(cv=cv,\n",
    "                            split=\"test\", src=src,\n",
    "                            tokenizer=tokenizer,\n",
    "                            max_length=max_length,\n",
    "                            gt_only='all' not in task,\n",
    "                            device=device,\n",
    "                            base_dirpath=os.path.join(args.data_path, \"GitTables/semtab_gittables/2022\"),\n",
    "                            small_tag=\"semi1\")\n",
    "padder = collate_fn(tokenizer.pad_token_id)\n",
    "test_dataloader_iter = DataLoader(test_dataset_iter,\n",
    "                                batch_size=1,\n",
    "                            #   collate_fn=collate_fn)\n",
    "                            collate_fn=padder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 1\n",
      "train 2\n",
      "train 3\n",
      "train 4\n",
      "train 3463\n"
     ]
    }
   ],
   "source": [
    "train_dataset_iter = GittablesTablewiseIterateDataset(cv=cv,\n",
    "                            split=\"train\", src=src,\n",
    "                            tokenizer=tokenizer,\n",
    "                            max_length=max_length,\n",
    "                            gt_only='all' not in task,\n",
    "                            device=device,\n",
    "                            base_dirpath=os.path.join(args.data_path, \"GitTables/semtab_gittables/2022\"),\n",
    "                            small_tag=\"semi1\")\n",
    "padder = collate_fn(tokenizer.pad_token_id)\n",
    "train_dataloader_iter = DataLoader(train_dataset_iter,\n",
    "                                batch_size=1,\n",
    "                            #   collate_fn=collate_fn)\n",
    "                            collate_fn=padder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************** max_unlabeled=1 **************************\n",
      "test\n",
      "test 1085\n",
      "ts_micro_f1=0.4876, ts_macro_f1=0.2104\n",
      "ts_micro_f1=0.4818, ts_macro_f1=0.2082\n",
      "ts_micro_f1=0.4941, ts_macro_f1=0.1724\n",
      "************************** max_unlabeled=2 **************************\n",
      "test\n",
      "test 1085\n",
      "ts_micro_f1=0.5014, ts_macro_f1=0.2123\n",
      "ts_micro_f1=0.5009, ts_macro_f1=0.2123\n",
      "ts_micro_f1=1.0000, ts_macro_f1=1.0000\n",
      "************************** max_unlabeled=4 **************************\n",
      "test\n",
      "test 1085\n",
      "ts_micro_f1=0.5235, ts_macro_f1=0.2501\n",
      "ts_micro_f1=0.5231, ts_macro_f1=0.2501\n",
      "ts_micro_f1=1.0000, ts_macro_f1=1.0000\n"
     ]
    }
   ],
   "source": [
    "for max_unlabeled in [1, 2, 4]:\n",
    "    print(f\"************************** max_unlabeled={max_unlabeled} **************************\")\n",
    "    src = None\n",
    "    test_dataset_iter = GittablesTablewiseIterateDataset(cv=cv,\n",
    "                                split=\"test\", src=src,\n",
    "                                tokenizer=tokenizer,\n",
    "                                max_length=max_length,\n",
    "                                gt_only='all' not in task,\n",
    "                                device=device,\n",
    "                                max_unlabeled = max_unlabeled,\n",
    "                                base_dirpath=os.path.join(args.data_path, \"GitTables/semtab_gittables/2022\"),\n",
    "                                small_tag=\"semi1\")\n",
    "    padder = collate_fn(tokenizer.pad_token_id)\n",
    "    test_dataloader_iter = DataLoader(test_dataset_iter,\n",
    "                                    batch_size=1,\n",
    "                                #   collate_fn=collate_fn)\n",
    "                                collate_fn=padder)\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    ft_embs_test = []\n",
    "    labels_test = []\n",
    "    logits_test = []\n",
    "    num_cols = []\n",
    "    for batch_idx, batch in enumerate(test_dataloader_iter):\n",
    "        cls_indexes = torch.LongTensor([[0, batch[\"cls_indexes\"].cpu().item()]]).to(device)\n",
    "        target_col_mask = batch[\"target_col_mask\"].T\n",
    "        logits, embs = model(batch[\"data\"].T, cls_indexes=cls_indexes, get_enc=True)\n",
    "        labels_test.append(batch[\"label\"].cpu())\n",
    "        logits_test.append(logits.detach().cpu())\n",
    "        num_cols.append(batch[\"target_col_mask\"].max().item())\n",
    "        ft_embs_test.append(embs.detach().cpu().reshape(1, -1))\n",
    "    labels_test = torch.cat(labels_test, dim=0)\n",
    "    logits_test = torch.stack(logits_test, dim=0)\n",
    "    preds_test = torch.argmax(logits_test, dim=1)\n",
    "    num_cols = torch.tensor(num_cols)\n",
    "    ft_embs_test = torch.cat(ft_embs_test, dim=0)\n",
    "    from sklearn.metrics import confusion_matrix, f1_score\n",
    "    mask = num_cols > 0\n",
    "    ts_pred_list = logits_test.argmax(\n",
    "                                1).cpu().detach().numpy().tolist()\n",
    "    ts_micro_f1 = f1_score(labels_test.reshape(-1).numpy().tolist(),\n",
    "                        ts_pred_list,\n",
    "                        average=\"micro\")\n",
    "    ts_macro_f1 = f1_score(labels_test.reshape(-1).numpy().tolist(),\n",
    "                        ts_pred_list,\n",
    "                        average=\"macro\")\n",
    "    print(\"ts_micro_f1={:.4f}, ts_macro_f1={:.4f}\".format(ts_micro_f1, ts_macro_f1))\n",
    "    ts_pred_list = logits_test.argmax(\n",
    "                                1).cpu().detach()[mask].numpy().tolist()\n",
    "    ts_micro_f1 = f1_score(labels_test.reshape(-1)[mask].numpy().tolist(),\n",
    "                        ts_pred_list,\n",
    "                        average=\"micro\")\n",
    "    ts_macro_f1 = f1_score(labels_test.reshape(-1)[mask].numpy().tolist(),\n",
    "                        ts_pred_list,\n",
    "                        average=\"macro\")\n",
    "    print(\"ts_micro_f1={:.4f}, ts_macro_f1={:.4f}\".format(ts_micro_f1, ts_macro_f1))\n",
    "    ts_pred_list = logits_test.argmax(\n",
    "                                1).cpu().detach()[~mask].numpy().tolist()\n",
    "    ts_micro_f1 = f1_score(labels_test.reshape(-1)[~mask].numpy().tolist(),\n",
    "                        ts_pred_list,\n",
    "                        average=\"micro\")\n",
    "    ts_macro_f1 = f1_score(labels_test.reshape(-1)[~mask].numpy().tolist(),\n",
    "                        ts_pred_list,\n",
    "                        average=\"macro\")\n",
    "    print(\"ts_micro_f1={:.4f}, ts_macro_f1={:.4f}\".format(ts_micro_f1, ts_macro_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************** max_unlabeled=4 **************************\n",
      "test\n",
      "test 1085\n",
      "ts_micro_f1=0.5235, ts_macro_f1=0.2502\n",
      "ts_micro_f1=0.5235, ts_macro_f1=0.2502\n",
      "ts_micro_f1=0.0000, ts_macro_f1=nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhihao/jupyterprojects/jupyter/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/home/zhihao/jupyterprojects/jupyter/lib/python3.8/site-packages/numpy/lib/function_base.py:495: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis)\n",
      "/home/zhihao/jupyterprojects/jupyter/lib/python3.8/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "for max_unlabeled in [4]:\n",
    "    print(f\"************************** max_unlabeled={max_unlabeled} **************************\")\n",
    "    src = None\n",
    "    test_dataset_iter = GittablesTablewiseIterateDataset(cv=cv,\n",
    "                                split=\"test\", src=src,\n",
    "                                tokenizer=tokenizer,\n",
    "                                max_length=max_length,\n",
    "                                gt_only='all' not in task,\n",
    "                                device=device,\n",
    "                                base_dirpath=os.path.join(args.data_path, \"GitTables/semtab_gittables/2022\"),\n",
    "                                small_tag=\"semi1\")\n",
    "    padder = collate_fn(tokenizer.pad_token_id)\n",
    "    test_dataloader_iter = DataLoader(test_dataset_iter,\n",
    "                                    batch_size=1,\n",
    "                                #   collate_fn=collate_fn)\n",
    "                                collate_fn=padder)\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    ft_embs_test = []\n",
    "    labels_test = []\n",
    "    logits_test = []\n",
    "    num_cols = []\n",
    "    for batch_idx, batch in enumerate(test_dataloader_iter):\n",
    "        target_col_mask = batch[\"target_col_mask\"].T\n",
    "        init_permutation_i = get_permutation(target_col_mask)\n",
    "        if 0 in init_permutation_i[:max_unlabeled]:\n",
    "            x = init_permutation_i[:max_unlabeled]\n",
    "        else:\n",
    "            x = init_permutation_i[:max_unlabeled-1] + [0]\n",
    "        new_batch_data = []\n",
    "        for col_i in x:\n",
    "            if col_i == 0:\n",
    "                if len(new_batch_data) == 0:\n",
    "                    cls_indexes_value = 0\n",
    "                else:\n",
    "                    cls_indexes_value = sum([len(new_batch_data[i]) for i in range(len(new_batch_data))])\n",
    "            new_batch_data.append(batch[\"data\"].T[target_col_mask==col_i])\n",
    "        new_batch_data = torch.cat(new_batch_data, dim=-1).reshape(1, -1)\n",
    "        cls_indexes = torch.tensor([0, cls_indexes_value]).reshape(1, -1).to(device)\n",
    "        logits, embs = model(new_batch_data, cls_indexes=cls_indexes, get_enc=True)\n",
    "        labels_test.append(batch[\"label\"].cpu())\n",
    "        logits_test.append(logits.detach().cpu())\n",
    "        num_cols.append(len(x))\n",
    "        assert len(x) <= max_unlabeled\n",
    "        ft_embs_test.append(embs.detach().cpu().reshape(1, -1))\n",
    "    labels_test = torch.cat(labels_test, dim=0)\n",
    "    logits_test = torch.stack(logits_test, dim=0)\n",
    "    preds_test = torch.argmax(logits_test, dim=1)\n",
    "    num_cols = torch.tensor(num_cols)\n",
    "    ft_embs_test = torch.cat(ft_embs_test, dim=0)\n",
    "    from sklearn.metrics import confusion_matrix, f1_score\n",
    "    mask = num_cols > 0\n",
    "    ts_pred_list = logits_test.argmax(\n",
    "                                1).cpu().detach().numpy().tolist()\n",
    "    ts_micro_f1 = f1_score(labels_test.reshape(-1).numpy().tolist(),\n",
    "                        ts_pred_list,\n",
    "                        average=\"micro\")\n",
    "    ts_macro_f1 = f1_score(labels_test.reshape(-1).numpy().tolist(),\n",
    "                        ts_pred_list,\n",
    "                        average=\"macro\")\n",
    "    print(\"ts_micro_f1={:.4f}, ts_macro_f1={:.4f}\".format(ts_micro_f1, ts_macro_f1))\n",
    "    ts_pred_list = logits_test.argmax(\n",
    "                                1).cpu().detach()[mask].numpy().tolist()\n",
    "    ts_micro_f1 = f1_score(labels_test.reshape(-1)[mask].numpy().tolist(),\n",
    "                        ts_pred_list,\n",
    "                        average=\"micro\")\n",
    "    ts_macro_f1 = f1_score(labels_test.reshape(-1)[mask].numpy().tolist(),\n",
    "                        ts_pred_list,\n",
    "                        average=\"macro\")\n",
    "    print(\"ts_micro_f1={:.4f}, ts_macro_f1={:.4f}\".format(ts_micro_f1, ts_macro_f1))\n",
    "    ts_pred_list = logits_test.argmax(\n",
    "                                1).cpu().detach()[~mask].numpy().tolist()\n",
    "    ts_micro_f1 = f1_score(labels_test.reshape(-1)[~mask].numpy().tolist(),\n",
    "                        ts_pred_list,\n",
    "                        average=\"micro\")\n",
    "    ts_macro_f1 = f1_score(labels_test.reshape(-1)[~mask].numpy().tolist(),\n",
    "                        ts_pred_list,\n",
    "                        average=\"macro\")\n",
    "    print(\"ts_micro_f1={:.4f}, ts_macro_f1={:.4f}\".format(ts_micro_f1, ts_macro_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'label', 'cls_indexes'])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 3, 7,  ..., 7, 7, 2])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: ylabel='Count'>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.histplot(num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3695852534562212"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(num_cols<7).sum().item()/len(num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_freq = torch.zeros(args.num_classes)\n",
    "for batch_idx, batch in enumerate(train_dataloader_iter):\n",
    "    class_freq[batch[\"label\"].item()] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def reweight_logits(logits, class_weights):\n",
    "#     # Reweight the logits by multiplying with class weights\n",
    "#     reweighted_logits = logits * torch.sqrt(class_weights)\n",
    "    \n",
    "#     # Apply softmax to the reweighted logits\n",
    "#     reweighted_probs = F.softmax(reweighted_logits, dim=-1)\n",
    "    \n",
    "#     return reweighted_probs\n",
    "def reweight_logits(logits, class_weights):\n",
    "    # Step 1: Apply exp(logits)\n",
    "    exp_logits = torch.exp(logits)\n",
    "    \n",
    "    # Step 2: Multiply by class weights\n",
    "    # reweighted_exp = exp_logits * torch.sqrt(class_weights)\n",
    "    reweighted_exp = exp_logits * class_weights\n",
    "    # Step 3: Normalize to get valid probabilities (like softmax)\n",
    "    reweighted_probs = reweighted_exp / reweighted_exp.sum(dim=-1, keepdim=True)\n",
    "    \n",
    "    return reweighted_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reweight_logits_after_softmax(logits, class_weights):\n",
    "    # Step 1: Apply exp(logits)\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    \n",
    "    # Step 2: Multiply by class weights\n",
    "    # reweighted_exp = exp_logits * torch.sqrt(class_weights)\n",
    "    reweighted_probs = probs * class_weights\n",
    "    # Step 3: Normalize to get valid probabilities (like softmax)\n",
    "    reweighted_probs = reweighted_probs / reweighted_probs.sum(dim=-1, keepdim=True)\n",
    "    \n",
    "    return reweighted_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_permutations = []\n",
    "num_cols = []\n",
    "for batch_idx, batch in enumerate(test_dataloader_iter):\n",
    "    target_col_mask = batch[\"target_col_mask\"].T\n",
    "    init_permutation_i = get_permutation(target_col_mask)\n",
    "    init_permutations.append(init_permutation_i)\n",
    "    num_cols.append(len(init_permutation_i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, batch in enumerate(test_dataloader_iter):\n",
    "    target_col_mask = batch[\"target_col_mask\"].T\n",
    "    init_permutation_i = get_permutation(target_col_mask)\n",
    "    if len(init_permutation_i) == 8:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 0, 7]\n",
      "(1, 2, 3, 0)\n",
      "(1, 2, 4, 0)\n",
      "(1, 2, 5, 0)\n",
      "(1, 2, 6, 0)\n",
      "(1, 2, 0, 7)\n",
      "(1, 3, 4, 0)\n",
      "(1, 3, 5, 0)\n",
      "(1, 3, 6, 0)\n",
      "(1, 3, 0, 7)\n",
      "(1, 4, 5, 0)\n",
      "(1, 4, 6, 0)\n",
      "(1, 4, 0, 7)\n",
      "(1, 5, 6, 0)\n",
      "(1, 5, 0, 7)\n",
      "(1, 6, 0, 7)\n",
      "(2, 3, 4, 0)\n",
      "(2, 3, 5, 0)\n",
      "(2, 3, 6, 0)\n",
      "(2, 3, 0, 7)\n",
      "(2, 4, 5, 0)\n",
      "(2, 4, 6, 0)\n",
      "(2, 4, 0, 7)\n",
      "(2, 5, 6, 0)\n",
      "(2, 5, 0, 7)\n",
      "(2, 6, 0, 7)\n",
      "(3, 4, 5, 0)\n",
      "(3, 4, 6, 0)\n",
      "(3, 4, 0, 7)\n",
      "(3, 5, 6, 0)\n",
      "(3, 5, 0, 7)\n",
      "(3, 6, 0, 7)\n",
      "(4, 5, 6, 0)\n",
      "(4, 5, 0, 7)\n",
      "(4, 6, 0, 7)\n",
      "(5, 6, 0, 7)\n"
     ]
    }
   ],
   "source": [
    "print(init_permutation_i)\n",
    "for r in range(4, 3, -1): # not \n",
    "    for x in itertools.combinations(init_permutation_i, r):\n",
    "        if 0 not in x:\n",
    "            continue\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: ylabel='Count'>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************Threshold: 0.9****************************\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# brute force perumate, find uncertain init prediction, weighted msp\n",
    "import itertools\n",
    "\n",
    "def is_sublist(A, B):\n",
    "    it = iter(B)\n",
    "    return all(x in it for x in A)\n",
    "def get_permutation(x):\n",
    "    new = []\n",
    "    x = x.tolist()\n",
    "    if len(x) == 1:\n",
    "        x = x[0]\n",
    "    for k in x:\n",
    "        if k not in new:\n",
    "            new.append(k)\n",
    "    return new\n",
    "    \n",
    "model.load_state_dict(best_state_dict, strict=False)\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "change_log = []\n",
    "score_init = []\n",
    "score_best = [] \n",
    "idx_list = []\n",
    "uncertain_init_mask = []\n",
    "uncertain_target_mask = []\n",
    "correct_init_mask = []\n",
    "correct_target_mask = []\n",
    "correct_permutation_mask = []\n",
    "correct_permutation_confident_mask = []\n",
    "correct_permutation_confident_scores = []\n",
    "correct_msp_mask = []\n",
    "init_target_mask = []\n",
    "ood_score_init_list = []\n",
    "ood_score_target_list = []\n",
    "ood_score_final_list = []\n",
    "msp_predicts = []\n",
    "max_col_length = 3\n",
    "\n",
    "alpha = 0.25\n",
    "class_weights = (1.0 / class_freq) ** alpha\n",
    "debias_threshold = 1.0\n",
    "# Normalize the weights\n",
    "class_weights /= class_weights.sum()\n",
    "for threshold in [0.9]:\n",
    "    print(f\"*********************Threshold: {threshold}****************************\")\n",
    "    ft_embs_test = []\n",
    "    labels_test = []\n",
    "    logits_test = []\n",
    "    log = defaultdict(list)\n",
    "    num_cols = []\n",
    "    corrected = 0\n",
    "    total_mistakes = 0\n",
    "\n",
    "    for batch_idx, batch in enumerate(test_dataloader_iter):\n",
    "        target_col_mask = batch[\"target_col_mask\"].T\n",
    "        init_permutation_i = get_permutation(target_col_mask)\n",
    "        if len(init_permutation_i)>4:\n",
    "            idx_list.append(batch_idx)\n",
    "            if 0 in init_permutation_i[:4]:\n",
    "                x = init_permutation_i[:4]\n",
    "            else:\n",
    "                x = init_permutation_i[:3] + [0]\n",
    "            new_batch_data = []\n",
    "            for col_i in x:\n",
    "                if col_i == 0:\n",
    "                    if len(new_batch_data) == 0:\n",
    "                        cls_indexes_value = 0\n",
    "                    else:\n",
    "                        cls_indexes_value = sum([len(new_batch_data[i]) for i in range(len(new_batch_data))])\n",
    "                new_batch_data.append(batch[\"data\"].T[target_col_mask==col_i])\n",
    "            new_batch_data = torch.cat(new_batch_data, dim=-1).reshape(1, -1)\n",
    "            cls_indexes = torch.tensor([0, cls_indexes_value]).reshape(1, -1).to(device)\n",
    "            logits = model(new_batch_data, cls_indexes=cls_indexes,)\n",
    "            logits = reweight_logits(logits.detach().cpu(), class_weights)\n",
    "            \n",
    "            num_cols.append(len(x))         \n",
    "            ood_score_init = logits.max().item()\n",
    "            ood_score_init_list.append(ood_score_init)\n",
    "            uncertain_init_mask.append(ood_score_init < threshold)\n",
    "            predict_init = logits.argmax().item()\n",
    "            correct_init_mask.append(predict_init == batch[\"label\"].item())\n",
    "            \n",
    "            \n",
    "            correct_permutation = False\n",
    "            correct_permutation_ood_score = 0.0\n",
    "            correct_permutation_confident = False\n",
    "            ood_score_max = max_msp = 0\n",
    "            max_msp_debiased = 0\n",
    "            debias_class = []\n",
    "            for r in range(4, 3, -1): # not \n",
    "                for x in itertools.combinations(init_permutation_i, r):\n",
    "                    if 0 not in x:\n",
    "                        continue\n",
    "                    # for x in itertools.permutations(subset):\n",
    "                    #     if not is_sublist(x, init_permutation_i):\n",
    "                    #         continue\n",
    "                    new_batch_data = []\n",
    "                    for col_i in x:\n",
    "                        if col_i == 0:\n",
    "                            if len(new_batch_data) == 0:\n",
    "                                cls_indexes_value = 0\n",
    "                            else:\n",
    "                                cls_indexes_value = sum([len(new_batch_data[i]) for i in range(len(new_batch_data))])\n",
    "                        new_batch_data.append(batch[\"data\"].T[target_col_mask==col_i])\n",
    "                    if 0 not in x:\n",
    "                        cls_indexes_value = 0\n",
    "                    new_batch_data = torch.cat(new_batch_data, dim=-1).reshape(1, -1)\n",
    "                    cls_indexes = torch.tensor([0, cls_indexes_value]).reshape(1, -1).to(device)\n",
    "                    logits_temp = model(new_batch_data, cls_indexes=cls_indexes,)\n",
    "                    logits_temp = reweight_logits(logits_temp.detach().cpu(), class_weights)\n",
    "                    msp_temp = logits_temp.max().item()\n",
    "                    predict_temp = logits_temp.argmax().item()\n",
    "                    # if 0 not in x and msp_temp > debias_threshold and (predict_temp!=predict_target or predict_temp!=predict_init):\n",
    "                    #     debias_class.append(predict_temp)\n",
    "                    #     continue\n",
    "                    # print(x, msp_temp, predict_temp)\n",
    "                    # if msp_temp > max_msp and 0 in x and predict_temp not in debias_class:\n",
    "                    if msp_temp > max_msp and 0 in x:\n",
    "                        max_msp = msp_temp\n",
    "                        msp_predict = predict_temp\n",
    "                    if predict_temp == batch[\"label\"].item():\n",
    "                        correct_permutation = True\n",
    "                        if msp_temp > correct_permutation_ood_score:\n",
    "                            correct_permutation_ood_score = msp_temp\n",
    "            msp_predicts.append(msp_predict)\n",
    "            ood_score_final_list.append(max_msp)\n",
    "            correct_permutation_mask.append(correct_permutation)\n",
    "            correct_permutation_confident_scores.append(correct_permutation_ood_score)\n",
    "            correct_msp_mask.append(msp_predict == batch[\"label\"].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in range(4, 3, -1): # not \n",
    "    for x in itertools.combinations([4,2 ,3], r):\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: ylabel='Count'>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAD4CAYAAAAdIcpQAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAAsTAAALEwEAmpwYAAAQ+klEQVR4nO3da7BdZX3H8e9PIuI9XE4ZmgRDR7QydlqZo6J0OmqsRbSEtog4XlInGsdbVRwV6wtbfaOj46111BSoofUCWlvipToUUKdVoge1KqA1g2IS0RwVsdWxiv77Yj/oMT3Js5F9Ocf9/czs2Ws9z7P2/j/ZZ/Yva629105VIUnSodxh2gVIklY+w0KS1GVYSJK6DAtJUpdhIUnqWjPtAsbhmGOOqY0bN067DElaVa6++upvV9Xccn2/lmGxceNGFhYWpl2GJK0qSW44WJ+HoSRJXYaFJKnLsJAkdRkWkqQuw0KS1GVYSJK6DAtJUpdhIUnqMiwkSV1jC4skFybZn+SLS9qOSnJZkq+0+yNbe5K8KcnuJJ9PcvKSbba08V9JsmVc9UrSuK3bcDxJxnpbt+H4sdQ+zst9vB34W+CiJW3nAZdX1auSnNfWXwI8Gjix3R4MvAV4cJKjgJcD80ABVyfZWVU3jbFuSRqLb+zdw+Pf9omxPsfFz3joWB53bHsWVfVx4LsHNG8GdrTlHcCZS9ovqoGrgLVJjgP+CLisqr7bAuIy4LRx1SxJWt6kz1kcW1U3tuVvAse25XXAniXj9ra2g7X/P0m2JVlIsrC4uDjaqiVpxk3tBHdVFYNDS6N6vO1VNV9V83Nzy15hV5L0K5p0WHyrHV6i3e9v7fuADUvGrW9tB2uXJE3QpMNiJ3DrJ5q2AJcuaX9K+1TUKcDN7XDVR4BHJTmyfXLqUa1NkjRBY/s0VJJ3AQ8Djkmyl8Gnml4FXJJkK3ADcHYb/iHgdGA38EPgqQBV9d0krwQ+3ca9oqoOPGkuSRqzsYVFVT3hIF2blhlbwLMP8jgXAheOsDRJ0m3kN7glSV2GhSSpy7CQJHUZFpKkLsNCktRlWEiSugwLSVKXYSFJ6jIsJEldhoUkqcuwkCR1GRaSpC7DQpLUZVhIkroMC0lSl2EhSeoyLCRJXYaFJKnLsJAkdRkWkqQuw0KS1GVYSJK6DAtJUpdhIUnqMiwkSV2GhSSpy7CQJHUZFpKkLsNCktRlWEiSugwLSVKXYSFJ6ppKWCR5QZJrknwxybuSHJHkhCS7kuxOcnGSw9vYO7X13a1/4zRqlqRZNvGwSLIO+AtgvqruDxwGnAO8Gnh9Vd0buAnY2jbZCtzU2l/fxkmSJmhah6HWAHdOsga4C3Aj8Ajgva1/B3BmW97c1mn9m5JkcqVKkiYeFlW1D3gt8HUGIXEzcDXwvaq6pQ3bC6xry+uAPW3bW9r4ow983CTbkiwkWVhcXBzvJCRpxkzjMNSRDPYWTgB+E7grcNrtfdyq2l5V81U1Pzc3d3sfTpK0xDQOQz0S+GpVLVbVT4D3AacCa9thKYD1wL62vA/YAND67wl8Z7IlS9Jsm0ZYfB04Jcld2rmHTcC1wJXAWW3MFuDStryzrdP6r6iqmmC9kjTzpnHOYheDE9WfAb7QatgOvAQ4N8luBuckLmibXAAc3drPBc6bdM2SNOvW9IeMXlW9HHj5Ac3XAw9aZuyPgMdNoi5J0vL8BrckqcuwkCR1GRaSpC7DQpLUZVhIkroMC0lSl2EhSeoyLCRJXYaFJKnLsJAkdRkWkqQuw0KS1GVYSJK6DAtJUpdhIUnqMiwkSV2GhSSpy7CQJHUZFpKkLsNCktRlWEiSugwLSVKXYSFJ6jIsJEldhoUkqcuwkCR1GRaSpC7DQpLUZVhIkroMC0lSl2EhSeoyLCRJXVMJiyRrk7w3yZeSXJfkIUmOSnJZkq+0+yPb2CR5U5LdST6f5ORp1CxJs2xaexZvBD5cVb8N/C5wHXAecHlVnQhc3tYBHg2c2G7bgLdMvlxJmm0TD4sk9wT+ALgAoKp+XFXfAzYDO9qwHcCZbXkzcFENXAWsTXLcRIuWpBk3jT2LE4BF4O+TfDbJ+UnuChxbVTe2Md8Ejm3L64A9S7bf29p+SZJtSRaSLCwuLo6xfEmaPUOFRZJTh2kb0hrgZOAtVfUA4Af84pATAFVVQN2WB62q7VU1X1Xzc3Nzv2JpkqTlDLtn8TdDtg1jL7C3qna19fcyCI9v3Xp4qd3vb/37gA1Ltl/f2iRJE7LmUJ1JHgI8FJhLcu6SrnsAh/0qT1hV30yyJ8l9q+rLwCbg2nbbAryq3V/aNtkJPCfJu4EHAzcvOVwlSZqAQ4YFcDhwtzbu7kvavw+cdTue97nAO5IcDlwPPJXBXs4lSbYCNwBnt7EfAk4HdgM/bGMlSRN0yLCoqo8BH0vy9qq6YVRPWlWfA+aX6dq0zNgCnj2q55Yk3Xa9PYtb3SnJdmDj0m2q6hHjKEqStLIMGxbvAd4KnA/8dHzlSJJWomHD4paq8pvTkjSjhv3o7PuTPCvJce0aTkclOWqslUmSVoxh9yy2tPsXLWkr4LdGW44kaSUaKiyq6oRxFyJJWrmGCoskT1muvaouGm05kqSVaNjDUA9csnwEg+9DfAYwLCRpBgx7GOq5S9eTrAXePY6CJEkrz696ifIfMLjUuCRpBgx7zuL9/OKS4YcB9wMuGVdRkqSVZdhzFq9dsnwLcENV7R1DPZKkFWiow1DtgoJfYnDl2SOBH4+zKEnSyjLsL+WdDXwKeByDS4fvSnJ7LlEuSVpFhj0M9TLggVW1HyDJHPBvDH7lTpL0a27YT0Pd4dagaL5zG7aVJK1yw+5ZfDjJR4B3tfXHM/gFO0nSDOj9Bve9gWOr6kVJ/hT4/db1SeAd4y5OkrQy9PYs3gC8FKCq3ge8DyDJ77S+Px5jbZKkFaJ33uHYqvrCgY2tbeNYKpIkrTi9sFh7iL47j7AOSdIK1guLhSRPP7AxydOAq8dTkiRppemds3g+8M9JnsgvwmEeOBz4kzHWJUlaQQ4ZFlX1LeChSR4O3L81f7Cqrhh7ZZKkFWPY37O4ErhyzLVIklYov4UtSeoyLCRJXYaFJKnLsJAkdRkWkqQuw0KS1DW1sEhyWJLPJvlAWz8hya4ku5NcnOTw1n6ntr679W+cVs2SNKumuWfxPOC6JeuvBl5fVfcGbgK2tvatwE2t/fVtnCRpgqYSFknWA48Bzm/rAR7BL36mdQdwZlve3NZp/ZvaeEnShExrz+INwIuBn7X1o4HvVdUtbX0vsK4trwP2ALT+m9v4X5JkW5KFJAuLi4tjLF2SZs/EwyLJY4H9VTXSq9ZW1faqmq+q+bm5uVE+tCTNvGF/g3uUTgXOSHI6cARwD+CNwNoka9rew3pgXxu/D9gA7E2yBrgn8J3Jly1Js2viexZV9dKqWl9VG4FzgCuq6okMLlR4Vhu2Bbi0Le9s67T+K6qqJliyJM28lfQ9i5cA5ybZzeCcxAWt/QLg6NZ+LnDelOqTpJk1jcNQP1dVHwU+2pavBx60zJgfAY+baGGSpF+ykvYsJEkrlGEhSeoyLCRJXYaFJKnLsJAkdRkWkqQuw0KS1GVYSJK6DAtJUpdhIUnqMiwkSV2GhSSpy7CQJHUZFpKkLsNCktRlWEiSugwLSVKXYSFJ6jIsJEldhoUkqcuwkCR1GRaSpC7DQpLUZVhIkroMC0lSl2EhSeoyLCRJXYaFJKnLsJAkdRkWkqQuw0KS1GVYSJK6Jh4WSTYkuTLJtUmuSfK81n5UksuSfKXdH9nak+RNSXYn+XySkyddsyTNumnsWdwCvLCqTgJOAZ6d5CTgPODyqjoRuLytAzwaOLHdtgFvmXzJkjTbJh4WVXVjVX2mLf83cB2wDtgM7GjDdgBntuXNwEU1cBWwNslxk61akmbbVM9ZJNkIPADYBRxbVTe2rm8Cx7bldcCeJZvtbW0HPta2JAtJFhYXF8dXtCTNoKmFRZK7Af8EPL+qvr+0r6oKqNvyeFW1varmq2p+bm5uhJVKkqYSFknuyCAo3lFV72vN37r18FK739/a9wEblmy+vrVJkiZkGp+GCnABcF1VvW5J105gS1veAly6pP0p7VNRpwA3LzlcJUmagDVTeM5TgScDX0jyudb2l8CrgEuSbAVuAM5ufR8CTgd2Az8EnjrRaiVJkw+Lqvp3IAfp3rTM+AKePdaiJEmH5De4JUldhoUkqcuwkCR1GRaSpC7DQpLUZVhIkroMC0lSl2EhSeoyLCRJXYaFJKnLsJAkdRkWkqQuw0KS1GVYSJK6DAtJUpdhIUnqMiwkSV2GhSSpy7CQJHUZFpKkLsNCktRlWCxj3YbjSTLW27oNx097mpI0tDXTLmAl+sbePTz+bZ8Y63Nc/IyHjvXxJWmU3LOQJHUZFpKkLsNCktRlWEiSugwLSVKXYSFJ6jIsJEldhoUkqcuwkCR1rZqwSHJaki8n2Z3kvGnXI0mzZFWERZLDgDcDjwZOAp6Q5KTpViVJs2NVhAXwIGB3VV1fVT8G3g1snnJNkjQzUlXTrqEryVnAaVX1tLb+ZODBVfWcJWO2Adva6n2BL9+OpzwG+Pbt2H61mbX5gnOeFc75trlXVc0t1/Frc9XZqtoObB/FYyVZqKr5UTzWajBr8wXnPCuc8+islsNQ+4ANS9bXtzZJ0gSslrD4NHBikhOSHA6cA+ycck2SNDNWxWGoqrolyXOAjwCHARdW1TVjfMqRHM5aRWZtvuCcZ4VzHpFVcYJbkjRdq+UwlCRpigwLSVLXTIZFkiOSfCrJfya5JslfLzPmTkkubpcX2ZVk4xRKHZkh53xukmuTfD7J5UnuNY1aR2WYOS8Z+2dJKsmq/pjlsHNOcnZ7ra9J8s5J1zlKQ/5tH5/kyiSfbX/fp0+j1lFKclibzweW6Rv9+1dVzdwNCHC3tnxHYBdwygFjngW8tS2fA1w87bonMOeHA3dpy8+chTm3vrsDHweuAuanXfcEXucTgc8CR7b135h23ROY83bgmW35JOBr0657BPM+F3gn8IFl+kb+/jWTexY18D9t9Y7tduCZ/s3Ajrb8XmBTkkyoxJEbZs5VdWVV/bCtXsXg+yyr1pCvM8ArgVcDP5pUbeMy5JyfDry5qm5q2+yfYIkjN+ScC7hHW74n8I0JlTcWSdYDjwHOP8iQkb9/zWRYwM934T4H7Acuq6pdBwxZB+yBwUd3gZuBoyda5IgNMeeltgL/OpHCxqg35yQnAxuq6oPTqG8chnid7wPcJ8l/JLkqyWkTL3LEhpjzXwFPSrIX+BDw3MlWOHJvAF4M/Owg/SN//5rZsKiqn1bV7zH43/ODktx/yiWN3bBzTvIkYB54zQTLG4tDzTnJHYDXAS+cUnljMcTrvIbBoaiHAU8A/i7J2knWOGpDzPkJwNuraj1wOvAP7fVfdZI8FthfVVdP8nlX5T/WKFXV94ArgQP/d/XzS4wkWcNg1/U7Ey1uTA4xZ5I8EngZcEZV/e+ESxubg8z57sD9gY8m+RpwCrBztZ/kvtUhXue9wM6q+klVfRX4LwbhseodYs5bgUvamE8CRzC44N5qdCpwRvubfTfwiCT/eMCYkb9/zWRYJJm79X9SSe4M/CHwpQOG7QS2tOWzgCuqnS1ajYaZc5IHAG9jEBSr+jg29OdcVTdX1TFVtbGqNjI4T3NGVS1Mo95RGPJv+18Y7FWQ5BgGh6Wun1iRIzbknL8ObGpj7scgLBYnWObIVNVLq2p9+5s9h8F705MOGDby969VcbmPMTgO2JHBjyrdAbikqj6Q5BXAQlXtBC5gsKu6G/gugxdlNRtmzq8B7ga8p50L+3pVnTG1im+/Yeb862aYOX8EeFSSa4GfAi+qqtW81zzMnF/I4HDbCxic7P7z1fyfv+WM+/3Ly31Ikrpm8jCUJOm2MSwkSV2GhSSpy7CQJHUZFpKkLsNCktRlWEiSuv4PCZDWbsBaiBwAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.histplot(num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in range(4, 3, -1):\n",
    "    for x in itertools.combinations(col_idx_set, r):\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "935 0 935 0 935\n"
     ]
    }
   ],
   "source": [
    "\n",
    "idx_list = torch.tensor(idx_list).reshape(-1)\n",
    "uncertain_init_mask = torch.tensor(uncertain_init_mask).reshape(-1)\n",
    "uncertain_target_mask = torch.tensor(uncertain_target_mask).reshape(-1)\n",
    "correct_init_mask = torch.tensor(correct_init_mask).reshape(-1)\n",
    "correct_target_mask = torch.tensor(correct_target_mask).reshape(-1)\n",
    "correct_permutation_mask = torch.tensor(correct_permutation_mask).reshape(-1)\n",
    "correct_permutation_confident_scores = torch.tensor(correct_permutation_confident_scores).reshape(-1)\n",
    "correct_msp_mask = torch.tensor(correct_msp_mask).reshape(-1)\n",
    "init_target_mask = torch.tensor(init_target_mask).reshape(-1)\n",
    "ood_score_init_list = torch.tensor(ood_score_init_list).reshape(-1)\n",
    "ood_score_target_list = torch.tensor(ood_score_target_list).reshape(-1)\n",
    "ood_score_final_list = torch.tensor(ood_score_final_list).reshape(-1)\n",
    "msp_predicts = torch.tensor(msp_predicts).reshape(-1)\n",
    "print(len(idx_list), len(init_target_mask), len(correct_init_mask), len(correct_target_mask), len(correct_permutation_mask))\n",
    "# torch.save({\"idx_list\": idx_list, \"uncertain_init_mask\": uncertain_init_mask, \"uncertain_target_mask\": uncertain_target_mask, \"correct_init_mask\": correct_init_mask, \"correct_target_mask\": correct_target_mask, \"correct_permutation_mask\": correct_permutation_mask, \"correct_msp_mask\": correct_msp_mask, \"init_target_mask\": init_target_mask, \"ood_score_init_list\": ood_score_init_list, \"ood_score_target_list\": ood_score_target_list, \"ood_score_final_list\": ood_score_final_list}, f\"./results/brute_force_permutation_alpha@{alpha}_drop.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************Threshold: 0.7****************************\n",
      "Init\n",
      "448 679 0.6597938144329897\n",
      "34 256 0.1328125\n",
      "MSP\n",
      "420 679 0.6185567010309279\n",
      "44 256 0.171875\n",
      "MSP & Init\n",
      "402\n",
      "466 679 0.6863033873343152\n",
      "62 256 0.2421875\n",
      "Permutation\n",
      "525 679 0.7731958762886598\n",
      "120 256 0.46875\n",
      "Permutation Confident\n",
      "498 679 0.7334315169366715\n",
      "76 256 0.296875\n",
      "*********************Threshold: 0.8****************************\n",
      "Init\n",
      "437 633 0.6903633491311216\n",
      "45 302 0.1490066225165563\n",
      "MSP\n",
      "406 633 0.641390205371248\n",
      "58 302 0.19205298013245034\n",
      "MSP & Init\n",
      "395\n",
      "448 633 0.707740916271722\n",
      "80 302 0.26490066225165565\n",
      "Permutation\n",
      "495 633 0.7819905213270142\n",
      "150 302 0.4966887417218543\n",
      "Permutation Confident\n",
      "468 633 0.7393364928909952\n",
      "94 302 0.31125827814569534\n",
      "*********************Threshold: 0.85****************************\n",
      "Init\n",
      "429 604 0.7102649006622517\n",
      "53 331 0.16012084592145015\n",
      "MSP\n",
      "401 604 0.6639072847682119\n",
      "63 331 0.1903323262839879\n",
      "MSP & Init\n",
      "392\n",
      "438 604 0.7251655629139073\n",
      "90 331 0.2719033232628399\n",
      "Permutation\n",
      "482 604 0.7980132450331126\n",
      "163 331 0.49244712990936557\n",
      "Permutation Confident\n",
      "455 604 0.7533112582781457\n",
      "93 331 0.2809667673716012\n",
      "*********************Threshold: 0.9****************************\n",
      "Init\n",
      "417 573 0.7277486910994765\n",
      "65 362 0.17955801104972377\n",
      "MSP\n",
      "389 573 0.6788830715532286\n",
      "75 362 0.20718232044198895\n",
      "MSP & Init\n",
      "381\n",
      "425 573 0.7417102966841187\n",
      "103 362 0.2845303867403315\n",
      "Permutation\n",
      "465 573 0.8115183246073299\n",
      "180 362 0.4972375690607735\n",
      "Permutation Confident\n",
      "438 573 0.7643979057591623\n",
      "96 362 0.26519337016574585\n"
     ]
    }
   ],
   "source": [
    "# weighted, only uncertain init\n",
    "\n",
    "for threshold in [0.7, 0.80,  0.85, 0.9]:\n",
    "    correct_permutation_confident_mask = correct_permutation_confident_scores>threshold\n",
    "    uncertain_init_mask = ood_score_init_list < threshold\n",
    "    uncertain_target_mask = ood_score_target_list < threshold\n",
    "    condition_mask = ~uncertain_init_mask\n",
    "    print(f\"*********************Threshold: {threshold}****************************\")\n",
    "    print(\"Init\")\n",
    "    # init prediction\n",
    "    print((condition_mask&correct_init_mask).sum().item(), \n",
    "        (condition_mask).sum().item(), \n",
    "        (condition_mask&correct_init_mask).sum().item()/(condition_mask).sum().item())\n",
    "    # init prediction\n",
    "    print((~condition_mask&correct_init_mask).sum().item(), \n",
    "        (~condition_mask).sum().item(), \n",
    "       (~condition_mask&correct_init_mask).sum().item()/(~condition_mask).sum().item())\n",
    "    print(\"MSP\")\n",
    "    # msp prediction\n",
    "    print((condition_mask&correct_msp_mask).sum().item(), \n",
    "        (condition_mask).sum().item(), \n",
    "        (condition_mask&correct_msp_mask).sum().item()/(condition_mask).sum().item())\n",
    "    # msp prediction\n",
    "    print((~condition_mask&correct_msp_mask).sum().item(), \n",
    "        (~condition_mask).sum().item(), \n",
    "        (~condition_mask&correct_msp_mask).sum().item()/(~condition_mask).sum().item())\n",
    "    print(\"MSP & Init\")\n",
    "    print(((condition_mask&correct_init_mask)&(condition_mask&correct_msp_mask)).sum().item())\n",
    "    # print(((~(init_target_mask&(~uncertain_init_mask & ~uncertain_target_mask))& correct_msp_mask)&(~(init_target_mask&(~uncertain_init_mask & ~uncertain_target_mask))& correct_init_mask)).sum().item())\n",
    "    print(((condition_mask&correct_init_mask)|(condition_mask&correct_msp_mask)).sum().item(),\n",
    "          (condition_mask).sum().item(), \n",
    "          ((condition_mask&correct_init_mask)|(condition_mask&correct_msp_mask)).sum().item()/(condition_mask).sum().item())\n",
    "    print(((~condition_mask&correct_init_mask)|(~condition_mask&correct_msp_mask)).sum().item(),\n",
    "          (~condition_mask).sum().item(), \n",
    "          ((~condition_mask&correct_init_mask)|(~condition_mask&correct_msp_mask)).sum().item()/(~condition_mask).sum().item())\n",
    "    print(\"Permutation\")\n",
    "    print((condition_mask&correct_permutation_mask).sum().item(), \n",
    "        (condition_mask).sum().item(), \n",
    "        (condition_mask&correct_permutation_mask).sum().item()/(condition_mask).sum().item())\n",
    "    print((~condition_mask&correct_permutation_mask).sum().item(), \n",
    "         (~condition_mask).sum().item(), \n",
    "        (~condition_mask&correct_permutation_mask).sum().item()/(~condition_mask).sum().item())\n",
    "    print(\"Permutation Confident\")\n",
    "    print((condition_mask&correct_permutation_confident_mask).sum().item(), \n",
    "        (condition_mask).sum().item(), \n",
    "        (condition_mask&correct_permutation_confident_mask).sum().item()/(condition_mask).sum().item())\n",
    "    print((~condition_mask&correct_permutation_confident_mask).sum().item(),\n",
    "            (~condition_mask).sum().item(), \n",
    "            (~condition_mask&correct_permutation_confident_mask).sum().item()/(~condition_mask).sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************Threshold: 0.82****************************\n",
      "Init\n",
      "524 742 0.706199460916442\n",
      "52 342 0.15204678362573099\n",
      "MSP\n",
      "497 742 0.6698113207547169\n",
      "65 342 0.19005847953216373\n",
      "MSP & Init\n",
      "479\n",
      "542 742 0.7304582210242587\n",
      "92 342 0.26900584795321636\n",
      "Permutation\n",
      "599 742 0.807277628032345\n",
      "151 342 0.4415204678362573\n",
      "Permutation Confident\n",
      "558 742 0.7520215633423181\n",
      "88 342 0.2573099415204678\n"
     ]
    }
   ],
   "source": [
    "# weighted, only uncertain init\n",
    "\n",
    "for threshold in [0.82]:\n",
    "    correct_permutation_confident_mask = correct_permutation_confident_scores>threshold\n",
    "    uncertain_init_mask = ood_score_init_list < threshold\n",
    "    uncertain_target_mask = ood_score_target_list < threshold\n",
    "    condition_mask = ~uncertain_init_mask\n",
    "    print(f\"*********************Threshold: {threshold}****************************\")\n",
    "    print(\"Init\")\n",
    "    # init prediction\n",
    "    print((condition_mask&correct_init_mask).sum().item(), \n",
    "        (condition_mask).sum().item(), \n",
    "        (condition_mask&correct_init_mask).sum().item()/(condition_mask).sum().item())\n",
    "    # init prediction\n",
    "    print((~condition_mask&correct_init_mask).sum().item(), \n",
    "        (~condition_mask).sum().item(), \n",
    "       (~condition_mask&correct_init_mask).sum().item()/(~condition_mask).sum().item())\n",
    "    print(\"MSP\")\n",
    "    # msp prediction\n",
    "    print((condition_mask&correct_msp_mask).sum().item(), \n",
    "        (condition_mask).sum().item(), \n",
    "        (condition_mask&correct_msp_mask).sum().item()/(condition_mask).sum().item())\n",
    "    # msp prediction\n",
    "    print((~condition_mask&correct_msp_mask).sum().item(), \n",
    "        (~condition_mask).sum().item(), \n",
    "        (~condition_mask&correct_msp_mask).sum().item()/(~condition_mask).sum().item())\n",
    "    print(\"MSP & Init\")\n",
    "    print(((condition_mask&correct_init_mask)&(condition_mask&correct_msp_mask)).sum().item())\n",
    "    # print(((~(init_target_mask&(~uncertain_init_mask & ~uncertain_target_mask))& correct_msp_mask)&(~(init_target_mask&(~uncertain_init_mask & ~uncertain_target_mask))& correct_init_mask)).sum().item())\n",
    "    print(((condition_mask&correct_init_mask)|(condition_mask&correct_msp_mask)).sum().item(),\n",
    "          (condition_mask).sum().item(), \n",
    "          ((condition_mask&correct_init_mask)|(condition_mask&correct_msp_mask)).sum().item()/(condition_mask).sum().item())\n",
    "    print(((~condition_mask&correct_init_mask)|(~condition_mask&correct_msp_mask)).sum().item(),\n",
    "          (~condition_mask).sum().item(), \n",
    "          ((~condition_mask&correct_init_mask)|(~condition_mask&correct_msp_mask)).sum().item()/(~condition_mask).sum().item())\n",
    "    print(\"Permutation\")\n",
    "    print((condition_mask&correct_permutation_mask).sum().item(), \n",
    "        (condition_mask).sum().item(), \n",
    "        (condition_mask&correct_permutation_mask).sum().item()/(condition_mask).sum().item())\n",
    "    print((~condition_mask&correct_permutation_mask).sum().item(), \n",
    "         (~condition_mask).sum().item(), \n",
    "        (~condition_mask&correct_permutation_mask).sum().item()/(~condition_mask).sum().item())\n",
    "    print(\"Permutation Confident\")\n",
    "    print((condition_mask&correct_permutation_confident_mask).sum().item(), \n",
    "        (condition_mask).sum().item(), \n",
    "        (condition_mask&correct_permutation_confident_mask).sum().item()/(condition_mask).sum().item())\n",
    "    print((~condition_mask&correct_permutation_confident_mask).sum().item(),\n",
    "            (~condition_mask).sum().item(), \n",
    "            (~condition_mask&correct_permutation_confident_mask).sum().item()/(~condition_mask).sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both correct tensor(25) tensor(0.6187) tensor(0.9433)\n",
      "MSP correct  tensor(40) tensor(0.5216) tensor(0.9643)\n",
      "Init correct tensor(27) tensor(0.5349) tensor(0.9423)\n",
      "Permutation correct tensor(13) tensor(0.5649) tensor(0.9761)\n"
     ]
    }
   ],
   "source": [
    "# MSP is wrong, but init is correct\n",
    "target_col_idx_msp_init = idx_list[~condition_mask&correct_msp_mask&correct_init_mask]\n",
    "target_col_idx_msp =  idx_list[~condition_mask&correct_msp_mask&~correct_init_mask]\n",
    "target_col_idx_init =  idx_list[~condition_mask&~correct_msp_mask&correct_init_mask]\n",
    "target_col_idx_permutation = idx_list[~condition_mask&correct_permutation_confident_mask&~correct_msp_mask&~correct_init_mask]\n",
    "target_col_idx_permutation_msp =  idx_list[~condition_mask&correct_permutation_confident_mask&~correct_msp_mask]\n",
    "target_col_idx_permutation_init =  idx_list[~condition_mask&correct_permutation_confident_mask&~correct_init_mask]\n",
    "\n",
    "print(\"Both correct\", (~condition_mask&correct_msp_mask&correct_init_mask).sum(), \n",
    "      ood_score_init_list[~condition_mask&correct_msp_mask&correct_init_mask].mean(), \n",
    "      # ood_score_target_list[~condition_mask&correct_msp_mask&correct_init_mask].mean(),\n",
    "      ood_score_final_list[~condition_mask&correct_msp_mask&correct_init_mask].mean())\n",
    "print(\"MSP correct \", (~condition_mask&correct_msp_mask&~correct_init_mask).sum(), \n",
    "      ood_score_init_list[~condition_mask&correct_msp_mask&~correct_init_mask].mean(),\n",
    "      #   ood_score_target_list[~condition_mask&correct_msp_mask&~correct_init_mask].mean(),\n",
    "        ood_score_final_list[~condition_mask&correct_msp_mask&~correct_init_mask].mean())\n",
    "print(\"Init correct\", (~condition_mask&~correct_msp_mask&correct_init_mask).sum(), \n",
    "      ood_score_init_list[~condition_mask&~correct_msp_mask&correct_init_mask].mean(),\n",
    "      #   ood_score_target_list[~condition_mask&~correct_msp_mask&correct_init_mask].mean(),\n",
    "        ood_score_final_list[~condition_mask&~correct_msp_mask&correct_init_mask].mean())\n",
    "print(\"Permutation correct\",(~condition_mask&correct_permutation_confident_mask&~correct_msp_mask&~correct_init_mask).sum(), \n",
    "      ood_score_init_list[~condition_mask&correct_permutation_confident_mask&~correct_msp_mask&~correct_init_mask].mean(),\n",
    "      #   ood_score_target_list[~condition_mask&correct_permutation_confident_mask&~correct_msp_mask&~correct_init_mask].mean(),\n",
    "        ood_score_final_list[~condition_mask&correct_permutation_confident_mask&~correct_msp_mask&~correct_init_mask].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  21,   52,   59,  118,  138,  154,  246,  265,  273,  275,  286,  287,\n",
       "         296,  325,  372,  419,  433,  568,  610,  643,  743,  808,  894,  929,\n",
       "         989, 1042, 1075])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_col_idx_permutation_msp\n",
    "# imbalance: 21, 52, 59, 154, 265, 275, 286\n",
    "# early stop: 21, 59, 246, 275\n",
    "# not imformative: 118, 138, 154 286\n",
    "# todo: 287"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "7 [1, 2, 3, 0, 4, 5, 6, 7]\n",
      "********************************************************\n",
      "(1, 2, 3, 0, 4, 5, 6) tensor(0.6667) tensor(28)\n",
      "(1, 2, 3, 0, 4, 5, 7) tensor(0.7467) tensor(28)\n",
      "(1, 2, 3, 0, 4, 6, 7) tensor(0.6455) tensor(28)\n",
      "(1, 2, 3, 0, 5, 6, 7) tensor(0.7037) tensor(28)\n",
      "(1, 2, 0, 4, 5, 6, 7) tensor(0.2869) tensor(62)\n",
      "(1, 3, 0, 4, 5, 6, 7) tensor(0.1574) tensor(90)\n",
      "(2, 3, 0, 4, 5, 6, 7) tensor(0.1626) tensor(62)\n",
      "(1, 2, 3, 0, 4, 5) tensor(0.6485) tensor(28)\n",
      "(1, 2, 3, 0, 4, 6) tensor(0.7471) tensor(28)\n",
      "(1, 2, 3, 0, 4, 7) tensor(0.2197) tensor(95)\n",
      "(1, 2, 3, 0, 5, 6) tensor(0.6111) tensor(28)\n",
      "(1, 2, 3, 0, 5, 7) tensor(0.2820) tensor(28)\n",
      "(1, 2, 3, 0, 6, 7) tensor(0.2656) tensor(95)\n",
      "(1, 2, 0, 4, 5, 6) tensor(0.3814) tensor(28)\n",
      "(1, 2, 0, 4, 5, 7) tensor(0.1848) tensor(62)\n",
      "(1, 2, 0, 4, 6, 7) tensor(0.2487) tensor(62)\n",
      "(1, 2, 0, 5, 6, 7) tensor(0.2260) tensor(28)\n",
      "(1, 3, 0, 4, 5, 6) tensor(0.6227) tensor(51)\n",
      "(1, 3, 0, 4, 5, 7) tensor(0.2212) tensor(95)\n",
      "(1, 3, 0, 4, 6, 7) tensor(0.2318) tensor(95)\n",
      "(1, 3, 0, 5, 6, 7) tensor(0.2748) tensor(95)\n",
      "(1, 0, 4, 5, 6, 7) tensor(0.2908) tensor(62)\n",
      "(2, 3, 0, 4, 5, 6) tensor(0.6289) tensor(51)\n",
      "(2, 3, 0, 4, 5, 7) tensor(0.2255) tensor(95)\n",
      "(2, 3, 0, 4, 6, 7) tensor(0.2247) tensor(95)\n",
      "(2, 3, 0, 5, 6, 7) tensor(0.2712) tensor(95)\n",
      "(2, 0, 4, 5, 6, 7) tensor(0.6842) tensor(1)\n",
      "(3, 0, 4, 5, 6, 7) tensor(0.2852) tensor(75)\n",
      "(1, 2, 3, 0, 4) tensor(0.7054) tensor(28)\n",
      "(1, 2, 3, 0, 5) tensor(0.5548) tensor(28)\n",
      "(1, 2, 3, 0, 6) tensor(0.6785) tensor(28)\n",
      "(1, 2, 3, 0, 7) tensor(0.3647) tensor(28)\n",
      "(1, 2, 0, 4, 5) tensor(0.3709) tensor(28)\n",
      "(1, 2, 0, 4, 6) tensor(0.5416) tensor(28)\n",
      "(1, 2, 0, 4, 7) tensor(0.2831) tensor(28)\n",
      "(1, 2, 0, 5, 6) tensor(0.3538) tensor(28)\n",
      "(1, 2, 0, 5, 7) tensor(0.4304) tensor(28)\n",
      "(1, 2, 0, 6, 7) tensor(0.3135) tensor(28)\n",
      "(1, 3, 0, 4, 5) tensor(0.6746) tensor(51)\n",
      "(1, 3, 0, 4, 6) tensor(0.4329) tensor(51)\n",
      "(1, 3, 0, 4, 7) tensor(0.2827) tensor(95)\n",
      "(1, 3, 0, 5, 6) tensor(0.6847) tensor(51)\n",
      "(1, 3, 0, 5, 7) tensor(0.2843) tensor(95)\n",
      "(1, 3, 0, 6, 7) tensor(0.2649) tensor(95)\n",
      "(1, 0, 4, 5, 6) tensor(0.5031) tensor(51)\n",
      "(1, 0, 4, 5, 7) tensor(0.2238) tensor(62)\n",
      "(1, 0, 4, 6, 7) tensor(0.2633) tensor(62)\n",
      "(1, 0, 5, 6, 7) tensor(0.2139) tensor(75)\n",
      "(2, 3, 0, 4, 5) tensor(0.6643) tensor(51)\n",
      "(2, 3, 0, 4, 6) tensor(0.3600) tensor(95)\n",
      "(2, 3, 0, 4, 7) tensor(0.3008) tensor(95)\n",
      "(2, 3, 0, 5, 6) tensor(0.6593) tensor(51)\n",
      "(2, 3, 0, 5, 7) tensor(0.3370) tensor(95)\n",
      "(2, 3, 0, 6, 7) tensor(0.2789) tensor(95)\n",
      "(2, 0, 4, 5, 6) tensor(0.4709) tensor(75)\n",
      "(2, 0, 4, 5, 7) tensor(0.2072) tensor(1)\n",
      "(2, 0, 4, 6, 7) tensor(0.2348) tensor(1)\n",
      "(2, 0, 5, 6, 7) tensor(0.2025) tensor(75)\n",
      "(3, 0, 4, 5, 6) tensor(0.6337) tensor(75)\n",
      "(3, 0, 4, 5, 7) tensor(0.3696) tensor(75)\n",
      "(3, 0, 4, 6, 7) tensor(0.1550) tensor(72)\n",
      "(3, 0, 5, 6, 7) tensor(0.5863) tensor(75)\n",
      "(0, 4, 5, 6, 7) tensor(0.2229) tensor(30)\n",
      "********************************************************\n",
      "(1, 2, 3, 0, 4, 6) tensor(0.7471) tensor(28)\n"
     ]
    }
   ],
   "source": [
    "# use target as head, the MSP context\n",
    "import itertools\n",
    "\n",
    "def is_sublist(A, B):\n",
    "    it = iter(B)\n",
    "    return all(x in it for x in A)\n",
    "    \n",
    "model.load_state_dict(best_state_dict, strict=False)\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "ft_embs_test = []\n",
    "labels_test = []\n",
    "logits_test = []\n",
    "log = defaultdict(list)\n",
    "num_cols = []\n",
    "threshold = 0.8\n",
    "corrected = 0\n",
    "total_mistakes = 0\n",
    "init_permutation = defaultdict(list)\n",
    "corrected_permutation = defaultdict(list)\n",
    "init_logits = defaultdict(list)\n",
    "corrected_logits = defaultdict(list)\n",
    "max_col_length = 3\n",
    "msp_threshold = 0.9\n",
    "\n",
    "\n",
    "alpha = 1.0\n",
    "\n",
    "class_weights = (1.0 / class_freq) ** alpha\n",
    "\n",
    "# Normalize the weights\n",
    "class_weights /= class_weights.sum()\n",
    "\n",
    "for batch_idx, batch in enumerate(test_dataset_iter):\n",
    "    if batch_idx == 21:\n",
    "        break\n",
    "cls_indexes = torch.LongTensor([[0, batch[\"cls_indexes\"].cpu().item()]]).to(device)\n",
    "target_col_mask = batch[\"target_col_mask\"].T\n",
    "col_idx_set = target_col_mask.unique().tolist()\n",
    "init_permutation_i = get_permutation(target_col_mask)\n",
    "successs = False\n",
    "# logits = model(batch[\"data\"].T, cls_indexes=cls_indexes,).detach().cpu()\n",
    "# logits = reweight_logits(logits, class_weights)\n",
    "# logits_init = logits.clone()\n",
    "# num_cols.append(batch[\"target_col_mask\"].max().item())\n",
    "\n",
    "\n",
    "# col_idx_set = target_col_mask.unique().tolist()\n",
    "# successs = False\n",
    "# init_permutation_i = get_permutation(target_col_mask)\n",
    "# init_permutation[batch_idx].append(get_permutation(target_col_mask))\n",
    "# init_logits[batch_idx].append(logits_init.detach().cpu())     \n",
    "# init_msp = logits_init.max().item()\n",
    "# print(batch[\"label\"].item())\n",
    "# print(get_permutation(target_col_mask), init_msp, init_logits[batch_idx][0].argmax().item()) \n",
    "print(batch[\"label\"].item())  \n",
    "print(batch[\"target_col_mask\"].max().item(), get_permutation(target_col_mask))\n",
    "print(\"********************************************************\")\n",
    "assert -1 not in col_idx_set\n",
    "max_msp = 0 \n",
    "# for r in range(1, min(len(col_idx_set), max_col_length) + 1):\n",
    "enough = False\n",
    "# for r in range(len(col_idx_set)-1, 0, -1):\n",
    "for r in range(len(col_idx_set)-1, len(col_idx_set)//2, -1):\n",
    "    for subset in itertools.combinations(col_idx_set, r):\n",
    "        if 0 not in subset and r != 1:\n",
    "            continue\n",
    "        for x in itertools.permutations(subset):\n",
    "            if not is_sublist(x, init_permutation_i):\n",
    "                continue\n",
    "            new_batch_data = []\n",
    "            for col_i in x:\n",
    "                if col_i == 0:\n",
    "                    if len(new_batch_data) == 0:\n",
    "                        cls_indexes_value = 0\n",
    "                    else:\n",
    "                        cls_indexes_value = sum([len(new_batch_data[i]) for i in range(len(new_batch_data))])\n",
    "                new_batch_data.append(batch[\"data\"].T[target_col_mask==col_i])\n",
    "            if 0 not in x:\n",
    "                cls_indexes_value = 0\n",
    "            new_batch_data = torch.cat(new_batch_data, dim=-1).reshape(1, -1)\n",
    "            cls_indexes = torch.tensor([0, cls_indexes_value]).reshape(1, -1).to(device)\n",
    "            logits_temp = model(new_batch_data, cls_indexes=cls_indexes,).detach().cpu()\n",
    "            logits_temp = reweight_logits(logits_temp, class_weights)\n",
    "            msp_temp = logits_temp.max()\n",
    "            predict_temp = logits_temp.argmax()\n",
    "            # msp_temp = F.softmax(logits_temp).max().item()\n",
    "            # predict_temp = F.softmax(logits_temp).argmax().item()\n",
    "\n",
    "            print(x, msp_temp,predict_temp)\n",
    "            if msp_temp > max_msp and 0 in x:\n",
    "                max_msp = msp_temp\n",
    "                best_msp_perm = x\n",
    "                msp_predict = predict_temp\n",
    "        \n",
    "print(\"********************************************************\")\n",
    "print(best_msp_perm, max_msp, msp_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "7 [1, 2, 3, 4, 5, 6, 7, 0]\n",
      "********************************************************\n",
      "(1, 2, 3, 4, 5, 6, 0) tensor(0.9756) tensor(2)\n",
      "(1, 2, 3, 4, 5, 7, 0) tensor(0.9756) tensor(2)\n",
      "(1, 2, 3, 4, 6, 7, 0) tensor(0.9892) tensor(2)\n",
      "(1, 2, 3, 5, 6, 7, 0) tensor(0.9861) tensor(2)\n",
      "(1, 2, 4, 5, 6, 7, 0) tensor(0.8058) tensor(2)\n",
      "(1, 3, 4, 5, 6, 7, 0) tensor(0.9832) tensor(2)\n",
      "(2, 3, 4, 5, 6, 7, 0) tensor(0.9849) tensor(2)\n",
      "(1, 2, 3, 4, 5, 0) tensor(0.9788) tensor(17)\n",
      "(1, 2, 3, 4, 6, 0) tensor(0.6931) tensor(2)\n",
      "(1, 2, 3, 4, 7, 0) tensor(0.8591) tensor(17)\n",
      "(1, 2, 3, 5, 6, 0) tensor(0.4576) tensor(17)\n",
      "(1, 2, 3, 5, 7, 0) tensor(0.9128) tensor(17)\n",
      "(1, 2, 3, 6, 7, 0) tensor(0.9318) tensor(17)\n",
      "(1, 2, 4, 5, 6, 0) tensor(0.9621) tensor(17)\n",
      "(1, 2, 4, 5, 7, 0) tensor(0.8874) tensor(17)\n",
      "(1, 2, 4, 6, 7, 0) tensor(0.9143) tensor(17)\n",
      "(1, 2, 5, 6, 7, 0) tensor(0.9435) tensor(17)\n",
      "(1, 3, 4, 5, 6, 0) tensor(0.6641) tensor(2)\n",
      "(1, 3, 4, 5, 7, 0) tensor(0.9132) tensor(17)\n",
      "(1, 3, 4, 6, 7, 0) tensor(0.9250) tensor(17)\n",
      "(1, 3, 5, 6, 7, 0) tensor(0.9355) tensor(17)\n",
      "(1, 4, 5, 6, 7, 0) tensor(0.9499) tensor(17)\n",
      "(2, 3, 4, 5, 6, 0) tensor(0.5688) tensor(2)\n",
      "(2, 3, 4, 5, 7, 0) tensor(0.8687) tensor(17)\n",
      "(2, 3, 4, 6, 7, 0) tensor(0.9038) tensor(17)\n",
      "(2, 3, 5, 6, 7, 0) tensor(0.9312) tensor(17)\n",
      "(2, 4, 5, 6, 7, 0) tensor(0.9316) tensor(17)\n",
      "(3, 4, 5, 6, 7, 0) tensor(0.9629) tensor(17)\n",
      "(1, 2, 3, 4, 0) tensor(0.7773) tensor(2)\n",
      "(1, 2, 3, 5, 0) tensor(0.7912) tensor(2)\n",
      "(1, 2, 3, 6, 0) tensor(0.9775) tensor(2)\n",
      "(1, 2, 3, 7, 0) tensor(0.9461) tensor(2)\n",
      "(1, 2, 4, 5, 0) tensor(0.5401) tensor(2)\n",
      "(1, 2, 4, 6, 0) tensor(0.9524) tensor(2)\n",
      "(1, 2, 4, 7, 0) tensor(0.8936) tensor(2)\n",
      "(1, 2, 5, 6, 0) tensor(0.8892) tensor(2)\n",
      "(1, 2, 5, 7, 0) tensor(0.6477) tensor(2)\n",
      "(1, 2, 6, 7, 0) tensor(0.9313) tensor(2)\n",
      "(1, 3, 4, 5, 0) tensor(0.7349) tensor(2)\n",
      "(1, 3, 4, 6, 0) tensor(0.9743) tensor(2)\n",
      "(1, 3, 4, 7, 0) tensor(0.9496) tensor(2)\n",
      "(1, 3, 5, 6, 0) tensor(0.9717) tensor(2)\n",
      "(1, 3, 5, 7, 0) tensor(0.9367) tensor(2)\n",
      "(1, 3, 6, 7, 0) tensor(0.9720) tensor(2)\n",
      "(1, 4, 5, 6, 0) tensor(0.8835) tensor(2)\n",
      "(1, 4, 5, 7, 0) tensor(0.6229) tensor(2)\n",
      "(1, 4, 6, 7, 0) tensor(0.9298) tensor(2)\n",
      "(1, 5, 6, 7, 0) tensor(0.8745) tensor(2)\n",
      "(2, 3, 4, 5, 0) tensor(0.7891) tensor(2)\n",
      "(2, 3, 4, 6, 0) tensor(0.9679) tensor(2)\n",
      "(2, 3, 4, 7, 0) tensor(0.9539) tensor(2)\n",
      "(2, 3, 5, 6, 0) tensor(0.9569) tensor(2)\n",
      "(2, 3, 5, 7, 0) tensor(0.9183) tensor(2)\n",
      "(2, 3, 6, 7, 0) tensor(0.9639) tensor(2)\n",
      "(2, 4, 5, 6, 0) tensor(0.8854) tensor(2)\n",
      "(2, 4, 5, 7, 0) tensor(0.7749) tensor(2)\n",
      "(2, 4, 6, 7, 0) tensor(0.9456) tensor(2)\n",
      "(2, 5, 6, 7, 0) tensor(0.9016) tensor(2)\n",
      "(3, 4, 5, 6, 0) tensor(0.9491) tensor(2)\n",
      "(3, 4, 5, 7, 0) tensor(0.9097) tensor(2)\n",
      "(3, 4, 6, 7, 0) tensor(0.9600) tensor(2)\n",
      "(3, 5, 6, 7, 0) tensor(0.9491) tensor(2)\n",
      "(4, 5, 6, 7, 0) tensor(0.8982) tensor(2)\n",
      "********************************************************\n",
      "(1, 2, 3, 4, 6, 7, 0) tensor(0.9892) tensor(2)\n"
     ]
    }
   ],
   "source": [
    "# use target as head, the MSP context\n",
    "import itertools\n",
    "\n",
    "def is_sublist(A, B):\n",
    "    it = iter(B)\n",
    "    return all(x in it for x in A)\n",
    "    \n",
    "model.load_state_dict(best_state_dict, strict=False)\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "ft_embs_test = []\n",
    "labels_test = []\n",
    "logits_test = []\n",
    "log = defaultdict(list)\n",
    "num_cols = []\n",
    "threshold = 0.8\n",
    "corrected = 0\n",
    "total_mistakes = 0\n",
    "init_permutation = defaultdict(list)\n",
    "corrected_permutation = defaultdict(list)\n",
    "init_logits = defaultdict(list)\n",
    "corrected_logits = defaultdict(list)\n",
    "max_col_length = 3\n",
    "msp_threshold = 0.9\n",
    "\n",
    "\n",
    "alpha = 1.0\n",
    "\n",
    "class_weights = (1.0 / class_freq) ** alpha\n",
    "\n",
    "# Normalize the weights\n",
    "class_weights /= class_weights.sum()\n",
    "\n",
    "for batch_idx, batch in enumerate(test_dataset_iter):\n",
    "    if batch_idx == 287:\n",
    "        break\n",
    "cls_indexes = torch.LongTensor([[0, batch[\"cls_indexes\"].cpu().item()]]).to(device)\n",
    "target_col_mask = batch[\"target_col_mask\"].T\n",
    "col_idx_set = target_col_mask.unique().tolist()\n",
    "init_permutation_i = get_permutation(target_col_mask)\n",
    "successs = False\n",
    "# logits = model(batch[\"data\"].T, cls_indexes=cls_indexes,).detach().cpu()\n",
    "# logits = reweight_logits(logits, class_weights)\n",
    "# logits_init = logits.clone()\n",
    "# num_cols.append(batch[\"target_col_mask\"].max().item())\n",
    "\n",
    "\n",
    "# col_idx_set = target_col_mask.unique().tolist()\n",
    "# successs = False\n",
    "# init_permutation_i = get_permutation(target_col_mask)\n",
    "# init_permutation[batch_idx].append(get_permutation(target_col_mask))\n",
    "# init_logits[batch_idx].append(logits_init.detach().cpu())     \n",
    "# init_msp = logits_init.max().item()\n",
    "# print(batch[\"label\"].item())\n",
    "# print(get_permutation(target_col_mask), init_msp, init_logits[batch_idx][0].argmax().item()) \n",
    "print(batch[\"label\"].item())  \n",
    "print(batch[\"target_col_mask\"].max().item(), get_permutation(target_col_mask))\n",
    "print(\"********************************************************\")\n",
    "assert -1 not in col_idx_set\n",
    "max_msp = 0 \n",
    "# for r in range(1, min(len(col_idx_set), max_col_length) + 1):\n",
    "enough = False\n",
    "# for r in range(len(col_idx_set)-1, 0, -1):\n",
    "for r in range(len(col_idx_set)-1, len(col_idx_set)//2, -1):\n",
    "    for subset in itertools.combinations(col_idx_set, r):\n",
    "        if 0 not in subset and r != 1:\n",
    "            continue\n",
    "        for x in itertools.permutations(subset):\n",
    "            if not is_sublist(x, init_permutation_i):\n",
    "                continue\n",
    "            new_batch_data = []\n",
    "            for col_i in x:\n",
    "                if col_i == 0:\n",
    "                    if len(new_batch_data) == 0:\n",
    "                        cls_indexes_value = 0\n",
    "                    else:\n",
    "                        cls_indexes_value = sum([len(new_batch_data[i]) for i in range(len(new_batch_data))])\n",
    "                new_batch_data.append(batch[\"data\"].T[target_col_mask==col_i])\n",
    "            if 0 not in x:\n",
    "                cls_indexes_value = 0\n",
    "            new_batch_data = torch.cat(new_batch_data, dim=-1).reshape(1, -1)\n",
    "            cls_indexes = torch.tensor([0, cls_indexes_value]).reshape(1, -1).to(device)\n",
    "            logits_temp = model(new_batch_data, cls_indexes=cls_indexes,).detach().cpu()\n",
    "            logits_temp = reweight_logits_after_softmax(logits_temp, class_weights)\n",
    "            msp_temp = logits_temp.max()\n",
    "            predict_temp = logits_temp.argmax()\n",
    "            # msp_temp = F.softmax(logits_temp).max().item()\n",
    "            # predict_temp = F.softmax(logits_temp).argmax().item()\n",
    "\n",
    "            print(x, msp_temp,predict_temp)\n",
    "            if msp_temp > max_msp and 0 in x:\n",
    "                max_msp = msp_temp\n",
    "                best_msp_perm = x\n",
    "                msp_predict = predict_temp\n",
    "        \n",
    "print(\"********************************************************\")\n",
    "print(best_msp_perm, max_msp, msp_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************alpha: 0.25; uncertain: 0.8; debias: 1.0; msp: 0.8****************************\n",
      "ts_micro_f1=0.5429, ts_macro_f1=0.2641\n",
      "ts_micro_f1=0.5424, ts_macro_f1=0.2641\n",
      "ts_micro_f1=1.0000, ts_macro_f1=1.0000\n",
      "*********************alpha: 0.25; uncertain: 0.8; debias: 1.0; msp: 0.9****************************\n",
      "ts_micro_f1=0.5419, ts_macro_f1=0.2632\n",
      "ts_micro_f1=0.5415, ts_macro_f1=0.2632\n",
      "ts_micro_f1=1.0000, ts_macro_f1=1.0000\n",
      "*********************alpha: 0.25; uncertain: 0.8; debias: 1.0; msp: 0.99****************************\n",
      "ts_micro_f1=0.5382, ts_macro_f1=0.2665\n",
      "ts_micro_f1=0.5378, ts_macro_f1=0.2665\n",
      "ts_micro_f1=1.0000, ts_macro_f1=1.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# brute force perumate\n",
    "import itertools\n",
    "\n",
    "\n",
    "    \n",
    "model.load_state_dict(best_state_dict, strict=False)\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "change_log = []\n",
    "score_init = []\n",
    "score_best = [] \n",
    "\n",
    "max_col_length = 3\n",
    "for alpha in [0.25]:\n",
    "    class_weights = (1.0 / class_freq) ** alpha\n",
    "    # Normalize the weights\n",
    "    class_weights /= class_weights.sum()\n",
    "    for uncertainty_threshold in [ 0.8]:\n",
    "        for msp_threshold in [ 0.8, 0.9, 0.99]:\n",
    "            for debias_threshold in [1.0]:\n",
    "                print(f\"*********************alpha: {alpha}; uncertain: {uncertainty_threshold}; debias: {debias_threshold}; msp: {msp_threshold}****************************\")\n",
    "                ft_embs_test = []\n",
    "                labels_test = []\n",
    "                logits_test = []\n",
    "                log = defaultdict(list)\n",
    "                num_cols = []\n",
    "                corrected = 0\n",
    "                total_mistakes = 0\n",
    "\n",
    "                for batch_idx, batch in enumerate(test_dataloader_iter):\n",
    "                    cls_indexes = torch.LongTensor([[0, batch[\"cls_indexes\"].cpu().item()]]).to(device)\n",
    "                    target_col_mask = batch[\"target_col_mask\"].T\n",
    "                    logits = model(batch[\"data\"].T, cls_indexes=cls_indexes,)\n",
    "                    logits = reweight_logits(logits.detach().cpu(), class_weights) # Here !!!!!!!!!!\n",
    "                    msp_init = logits.max().item()\n",
    "                    num_cols.append(batch[\"target_col_mask\"].max().item())\n",
    "                    predict_init = logits.argmax().item()\n",
    "                    max_msp = 0\n",
    "                    \n",
    "                    if 1 in target_col_mask and msp_init < uncertainty_threshold:\n",
    "                        init_permutation_i = get_permutation(target_col_mask)\n",
    "                        col_idx_set = target_col_mask.unique().tolist()\n",
    "                        debias_classes = []\n",
    "                        assert -1 not in col_idx_set\n",
    "                        for r in range(len(col_idx_set)-1, len(col_idx_set)//2, -1):\n",
    "                            for subset in itertools.combinations(col_idx_set, r):\n",
    "                                if 0 not in subset and r != 1:\n",
    "                                    continue\n",
    "                                for x in itertools.permutations(subset):\n",
    "                                    if not is_sublist(x, init_permutation_i):\n",
    "                                        continue\n",
    "                                    new_batch_data = []\n",
    "                                    for col_i in x:\n",
    "                                        if col_i == 0:\n",
    "                                            if len(new_batch_data) == 0:\n",
    "                                                cls_indexes_value = 0\n",
    "                                            else:\n",
    "                                                cls_indexes_value = sum([len(new_batch_data[i]) for i in range(len(new_batch_data))])\n",
    "                                        new_batch_data.append(batch[\"data\"].T[target_col_mask==col_i])\n",
    "                                    if 0 not in x:\n",
    "                                        cls_indexes_value = 0\n",
    "                                    new_batch_data = torch.cat(new_batch_data, dim=-1).reshape(1, -1)\n",
    "                                    cls_indexes = torch.tensor([0, cls_indexes_value]).reshape(1, -1).to(device)\n",
    "                                    logits_temp = model(new_batch_data, cls_indexes=cls_indexes,)\n",
    "                                    logits_temp = reweight_logits(logits_temp.detach().cpu(), class_weights)\n",
    "                                    msp_temp = logits_temp.max().item()\n",
    "                                    predict_temp = logits_temp.argmax().item()\n",
    "                                    # if len(x) == 1 and 0 in x:\n",
    "                                    #     predict_target = predict_temp\n",
    "                                    #     msp_target = msp_temp\n",
    "                                    # # print(x, msp_temp, predict_temp)\n",
    "                                    # if 0 not in x and msp_temp > debias_threshold and (predict_temp != predict_target):\n",
    "                                    #     debias_classes.append(predict_temp)\n",
    "                                    #     continue\n",
    "                                    if msp_temp > max_msp and 0 in x and predict_temp not in debias_classes:\n",
    "                                        max_msp = msp_temp\n",
    "                                        best_msp_perm = x\n",
    "                                        msp_predict = predict_temp\n",
    "                                        logits_msp = logits_temp.clone()\n",
    "                    if max_msp > msp_threshold:\n",
    "                        logits = logits_msp.clone()\n",
    "                    labels_test.append(batch[\"label\"].cpu())\n",
    "                    logits_test.append(logits.detach().cpu())\n",
    "                labels_test = torch.cat(labels_test, dim=0)\n",
    "                logits_test = torch.stack(logits_test, dim=0)\n",
    "                preds_test = torch.argmax(logits_test, dim=1)\n",
    "                num_cols = torch.tensor(num_cols)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                from sklearn.metrics import confusion_matrix, f1_score\n",
    "                mask = num_cols > 0\n",
    "                ts_pred_list = logits_test.argmax(\n",
    "                                            1).cpu().detach().numpy().tolist()\n",
    "                ts_micro_f1 = f1_score(labels_test.reshape(-1).numpy().tolist(),\n",
    "                                    ts_pred_list,\n",
    "                                    average=\"micro\")\n",
    "                ts_macro_f1 = f1_score(labels_test.reshape(-1).numpy().tolist(),\n",
    "                                    ts_pred_list,\n",
    "                                    average=\"macro\")\n",
    "                print(\"ts_micro_f1={:.4f}, ts_macro_f1={:.4f}\".format(ts_micro_f1, ts_macro_f1))\n",
    "                ts_pred_list = logits_test.argmax(\n",
    "                                            1).cpu().detach()[mask].numpy().tolist()\n",
    "                ts_micro_f1 = f1_score(labels_test.reshape(-1)[mask].numpy().tolist(),\n",
    "                                    ts_pred_list,\n",
    "                                    average=\"micro\")\n",
    "                ts_macro_f1 = f1_score(labels_test.reshape(-1)[mask].numpy().tolist(),\n",
    "                                    ts_pred_list,\n",
    "                                    average=\"macro\")\n",
    "                print(\"ts_micro_f1={:.4f}, ts_macro_f1={:.4f}\".format(ts_micro_f1, ts_macro_f1))\n",
    "                ts_pred_list = logits_test.argmax(\n",
    "                                            1).cpu().detach()[~mask].numpy().tolist()\n",
    "                ts_micro_f1 = f1_score(labels_test.reshape(-1)[~mask].numpy().tolist(),\n",
    "                                    ts_pred_list,\n",
    "                                    average=\"micro\")\n",
    "                ts_macro_f1 = f1_score(labels_test.reshape(-1)[~mask].numpy().tolist(),\n",
    "                                    ts_pred_list,\n",
    "                                    average=\"macro\")\n",
    "                print(\"ts_micro_f1={:.4f}, ts_macro_f1={:.4f}\".format(ts_micro_f1, ts_macro_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************alpha: 0.25; uncertain: 0.8; debias: 1.0; msp: 0.0****************************\n",
      "ts_micro_f1=0.5419, ts_macro_f1=0.2611\n",
      "ts_micro_f1=0.5415, ts_macro_f1=0.2610\n",
      "ts_micro_f1=1.0000, ts_macro_f1=1.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# brute force perumate\n",
    "import itertools\n",
    "\n",
    "\n",
    "    \n",
    "model.load_state_dict(best_state_dict, strict=False)\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "change_log = []\n",
    "score_init = []\n",
    "score_best = [] \n",
    "\n",
    "max_col_length = 3\n",
    "for alpha in [0.25]:\n",
    "    class_weights = (1.0 / class_freq) ** alpha\n",
    "    # Normalize the weights\n",
    "    class_weights /= class_weights.sum()\n",
    "    for uncertainty_threshold in [ 0.8]:\n",
    "        for msp_threshold in [ 0.0]:\n",
    "            for debias_threshold in [1.0]:\n",
    "                print(f\"*********************alpha: {alpha}; uncertain: {uncertainty_threshold}; debias: {debias_threshold}; msp: {msp_threshold}****************************\")\n",
    "                ft_embs_test = []\n",
    "                labels_test = []\n",
    "                logits_test = []\n",
    "                log = defaultdict(list)\n",
    "                num_cols = []\n",
    "                corrected = 0\n",
    "                total_mistakes = 0\n",
    "\n",
    "                for batch_idx, batch in enumerate(test_dataloader_iter):\n",
    "                    cls_indexes = torch.LongTensor([[0, batch[\"cls_indexes\"].cpu().item()]]).to(device)\n",
    "                    target_col_mask = batch[\"target_col_mask\"].T\n",
    "                    logits = model(batch[\"data\"].T, cls_indexes=cls_indexes,)\n",
    "                    logits = reweight_logits(logits.detach().cpu(), class_weights) # Here !!!!!!!!!!\n",
    "                    msp_init = logits.max().item()\n",
    "                    num_cols.append(batch[\"target_col_mask\"].max().item())\n",
    "                    predict_init = logits.argmax().item()\n",
    "                    max_msp = 0\n",
    "                    \n",
    "                    if 1 in target_col_mask and msp_init < uncertainty_threshold:\n",
    "                        init_permutation_i = get_permutation(target_col_mask)\n",
    "                        col_idx_set = target_col_mask.unique().tolist()\n",
    "                        debias_classes = []\n",
    "                        assert -1 not in col_idx_set\n",
    "                        for r in range(len(col_idx_set)-1, len(col_idx_set)//2, -1):\n",
    "                            for subset in itertools.combinations(col_idx_set, r):\n",
    "                                if 0 not in subset and r != 1:\n",
    "                                    continue\n",
    "                                for x in itertools.permutations(subset):\n",
    "                                    if not is_sublist(x, init_permutation_i):\n",
    "                                        continue\n",
    "                                    new_batch_data = []\n",
    "                                    for col_i in x:\n",
    "                                        if col_i == 0:\n",
    "                                            if len(new_batch_data) == 0:\n",
    "                                                cls_indexes_value = 0\n",
    "                                            else:\n",
    "                                                cls_indexes_value = sum([len(new_batch_data[i]) for i in range(len(new_batch_data))])\n",
    "                                        new_batch_data.append(batch[\"data\"].T[target_col_mask==col_i])\n",
    "                                    if 0 not in x:\n",
    "                                        cls_indexes_value = 0\n",
    "                                    new_batch_data = torch.cat(new_batch_data, dim=-1).reshape(1, -1)\n",
    "                                    cls_indexes = torch.tensor([0, cls_indexes_value]).reshape(1, -1).to(device)\n",
    "                                    logits_temp = model(new_batch_data, cls_indexes=cls_indexes,)\n",
    "                                    logits_temp = reweight_logits(logits_temp.detach().cpu(), class_weights)\n",
    "                                    msp_temp = logits_temp.max().item()\n",
    "                                    predict_temp = logits_temp.argmax().item()\n",
    "                                    # if len(x) == 1 and 0 in x:\n",
    "                                    #     predict_target = predict_temp\n",
    "                                    #     msp_target = msp_temp\n",
    "                                    # # print(x, msp_temp, predict_temp)\n",
    "                                    # if 0 not in x and msp_temp > debias_threshold and (predict_temp != predict_target):\n",
    "                                    #     debias_classes.append(predict_temp)\n",
    "                                    #     continue\n",
    "                                    if msp_temp > max_msp and 0 in x and predict_temp not in debias_classes:\n",
    "                                        max_msp = msp_temp\n",
    "                                        best_msp_perm = x\n",
    "                                        msp_predict = predict_temp\n",
    "                                        logits_msp = logits_temp.clone()\n",
    "                    if max_msp > msp_threshold:\n",
    "                        logits = logits_msp.clone()\n",
    "                    labels_test.append(batch[\"label\"].cpu())\n",
    "                    logits_test.append(logits.detach().cpu())\n",
    "                labels_test = torch.cat(labels_test, dim=0)\n",
    "                logits_test = torch.stack(logits_test, dim=0)\n",
    "                preds_test = torch.argmax(logits_test, dim=1)\n",
    "                num_cols = torch.tensor(num_cols)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                from sklearn.metrics import confusion_matrix, f1_score\n",
    "                mask = num_cols > 0\n",
    "                ts_pred_list = logits_test.argmax(\n",
    "                                            1).cpu().detach().numpy().tolist()\n",
    "                ts_micro_f1 = f1_score(labels_test.reshape(-1).numpy().tolist(),\n",
    "                                    ts_pred_list,\n",
    "                                    average=\"micro\")\n",
    "                ts_macro_f1 = f1_score(labels_test.reshape(-1).numpy().tolist(),\n",
    "                                    ts_pred_list,\n",
    "                                    average=\"macro\")\n",
    "                print(\"ts_micro_f1={:.4f}, ts_macro_f1={:.4f}\".format(ts_micro_f1, ts_macro_f1))\n",
    "                ts_pred_list = logits_test.argmax(\n",
    "                                            1).cpu().detach()[mask].numpy().tolist()\n",
    "                ts_micro_f1 = f1_score(labels_test.reshape(-1)[mask].numpy().tolist(),\n",
    "                                    ts_pred_list,\n",
    "                                    average=\"micro\")\n",
    "                ts_macro_f1 = f1_score(labels_test.reshape(-1)[mask].numpy().tolist(),\n",
    "                                    ts_pred_list,\n",
    "                                    average=\"macro\")\n",
    "                print(\"ts_micro_f1={:.4f}, ts_macro_f1={:.4f}\".format(ts_micro_f1, ts_macro_f1))\n",
    "                ts_pred_list = logits_test.argmax(\n",
    "                                            1).cpu().detach()[~mask].numpy().tolist()\n",
    "                ts_micro_f1 = f1_score(labels_test.reshape(-1)[~mask].numpy().tolist(),\n",
    "                                    ts_pred_list,\n",
    "                                    average=\"micro\")\n",
    "                ts_macro_f1 = f1_score(labels_test.reshape(-1)[~mask].numpy().tolist(),\n",
    "                                    ts_pred_list,\n",
    "                                    average=\"macro\")\n",
    "                print(\"ts_micro_f1={:.4f}, ts_macro_f1={:.4f}\".format(ts_micro_f1, ts_macro_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************alpha: 0.25; uncertain: 0.85; debias: 1.0; msp: 0.0****************************\n",
      "ts_micro_f1=0.5429, ts_macro_f1=0.2634\n",
      "ts_micro_f1=0.5424, ts_macro_f1=0.2634\n",
      "ts_micro_f1=1.0000, ts_macro_f1=1.0000\n",
      "*********************alpha: 0.25; uncertain: 0.85; debias: 1.0; msp: 0.8****************************\n",
      "ts_micro_f1=0.5438, ts_macro_f1=0.2665\n",
      "ts_micro_f1=0.5434, ts_macro_f1=0.2665\n",
      "ts_micro_f1=1.0000, ts_macro_f1=1.0000\n",
      "*********************alpha: 0.25; uncertain: 0.85; debias: 1.0; msp: 0.85****************************\n",
      "ts_micro_f1=0.5419, ts_macro_f1=0.2636\n",
      "ts_micro_f1=0.5415, ts_macro_f1=0.2636\n",
      "ts_micro_f1=1.0000, ts_macro_f1=1.0000\n",
      "*********************alpha: 0.25; uncertain: 0.85; debias: 1.0; msp: 0.9****************************\n",
      "ts_micro_f1=0.5429, ts_macro_f1=0.2655\n",
      "ts_micro_f1=0.5424, ts_macro_f1=0.2655\n",
      "ts_micro_f1=1.0000, ts_macro_f1=1.0000\n",
      "*********************alpha: 0.25; uncertain: 0.85; debias: 1.0; msp: 0.99****************************\n",
      "ts_micro_f1=0.5392, ts_macro_f1=0.2678\n",
      "ts_micro_f1=0.5387, ts_macro_f1=0.2678\n",
      "ts_micro_f1=1.0000, ts_macro_f1=1.0000\n",
      "*********************alpha: 0.25; uncertain: 0.9; debias: 1.0; msp: 0.0****************************\n",
      "ts_micro_f1=0.5401, ts_macro_f1=0.2593\n",
      "ts_micro_f1=0.5397, ts_macro_f1=0.2593\n",
      "ts_micro_f1=1.0000, ts_macro_f1=1.0000\n",
      "*********************alpha: 0.25; uncertain: 0.9; debias: 1.0; msp: 0.8****************************\n",
      "ts_micro_f1=0.5410, ts_macro_f1=0.2624\n",
      "ts_micro_f1=0.5406, ts_macro_f1=0.2624\n",
      "ts_micro_f1=1.0000, ts_macro_f1=1.0000\n",
      "*********************alpha: 0.25; uncertain: 0.9; debias: 1.0; msp: 0.85****************************\n",
      "ts_micro_f1=0.5392, ts_macro_f1=0.2597\n",
      "ts_micro_f1=0.5387, ts_macro_f1=0.2597\n",
      "ts_micro_f1=1.0000, ts_macro_f1=1.0000\n",
      "*********************alpha: 0.25; uncertain: 0.9; debias: 1.0; msp: 0.9****************************\n",
      "ts_micro_f1=0.5401, ts_macro_f1=0.2618\n",
      "ts_micro_f1=0.5397, ts_macro_f1=0.2618\n",
      "ts_micro_f1=1.0000, ts_macro_f1=1.0000\n",
      "*********************alpha: 0.25; uncertain: 0.9; debias: 1.0; msp: 0.99****************************\n",
      "ts_micro_f1=0.5373, ts_macro_f1=0.2648\n",
      "ts_micro_f1=0.5369, ts_macro_f1=0.2648\n",
      "ts_micro_f1=1.0000, ts_macro_f1=1.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# brute force perumate\n",
    "import itertools\n",
    "\n",
    "\n",
    "    \n",
    "model.load_state_dict(best_state_dict, strict=False)\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "change_log = []\n",
    "score_init = []\n",
    "score_best = [] \n",
    "\n",
    "max_col_length = 3\n",
    "for alpha in [0.25]:\n",
    "    class_weights = (1.0 / class_freq) ** alpha\n",
    "    # Normalize the weights\n",
    "    class_weights /= class_weights.sum()\n",
    "    for uncertainty_threshold in [ 0.85, 0.9]:\n",
    "        for msp_threshold in [ 0.0, 0.8, 0.85, 0.9, 0.99]:\n",
    "            for debias_threshold in [1.0]:\n",
    "                print(f\"*********************alpha: {alpha}; uncertain: {uncertainty_threshold}; debias: {debias_threshold}; msp: {msp_threshold}****************************\")\n",
    "                ft_embs_test = []\n",
    "                labels_test = []\n",
    "                logits_test = []\n",
    "                log = defaultdict(list)\n",
    "                num_cols = []\n",
    "                corrected = 0\n",
    "                total_mistakes = 0\n",
    "\n",
    "                for batch_idx, batch in enumerate(test_dataloader_iter):\n",
    "                    cls_indexes = torch.LongTensor([[0, batch[\"cls_indexes\"].cpu().item()]]).to(device)\n",
    "                    target_col_mask = batch[\"target_col_mask\"].T\n",
    "                    logits = model(batch[\"data\"].T, cls_indexes=cls_indexes,)\n",
    "                    logits = reweight_logits(logits.detach().cpu(), class_weights) # Here !!!!!!!!!!\n",
    "                    msp_init = logits.max().item()\n",
    "                    num_cols.append(batch[\"target_col_mask\"].max().item())\n",
    "                    predict_init = logits.argmax().item()\n",
    "                    max_msp = 0\n",
    "                    \n",
    "                    if 1 in target_col_mask and msp_init < uncertainty_threshold:\n",
    "                        init_permutation_i = get_permutation(target_col_mask)\n",
    "                        col_idx_set = target_col_mask.unique().tolist()\n",
    "                        debias_classes = []\n",
    "                        assert -1 not in col_idx_set\n",
    "                        for r in range(len(col_idx_set)-1, len(col_idx_set)//2, -1):\n",
    "                            for subset in itertools.combinations(col_idx_set, r):\n",
    "                                if 0 not in subset and r != 1:\n",
    "                                    continue\n",
    "                                for x in itertools.permutations(subset):\n",
    "                                    if not is_sublist(x, init_permutation_i):\n",
    "                                        continue\n",
    "                                    new_batch_data = []\n",
    "                                    for col_i in x:\n",
    "                                        if col_i == 0:\n",
    "                                            if len(new_batch_data) == 0:\n",
    "                                                cls_indexes_value = 0\n",
    "                                            else:\n",
    "                                                cls_indexes_value = sum([len(new_batch_data[i]) for i in range(len(new_batch_data))])\n",
    "                                        new_batch_data.append(batch[\"data\"].T[target_col_mask==col_i])\n",
    "                                    if 0 not in x:\n",
    "                                        cls_indexes_value = 0\n",
    "                                    new_batch_data = torch.cat(new_batch_data, dim=-1).reshape(1, -1)\n",
    "                                    cls_indexes = torch.tensor([0, cls_indexes_value]).reshape(1, -1).to(device)\n",
    "                                    logits_temp = model(new_batch_data, cls_indexes=cls_indexes,)\n",
    "                                    logits_temp = reweight_logits(logits_temp.detach().cpu(), class_weights)\n",
    "                                    msp_temp = logits_temp.max().item()\n",
    "                                    predict_temp = logits_temp.argmax().item()\n",
    "                                    # if len(x) == 1 and 0 in x:\n",
    "                                    #     predict_target = predict_temp\n",
    "                                    #     msp_target = msp_temp\n",
    "                                    # # print(x, msp_temp, predict_temp)\n",
    "                                    # if 0 not in x and msp_temp > debias_threshold and (predict_temp != predict_target):\n",
    "                                    #     debias_classes.append(predict_temp)\n",
    "                                    #     continue\n",
    "                                    if msp_temp > max_msp and 0 in x and predict_temp not in debias_classes:\n",
    "                                        max_msp = msp_temp\n",
    "                                        best_msp_perm = x\n",
    "                                        msp_predict = predict_temp\n",
    "                                        logits_msp = logits_temp.clone()\n",
    "                    if max_msp > msp_threshold:\n",
    "                        logits = logits_msp.clone()\n",
    "                    labels_test.append(batch[\"label\"].cpu())\n",
    "                    logits_test.append(logits.detach().cpu())\n",
    "                labels_test = torch.cat(labels_test, dim=0)\n",
    "                logits_test = torch.stack(logits_test, dim=0)\n",
    "                preds_test = torch.argmax(logits_test, dim=1)\n",
    "                num_cols = torch.tensor(num_cols)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                from sklearn.metrics import confusion_matrix, f1_score\n",
    "                mask = num_cols > 0\n",
    "                ts_pred_list = logits_test.argmax(\n",
    "                                            1).cpu().detach().numpy().tolist()\n",
    "                ts_micro_f1 = f1_score(labels_test.reshape(-1).numpy().tolist(),\n",
    "                                    ts_pred_list,\n",
    "                                    average=\"micro\")\n",
    "                ts_macro_f1 = f1_score(labels_test.reshape(-1).numpy().tolist(),\n",
    "                                    ts_pred_list,\n",
    "                                    average=\"macro\")\n",
    "                print(\"ts_micro_f1={:.4f}, ts_macro_f1={:.4f}\".format(ts_micro_f1, ts_macro_f1))\n",
    "                ts_pred_list = logits_test.argmax(\n",
    "                                            1).cpu().detach()[mask].numpy().tolist()\n",
    "                ts_micro_f1 = f1_score(labels_test.reshape(-1)[mask].numpy().tolist(),\n",
    "                                    ts_pred_list,\n",
    "                                    average=\"micro\")\n",
    "                ts_macro_f1 = f1_score(labels_test.reshape(-1)[mask].numpy().tolist(),\n",
    "                                    ts_pred_list,\n",
    "                                    average=\"macro\")\n",
    "                print(\"ts_micro_f1={:.4f}, ts_macro_f1={:.4f}\".format(ts_micro_f1, ts_macro_f1))\n",
    "                ts_pred_list = logits_test.argmax(\n",
    "                                            1).cpu().detach()[~mask].numpy().tolist()\n",
    "                ts_micro_f1 = f1_score(labels_test.reshape(-1)[~mask].numpy().tolist(),\n",
    "                                    ts_pred_list,\n",
    "                                    average=\"micro\")\n",
    "                ts_macro_f1 = f1_score(labels_test.reshape(-1)[~mask].numpy().tolist(),\n",
    "                                    ts_pred_list,\n",
    "                                    average=\"macro\")\n",
    "                print(\"ts_micro_f1={:.4f}, ts_macro_f1={:.4f}\".format(ts_micro_f1, ts_macro_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
